{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件GAN（Conditional Generative Adversarial Networks，cGAN）是GAN的一种变体，它在生成器和判别器的输入中增加了额外的条件信息，以指导生成器生成特定类别的图像。在具体实现时，可以将条件信息作为输入数据的一部分，与随机噪声级联起来作为生成器和判别器的输入。\n",
    "\n",
    "以下是实现条件GAN的一般步骤：\n",
    "\n",
    "定义生成器（Generator）和判别器（Discriminator）的网络结构。在这里，需要修改网络结构，使其能够接受条件信息作为输入，并将其与随机噪声级联起来。生成器的输出将是生成的图像，而判别器的输出将是对图像的真实性评估。\n",
    "\n",
    "定义生成器和判别器的损失函数。对于条件GAN，生成器和判别器的损失函数通常包括GAN损失和条件信息损失。GAN损失用于衡量生成的图像与真实图像之间的差异，而条件信息损失用于确保生成的图像满足给定的条件信息。\n",
    "\n",
    "编写训练循环。在每个训练迭代中，首先从数据集中随机采样真实图像和相应的条件信息，然后使用生成器生成假图像。接下来，将真实图像、假图像和相应的条件信息输入到判别器中，计算并更新生成器和判别器的损失函数。\n",
    "\n",
    "训练生成器和判别器。通过反复迭代训练循环，不断更新生成器和判别器的参数，直到达到预定的训练轮数或损失收敛。\n",
    "\n",
    "评估模型性能。在训练完成后，可以使用生成器生成图像，并通过人工评估或使用一些指标来评估生成的图像质量和生成的图像是否满足条件信息。\n",
    "\n",
    "以下是一个简单的示例代码，演示了如何使用PyTorch实现条件GAN："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Batch [0/938], Loss D: 0.6079, Loss G: 0.6753\n",
      "Epoch [0/200], Batch [100/938], Loss D: 0.3967, Loss G: 0.7398\n",
      "Epoch [0/200], Batch [200/938], Loss D: 0.1798, Loss G: 1.5792\n",
      "Epoch [0/200], Batch [300/938], Loss D: 0.2201, Loss G: 1.3184\n",
      "Epoch [0/200], Batch [400/938], Loss D: 0.3957, Loss G: 0.8844\n",
      "Epoch [0/200], Batch [500/938], Loss D: 0.3860, Loss G: 0.9290\n",
      "Epoch [0/200], Batch [600/938], Loss D: 0.4031, Loss G: 1.0419\n",
      "Epoch [0/200], Batch [700/938], Loss D: 0.5204, Loss G: 0.8032\n",
      "Epoch [0/200], Batch [800/938], Loss D: 0.4311, Loss G: 1.0606\n",
      "Epoch [0/200], Batch [900/938], Loss D: 0.2132, Loss G: 1.8064\n",
      "Epoch [1/200], Batch [0/938], Loss D: 0.4192, Loss G: 1.1212\n",
      "Epoch [1/200], Batch [100/938], Loss D: 0.1690, Loss G: 1.7858\n",
      "Epoch [1/200], Batch [200/938], Loss D: 0.4165, Loss G: 1.1407\n",
      "Epoch [1/200], Batch [300/938], Loss D: 0.4849, Loss G: 0.9997\n",
      "Epoch [1/200], Batch [400/938], Loss D: 0.4784, Loss G: 1.0441\n",
      "Epoch [1/200], Batch [500/938], Loss D: 0.3432, Loss G: 1.3508\n",
      "Epoch [1/200], Batch [600/938], Loss D: 0.2566, Loss G: 1.7195\n",
      "Epoch [1/200], Batch [700/938], Loss D: 0.3258, Loss G: 1.3759\n",
      "Epoch [1/200], Batch [800/938], Loss D: 0.3285, Loss G: 1.4204\n",
      "Epoch [1/200], Batch [900/938], Loss D: 0.4030, Loss G: 1.3295\n",
      "Epoch [2/200], Batch [0/938], Loss D: 0.4865, Loss G: 1.1897\n",
      "Epoch [2/200], Batch [100/938], Loss D: 0.4002, Loss G: 1.1153\n",
      "Epoch [2/200], Batch [200/938], Loss D: 0.6841, Loss G: 0.7262\n",
      "Epoch [2/200], Batch [300/938], Loss D: 0.7087, Loss G: 0.7334\n",
      "Epoch [2/200], Batch [400/938], Loss D: 0.5984, Loss G: 0.9096\n",
      "Epoch [2/200], Batch [500/938], Loss D: 0.8815, Loss G: 0.7616\n",
      "Epoch [2/200], Batch [600/938], Loss D: 0.7252, Loss G: 0.9790\n",
      "Epoch [2/200], Batch [700/938], Loss D: 0.4082, Loss G: 1.4035\n",
      "Epoch [2/200], Batch [800/938], Loss D: 0.4148, Loss G: 1.3365\n",
      "Epoch [2/200], Batch [900/938], Loss D: 0.4346, Loss G: 1.2181\n",
      "Epoch [3/200], Batch [0/938], Loss D: 0.2627, Loss G: 1.5989\n",
      "Epoch [3/200], Batch [100/938], Loss D: 0.3327, Loss G: 1.4942\n",
      "Epoch [3/200], Batch [200/938], Loss D: 0.3287, Loss G: 1.3194\n",
      "Epoch [3/200], Batch [300/938], Loss D: 0.3836, Loss G: 1.3200\n",
      "Epoch [3/200], Batch [400/938], Loss D: 0.6091, Loss G: 0.7982\n",
      "Epoch [3/200], Batch [500/938], Loss D: 0.7914, Loss G: 0.5837\n",
      "Epoch [3/200], Batch [600/938], Loss D: 0.6836, Loss G: 0.7312\n",
      "Epoch [3/200], Batch [700/938], Loss D: 0.7287, Loss G: 0.7543\n",
      "Epoch [3/200], Batch [800/938], Loss D: 0.7545, Loss G: 0.7201\n",
      "Epoch [3/200], Batch [900/938], Loss D: 0.6438, Loss G: 0.8414\n",
      "Epoch [4/200], Batch [0/938], Loss D: 0.6399, Loss G: 0.8772\n",
      "Epoch [4/200], Batch [100/938], Loss D: 0.7437, Loss G: 0.6925\n",
      "Epoch [4/200], Batch [200/938], Loss D: 0.4014, Loss G: 1.1337\n",
      "Epoch [4/200], Batch [300/938], Loss D: 0.5631, Loss G: 1.0061\n",
      "Epoch [4/200], Batch [400/938], Loss D: 0.6276, Loss G: 0.8270\n",
      "Epoch [4/200], Batch [500/938], Loss D: 0.6013, Loss G: 0.8475\n",
      "Epoch [4/200], Batch [600/938], Loss D: 0.5773, Loss G: 0.9471\n",
      "Epoch [4/200], Batch [700/938], Loss D: 0.6828, Loss G: 0.7878\n",
      "Epoch [4/200], Batch [800/938], Loss D: 0.7944, Loss G: 0.6318\n",
      "Epoch [4/200], Batch [900/938], Loss D: 0.5007, Loss G: 0.8994\n",
      "Epoch [5/200], Batch [0/938], Loss D: 0.6444, Loss G: 0.8018\n",
      "Epoch [5/200], Batch [100/938], Loss D: 0.7500, Loss G: 0.6513\n",
      "Epoch [5/200], Batch [200/938], Loss D: 0.6535, Loss G: 0.7683\n",
      "Epoch [5/200], Batch [300/938], Loss D: 0.5985, Loss G: 0.8223\n",
      "Epoch [5/200], Batch [400/938], Loss D: 0.6319, Loss G: 0.7160\n",
      "Epoch [5/200], Batch [500/938], Loss D: 0.5433, Loss G: 0.9313\n",
      "Epoch [5/200], Batch [600/938], Loss D: 0.7709, Loss G: 0.6816\n",
      "Epoch [5/200], Batch [700/938], Loss D: 0.4099, Loss G: 1.2628\n",
      "Epoch [5/200], Batch [800/938], Loss D: 0.8134, Loss G: 0.7388\n",
      "Epoch [5/200], Batch [900/938], Loss D: 0.6416, Loss G: 0.9363\n",
      "Epoch [6/200], Batch [0/938], Loss D: 0.2635, Loss G: 1.5874\n",
      "Epoch [6/200], Batch [100/938], Loss D: 0.9047, Loss G: 0.5124\n",
      "Epoch [6/200], Batch [200/938], Loss D: 0.9224, Loss G: 0.5732\n",
      "Epoch [6/200], Batch [300/938], Loss D: 0.5421, Loss G: 0.9869\n",
      "Epoch [6/200], Batch [400/938], Loss D: 0.5853, Loss G: 0.8404\n",
      "Epoch [6/200], Batch [500/938], Loss D: 0.5033, Loss G: 1.0058\n",
      "Epoch [6/200], Batch [600/938], Loss D: 0.6158, Loss G: 0.8740\n",
      "Epoch [6/200], Batch [700/938], Loss D: 0.8309, Loss G: 0.7366\n",
      "Epoch [6/200], Batch [800/938], Loss D: 0.3372, Loss G: 1.3415\n",
      "Epoch [6/200], Batch [900/938], Loss D: 0.7343, Loss G: 0.8138\n",
      "Epoch [7/200], Batch [0/938], Loss D: 0.6629, Loss G: 0.8227\n",
      "Epoch [7/200], Batch [100/938], Loss D: 0.5214, Loss G: 1.1574\n",
      "Epoch [7/200], Batch [200/938], Loss D: 0.3071, Loss G: 1.5813\n",
      "Epoch [7/200], Batch [300/938], Loss D: 0.4884, Loss G: 1.0377\n",
      "Epoch [7/200], Batch [400/938], Loss D: 0.9790, Loss G: 0.5191\n",
      "Epoch [7/200], Batch [500/938], Loss D: 0.6370, Loss G: 0.7930\n",
      "Epoch [7/200], Batch [600/938], Loss D: 0.5466, Loss G: 1.0634\n",
      "Epoch [7/200], Batch [700/938], Loss D: 0.9543, Loss G: 0.5742\n",
      "Epoch [7/200], Batch [800/938], Loss D: 0.6583, Loss G: 0.9123\n",
      "Epoch [7/200], Batch [900/938], Loss D: 0.9173, Loss G: 0.6661\n",
      "Epoch [8/200], Batch [0/938], Loss D: 0.6365, Loss G: 0.9626\n",
      "Epoch [8/200], Batch [100/938], Loss D: 0.5391, Loss G: 1.1009\n",
      "Epoch [8/200], Batch [200/938], Loss D: 0.5005, Loss G: 1.2596\n",
      "Epoch [8/200], Batch [300/938], Loss D: 0.7531, Loss G: 0.8002\n",
      "Epoch [8/200], Batch [400/938], Loss D: 0.3655, Loss G: 1.4461\n",
      "Epoch [8/200], Batch [500/938], Loss D: 0.7906, Loss G: 0.7222\n",
      "Epoch [8/200], Batch [600/938], Loss D: 0.7398, Loss G: 0.8559\n",
      "Epoch [8/200], Batch [700/938], Loss D: 0.4277, Loss G: 1.2576\n",
      "Epoch [8/200], Batch [800/938], Loss D: 0.4689, Loss G: 1.1941\n",
      "Epoch [8/200], Batch [900/938], Loss D: 0.5321, Loss G: 1.1486\n",
      "Epoch [9/200], Batch [0/938], Loss D: 0.7389, Loss G: 0.8076\n",
      "Epoch [9/200], Batch [100/938], Loss D: 0.7171, Loss G: 0.8663\n",
      "Epoch [9/200], Batch [200/938], Loss D: 0.6804, Loss G: 0.9471\n",
      "Epoch [9/200], Batch [300/938], Loss D: 0.5826, Loss G: 0.9566\n",
      "Epoch [9/200], Batch [400/938], Loss D: 0.4165, Loss G: 1.4040\n",
      "Epoch [9/200], Batch [500/938], Loss D: 0.6573, Loss G: 0.7734\n",
      "Epoch [9/200], Batch [600/938], Loss D: 0.5116, Loss G: 1.1557\n",
      "Epoch [9/200], Batch [700/938], Loss D: 0.5210, Loss G: 1.0853\n",
      "Epoch [9/200], Batch [800/938], Loss D: 0.6052, Loss G: 0.8337\n",
      "Epoch [9/200], Batch [900/938], Loss D: 0.5055, Loss G: 1.1893\n",
      "Epoch [10/200], Batch [0/938], Loss D: 0.3042, Loss G: 1.5394\n",
      "Epoch [10/200], Batch [100/938], Loss D: 0.8050, Loss G: 0.6831\n",
      "Epoch [10/200], Batch [200/938], Loss D: 0.8101, Loss G: 0.6589\n",
      "Epoch [10/200], Batch [300/938], Loss D: 0.6937, Loss G: 0.7824\n",
      "Epoch [10/200], Batch [400/938], Loss D: 0.5156, Loss G: 0.9960\n",
      "Epoch [10/200], Batch [500/938], Loss D: 0.4888, Loss G: 1.1425\n",
      "Epoch [10/200], Batch [600/938], Loss D: 0.4503, Loss G: 1.1332\n",
      "Epoch [10/200], Batch [700/938], Loss D: 0.6325, Loss G: 0.8970\n",
      "Epoch [10/200], Batch [800/938], Loss D: 0.7035, Loss G: 0.9549\n",
      "Epoch [10/200], Batch [900/938], Loss D: 0.5621, Loss G: 1.1432\n",
      "Epoch [11/200], Batch [0/938], Loss D: 0.5763, Loss G: 0.8984\n",
      "Epoch [11/200], Batch [100/938], Loss D: 0.5570, Loss G: 0.9905\n",
      "Epoch [11/200], Batch [200/938], Loss D: 0.5941, Loss G: 0.9317\n",
      "Epoch [11/200], Batch [300/938], Loss D: 0.6571, Loss G: 0.9346\n",
      "Epoch [11/200], Batch [400/938], Loss D: 0.5790, Loss G: 1.1703\n",
      "Epoch [11/200], Batch [500/938], Loss D: 0.3853, Loss G: 1.4731\n",
      "Epoch [11/200], Batch [600/938], Loss D: 0.5662, Loss G: 1.4009\n",
      "Epoch [11/200], Batch [700/938], Loss D: 0.6580, Loss G: 1.0273\n",
      "Epoch [11/200], Batch [800/938], Loss D: 0.4309, Loss G: 1.1114\n",
      "Epoch [11/200], Batch [900/938], Loss D: 0.3519, Loss G: 1.4767\n",
      "Epoch [12/200], Batch [0/938], Loss D: 0.4406, Loss G: 1.1028\n",
      "Epoch [12/200], Batch [100/938], Loss D: 0.3196, Loss G: 1.3835\n",
      "Epoch [12/200], Batch [200/938], Loss D: 0.4289, Loss G: 1.3191\n",
      "Epoch [12/200], Batch [300/938], Loss D: 0.4719, Loss G: 1.1415\n",
      "Epoch [12/200], Batch [400/938], Loss D: 0.4621, Loss G: 1.0417\n",
      "Epoch [12/200], Batch [500/938], Loss D: 0.6294, Loss G: 0.8030\n",
      "Epoch [12/200], Batch [600/938], Loss D: 0.5866, Loss G: 0.9339\n",
      "Epoch [12/200], Batch [700/938], Loss D: 0.8348, Loss G: 0.7289\n",
      "Epoch [12/200], Batch [800/938], Loss D: 0.7702, Loss G: 0.7729\n",
      "Epoch [12/200], Batch [900/938], Loss D: 0.6033, Loss G: 1.0592\n",
      "Epoch [13/200], Batch [0/938], Loss D: 0.6050, Loss G: 1.0700\n",
      "Epoch [13/200], Batch [100/938], Loss D: 0.5427, Loss G: 1.1256\n",
      "Epoch [13/200], Batch [200/938], Loss D: 0.5336, Loss G: 1.1339\n",
      "Epoch [13/200], Batch [300/938], Loss D: 0.6723, Loss G: 0.8744\n",
      "Epoch [13/200], Batch [400/938], Loss D: 0.7200, Loss G: 0.8651\n",
      "Epoch [13/200], Batch [500/938], Loss D: 0.7437, Loss G: 0.8971\n",
      "Epoch [13/200], Batch [600/938], Loss D: 0.5299, Loss G: 1.0686\n",
      "Epoch [13/200], Batch [700/938], Loss D: 0.5598, Loss G: 1.1303\n",
      "Epoch [13/200], Batch [800/938], Loss D: 0.7303, Loss G: 0.7328\n",
      "Epoch [13/200], Batch [900/938], Loss D: 0.5801, Loss G: 1.0641\n",
      "Epoch [14/200], Batch [0/938], Loss D: 0.4833, Loss G: 1.2096\n",
      "Epoch [14/200], Batch [100/938], Loss D: 0.4608, Loss G: 1.2669\n",
      "Epoch [14/200], Batch [200/938], Loss D: 0.4544, Loss G: 1.3694\n",
      "Epoch [14/200], Batch [300/938], Loss D: 0.4491, Loss G: 1.1799\n",
      "Epoch [14/200], Batch [400/938], Loss D: 0.4665, Loss G: 1.1865\n",
      "Epoch [14/200], Batch [500/938], Loss D: 0.8076, Loss G: 0.7615\n",
      "Epoch [14/200], Batch [600/938], Loss D: 0.5235, Loss G: 1.0298\n",
      "Epoch [14/200], Batch [700/938], Loss D: 0.4096, Loss G: 1.3605\n",
      "Epoch [14/200], Batch [800/938], Loss D: 0.9129, Loss G: 0.6090\n",
      "Epoch [14/200], Batch [900/938], Loss D: 0.9907, Loss G: 0.4946\n",
      "Epoch [15/200], Batch [0/938], Loss D: 0.3575, Loss G: 1.4899\n",
      "Epoch [15/200], Batch [100/938], Loss D: 0.4999, Loss G: 1.0377\n",
      "Epoch [15/200], Batch [200/938], Loss D: 0.5751, Loss G: 0.8630\n",
      "Epoch [15/200], Batch [300/938], Loss D: 0.7234, Loss G: 0.8398\n",
      "Epoch [15/200], Batch [400/938], Loss D: 0.6292, Loss G: 0.9723\n",
      "Epoch [15/200], Batch [500/938], Loss D: 0.4465, Loss G: 1.2323\n",
      "Epoch [15/200], Batch [600/938], Loss D: 0.3778, Loss G: 1.4767\n",
      "Epoch [15/200], Batch [700/938], Loss D: 0.4599, Loss G: 1.1845\n",
      "Epoch [15/200], Batch [800/938], Loss D: 0.6017, Loss G: 0.9674\n",
      "Epoch [15/200], Batch [900/938], Loss D: 0.7123, Loss G: 0.7384\n",
      "Epoch [16/200], Batch [0/938], Loss D: 0.4953, Loss G: 1.3277\n",
      "Epoch [16/200], Batch [100/938], Loss D: 0.5676, Loss G: 1.1842\n",
      "Epoch [16/200], Batch [200/938], Loss D: 0.5336, Loss G: 1.0744\n",
      "Epoch [16/200], Batch [300/938], Loss D: 0.4575, Loss G: 1.4001\n",
      "Epoch [16/200], Batch [400/938], Loss D: 0.4155, Loss G: 1.2859\n",
      "Epoch [16/200], Batch [500/938], Loss D: 0.4418, Loss G: 1.2805\n",
      "Epoch [16/200], Batch [600/938], Loss D: 0.7425, Loss G: 0.9107\n",
      "Epoch [16/200], Batch [700/938], Loss D: 0.5631, Loss G: 1.1448\n",
      "Epoch [16/200], Batch [800/938], Loss D: 0.5490, Loss G: 1.2556\n",
      "Epoch [16/200], Batch [900/938], Loss D: 0.7288, Loss G: 1.0555\n",
      "Epoch [17/200], Batch [0/938], Loss D: 0.4824, Loss G: 1.3663\n",
      "Epoch [17/200], Batch [100/938], Loss D: 0.5808, Loss G: 1.2704\n",
      "Epoch [17/200], Batch [200/938], Loss D: 0.5067, Loss G: 1.1363\n",
      "Epoch [17/200], Batch [300/938], Loss D: 0.6158, Loss G: 1.1885\n",
      "Epoch [17/200], Batch [400/938], Loss D: 0.3437, Loss G: 1.6739\n",
      "Epoch [17/200], Batch [500/938], Loss D: 0.4461, Loss G: 1.2593\n",
      "Epoch [17/200], Batch [600/938], Loss D: 0.3871, Loss G: 1.7454\n",
      "Epoch [17/200], Batch [700/938], Loss D: 0.1663, Loss G: 2.1459\n",
      "Epoch [17/200], Batch [800/938], Loss D: 0.2803, Loss G: 1.7474\n",
      "Epoch [17/200], Batch [900/938], Loss D: 0.2087, Loss G: 2.1342\n",
      "Epoch [18/200], Batch [0/938], Loss D: 0.2634, Loss G: 1.6038\n",
      "Epoch [18/200], Batch [100/938], Loss D: 0.3760, Loss G: 1.2772\n",
      "Epoch [18/200], Batch [200/938], Loss D: 0.4729, Loss G: 1.0954\n",
      "Epoch [18/200], Batch [300/938], Loss D: 0.3703, Loss G: 1.3988\n",
      "Epoch [18/200], Batch [400/938], Loss D: 0.5520, Loss G: 1.2204\n",
      "Epoch [18/200], Batch [500/938], Loss D: 0.3030, Loss G: 1.6017\n",
      "Epoch [18/200], Batch [600/938], Loss D: 0.4874, Loss G: 1.2228\n",
      "Epoch [18/200], Batch [700/938], Loss D: 0.5752, Loss G: 1.2746\n",
      "Epoch [18/200], Batch [800/938], Loss D: 0.2854, Loss G: 2.0590\n",
      "Epoch [18/200], Batch [900/938], Loss D: 0.3060, Loss G: 1.7017\n",
      "Epoch [19/200], Batch [0/938], Loss D: 0.3103, Loss G: 2.0719\n",
      "Epoch [19/200], Batch [100/938], Loss D: 0.2288, Loss G: 2.1956\n",
      "Epoch [19/200], Batch [200/938], Loss D: 0.4658, Loss G: 1.6440\n",
      "Epoch [19/200], Batch [300/938], Loss D: 0.4556, Loss G: 1.5181\n",
      "Epoch [19/200], Batch [400/938], Loss D: 0.3208, Loss G: 1.7958\n",
      "Epoch [19/200], Batch [500/938], Loss D: 0.4116, Loss G: 1.7542\n",
      "Epoch [19/200], Batch [600/938], Loss D: 0.3081, Loss G: 1.6741\n",
      "Epoch [19/200], Batch [700/938], Loss D: 0.1742, Loss G: 2.1488\n",
      "Epoch [19/200], Batch [800/938], Loss D: 0.3321, Loss G: 1.5906\n",
      "Epoch [19/200], Batch [900/938], Loss D: 0.4088, Loss G: 1.5618\n",
      "Epoch [20/200], Batch [0/938], Loss D: 0.3821, Loss G: 1.5187\n",
      "Epoch [20/200], Batch [100/938], Loss D: 0.2569, Loss G: 1.9329\n",
      "Epoch [20/200], Batch [200/938], Loss D: 0.2983, Loss G: 1.6081\n",
      "Epoch [20/200], Batch [300/938], Loss D: 0.3128, Loss G: 2.0136\n",
      "Epoch [20/200], Batch [400/938], Loss D: 0.3786, Loss G: 1.5441\n",
      "Epoch [20/200], Batch [500/938], Loss D: 0.4651, Loss G: 1.4267\n",
      "Epoch [20/200], Batch [600/938], Loss D: 0.3463, Loss G: 1.9765\n",
      "Epoch [20/200], Batch [700/938], Loss D: 0.4086, Loss G: 1.5887\n",
      "Epoch [20/200], Batch [800/938], Loss D: 0.5156, Loss G: 1.7267\n",
      "Epoch [20/200], Batch [900/938], Loss D: 0.3881, Loss G: 1.7667\n",
      "Epoch [21/200], Batch [0/938], Loss D: 0.2723, Loss G: 1.7519\n",
      "Epoch [21/200], Batch [100/938], Loss D: 0.3803, Loss G: 1.7755\n",
      "Epoch [21/200], Batch [200/938], Loss D: 0.2658, Loss G: 1.8861\n",
      "Epoch [21/200], Batch [300/938], Loss D: 0.4171, Loss G: 1.5828\n",
      "Epoch [21/200], Batch [400/938], Loss D: 0.4680, Loss G: 1.6352\n",
      "Epoch [21/200], Batch [500/938], Loss D: 0.2681, Loss G: 1.8103\n",
      "Epoch [21/200], Batch [600/938], Loss D: 0.3395, Loss G: 1.4787\n",
      "Epoch [21/200], Batch [700/938], Loss D: 0.4192, Loss G: 1.6197\n",
      "Epoch [21/200], Batch [800/938], Loss D: 0.3244, Loss G: 1.7366\n",
      "Epoch [21/200], Batch [900/938], Loss D: 0.4500, Loss G: 1.5864\n",
      "Epoch [22/200], Batch [0/938], Loss D: 0.4187, Loss G: 1.6540\n",
      "Epoch [22/200], Batch [100/938], Loss D: 0.2938, Loss G: 1.6313\n",
      "Epoch [22/200], Batch [200/938], Loss D: 0.4301, Loss G: 1.4910\n",
      "Epoch [22/200], Batch [300/938], Loss D: 0.6419, Loss G: 1.3552\n",
      "Epoch [22/200], Batch [400/938], Loss D: 0.3548, Loss G: 1.4296\n",
      "Epoch [22/200], Batch [500/938], Loss D: 0.3513, Loss G: 1.8373\n",
      "Epoch [22/200], Batch [600/938], Loss D: 0.6005, Loss G: 1.3468\n",
      "Epoch [22/200], Batch [700/938], Loss D: 0.5388, Loss G: 1.4469\n",
      "Epoch [22/200], Batch [800/938], Loss D: 0.4228, Loss G: 1.2819\n",
      "Epoch [22/200], Batch [900/938], Loss D: 0.3700, Loss G: 1.4455\n",
      "Epoch [23/200], Batch [0/938], Loss D: 0.5475, Loss G: 1.6026\n",
      "Epoch [23/200], Batch [100/938], Loss D: 0.7692, Loss G: 1.0126\n",
      "Epoch [23/200], Batch [200/938], Loss D: 0.5773, Loss G: 1.1671\n",
      "Epoch [23/200], Batch [300/938], Loss D: 0.3733, Loss G: 1.4835\n",
      "Epoch [23/200], Batch [400/938], Loss D: 0.4974, Loss G: 1.3926\n",
      "Epoch [23/200], Batch [500/938], Loss D: 0.5059, Loss G: 1.1692\n",
      "Epoch [23/200], Batch [600/938], Loss D: 0.4881, Loss G: 1.4086\n",
      "Epoch [23/200], Batch [700/938], Loss D: 0.3649, Loss G: 1.4563\n",
      "Epoch [23/200], Batch [800/938], Loss D: 0.4481, Loss G: 1.4543\n",
      "Epoch [23/200], Batch [900/938], Loss D: 0.2730, Loss G: 1.7604\n",
      "Epoch [24/200], Batch [0/938], Loss D: 0.4997, Loss G: 1.2554\n",
      "Epoch [24/200], Batch [100/938], Loss D: 0.3482, Loss G: 1.8421\n",
      "Epoch [24/200], Batch [200/938], Loss D: 0.3615, Loss G: 1.8045\n",
      "Epoch [24/200], Batch [300/938], Loss D: 0.5859, Loss G: 1.0536\n",
      "Epoch [24/200], Batch [400/938], Loss D: 0.6086, Loss G: 1.3288\n",
      "Epoch [24/200], Batch [500/938], Loss D: 0.4377, Loss G: 1.5397\n",
      "Epoch [24/200], Batch [600/938], Loss D: 0.7103, Loss G: 0.9928\n",
      "Epoch [24/200], Batch [700/938], Loss D: 0.5173, Loss G: 1.3166\n",
      "Epoch [24/200], Batch [800/938], Loss D: 0.4658, Loss G: 1.3378\n",
      "Epoch [24/200], Batch [900/938], Loss D: 0.4591, Loss G: 1.5898\n",
      "Epoch [25/200], Batch [0/938], Loss D: 0.6306, Loss G: 1.2766\n",
      "Epoch [25/200], Batch [100/938], Loss D: 0.6461, Loss G: 1.2539\n",
      "Epoch [25/200], Batch [200/938], Loss D: 0.5905, Loss G: 1.4540\n",
      "Epoch [25/200], Batch [300/938], Loss D: 0.5431, Loss G: 1.0109\n",
      "Epoch [25/200], Batch [400/938], Loss D: 0.6080, Loss G: 1.4542\n",
      "Epoch [25/200], Batch [500/938], Loss D: 0.4604, Loss G: 1.4481\n",
      "Epoch [25/200], Batch [600/938], Loss D: 0.6440, Loss G: 1.3549\n",
      "Epoch [25/200], Batch [700/938], Loss D: 0.4408, Loss G: 1.4751\n",
      "Epoch [25/200], Batch [800/938], Loss D: 0.3793, Loss G: 1.5048\n",
      "Epoch [25/200], Batch [900/938], Loss D: 0.5733, Loss G: 1.1826\n",
      "Epoch [26/200], Batch [0/938], Loss D: 0.5507, Loss G: 1.1934\n",
      "Epoch [26/200], Batch [100/938], Loss D: 0.6301, Loss G: 1.2093\n",
      "Epoch [26/200], Batch [200/938], Loss D: 0.4230, Loss G: 1.5737\n",
      "Epoch [26/200], Batch [300/938], Loss D: 0.5503, Loss G: 1.0647\n",
      "Epoch [26/200], Batch [400/938], Loss D: 0.7576, Loss G: 0.9074\n",
      "Epoch [26/200], Batch [500/938], Loss D: 0.7058, Loss G: 1.3936\n",
      "Epoch [26/200], Batch [600/938], Loss D: 0.4513, Loss G: 1.5913\n",
      "Epoch [26/200], Batch [700/938], Loss D: 0.7161, Loss G: 1.2565\n",
      "Epoch [26/200], Batch [800/938], Loss D: 0.8502, Loss G: 0.9963\n",
      "Epoch [26/200], Batch [900/938], Loss D: 0.3597, Loss G: 1.7822\n",
      "Epoch [27/200], Batch [0/938], Loss D: 0.6010, Loss G: 1.4391\n",
      "Epoch [27/200], Batch [100/938], Loss D: 0.4830, Loss G: 1.7329\n",
      "Epoch [27/200], Batch [200/938], Loss D: 0.5922, Loss G: 1.0194\n",
      "Epoch [27/200], Batch [300/938], Loss D: 0.5953, Loss G: 1.0914\n",
      "Epoch [27/200], Batch [400/938], Loss D: 0.5623, Loss G: 1.3385\n",
      "Epoch [27/200], Batch [500/938], Loss D: 0.4456, Loss G: 1.8287\n",
      "Epoch [27/200], Batch [600/938], Loss D: 0.3885, Loss G: 1.8347\n",
      "Epoch [27/200], Batch [700/938], Loss D: 0.5248, Loss G: 1.3766\n",
      "Epoch [27/200], Batch [800/938], Loss D: 0.7081, Loss G: 0.9765\n",
      "Epoch [27/200], Batch [900/938], Loss D: 0.7081, Loss G: 1.1262\n",
      "Epoch [28/200], Batch [0/938], Loss D: 0.6214, Loss G: 1.2495\n",
      "Epoch [28/200], Batch [100/938], Loss D: 0.5083, Loss G: 1.2680\n",
      "Epoch [28/200], Batch [200/938], Loss D: 0.5575, Loss G: 1.2969\n",
      "Epoch [28/200], Batch [300/938], Loss D: 0.4420, Loss G: 1.6604\n",
      "Epoch [28/200], Batch [400/938], Loss D: 0.6623, Loss G: 1.1327\n",
      "Epoch [28/200], Batch [500/938], Loss D: 0.5333, Loss G: 1.1328\n",
      "Epoch [28/200], Batch [600/938], Loss D: 0.7638, Loss G: 0.9436\n",
      "Epoch [28/200], Batch [700/938], Loss D: 0.6573, Loss G: 0.9806\n",
      "Epoch [28/200], Batch [800/938], Loss D: 0.6538, Loss G: 1.3889\n",
      "Epoch [28/200], Batch [900/938], Loss D: 0.8718, Loss G: 0.8713\n",
      "Epoch [29/200], Batch [0/938], Loss D: 0.5479, Loss G: 1.1715\n",
      "Epoch [29/200], Batch [100/938], Loss D: 0.5687, Loss G: 1.4571\n",
      "Epoch [29/200], Batch [200/938], Loss D: 0.5996, Loss G: 1.2987\n",
      "Epoch [29/200], Batch [300/938], Loss D: 0.5543, Loss G: 1.3026\n",
      "Epoch [29/200], Batch [400/938], Loss D: 0.5944, Loss G: 1.2419\n",
      "Epoch [29/200], Batch [500/938], Loss D: 0.8254, Loss G: 1.0300\n",
      "Epoch [29/200], Batch [600/938], Loss D: 0.7278, Loss G: 1.1934\n",
      "Epoch [29/200], Batch [700/938], Loss D: 0.3273, Loss G: 2.0196\n",
      "Epoch [29/200], Batch [800/938], Loss D: 0.5173, Loss G: 1.4001\n",
      "Epoch [29/200], Batch [900/938], Loss D: 0.7424, Loss G: 1.1044\n",
      "Epoch [30/200], Batch [0/938], Loss D: 0.2770, Loss G: 2.0173\n",
      "Epoch [30/200], Batch [100/938], Loss D: 0.5129, Loss G: 1.0869\n",
      "Epoch [30/200], Batch [200/938], Loss D: 0.5784, Loss G: 1.1658\n",
      "Epoch [30/200], Batch [300/938], Loss D: 0.5097, Loss G: 1.2474\n",
      "Epoch [30/200], Batch [400/938], Loss D: 0.5684, Loss G: 1.0069\n",
      "Epoch [30/200], Batch [500/938], Loss D: 0.5657, Loss G: 1.1106\n",
      "Epoch [30/200], Batch [600/938], Loss D: 0.4797, Loss G: 1.0744\n",
      "Epoch [30/200], Batch [700/938], Loss D: 0.6132, Loss G: 1.3118\n",
      "Epoch [30/200], Batch [800/938], Loss D: 0.6022, Loss G: 1.3656\n",
      "Epoch [30/200], Batch [900/938], Loss D: 0.4013, Loss G: 1.6314\n",
      "Epoch [31/200], Batch [0/938], Loss D: 0.4646, Loss G: 1.5988\n",
      "Epoch [31/200], Batch [100/938], Loss D: 0.5830, Loss G: 1.1347\n",
      "Epoch [31/200], Batch [200/938], Loss D: 0.4640, Loss G: 1.1734\n",
      "Epoch [31/200], Batch [300/938], Loss D: 0.5561, Loss G: 1.2352\n",
      "Epoch [31/200], Batch [400/938], Loss D: 0.5086, Loss G: 1.3350\n",
      "Epoch [31/200], Batch [500/938], Loss D: 0.5516, Loss G: 1.3621\n",
      "Epoch [31/200], Batch [600/938], Loss D: 0.6282, Loss G: 1.4472\n",
      "Epoch [31/200], Batch [700/938], Loss D: 0.3942, Loss G: 1.6798\n",
      "Epoch [31/200], Batch [800/938], Loss D: 0.5221, Loss G: 1.5681\n",
      "Epoch [31/200], Batch [900/938], Loss D: 0.4304, Loss G: 1.7511\n",
      "Epoch [32/200], Batch [0/938], Loss D: 0.2662, Loss G: 1.9227\n",
      "Epoch [32/200], Batch [100/938], Loss D: 0.4032, Loss G: 1.5524\n",
      "Epoch [32/200], Batch [200/938], Loss D: 0.3420, Loss G: 1.6643\n",
      "Epoch [32/200], Batch [300/938], Loss D: 0.6360, Loss G: 1.4269\n",
      "Epoch [32/200], Batch [400/938], Loss D: 0.4969, Loss G: 1.5547\n",
      "Epoch [32/200], Batch [500/938], Loss D: 0.3041, Loss G: 1.9133\n",
      "Epoch [32/200], Batch [600/938], Loss D: 0.3654, Loss G: 1.5891\n",
      "Epoch [32/200], Batch [700/938], Loss D: 0.6751, Loss G: 1.2024\n",
      "Epoch [32/200], Batch [800/938], Loss D: 0.4108, Loss G: 1.5658\n",
      "Epoch [32/200], Batch [900/938], Loss D: 0.4271, Loss G: 1.6394\n",
      "Epoch [33/200], Batch [0/938], Loss D: 0.3992, Loss G: 1.9101\n",
      "Epoch [33/200], Batch [100/938], Loss D: 0.3396, Loss G: 1.8971\n",
      "Epoch [33/200], Batch [200/938], Loss D: 0.4192, Loss G: 1.7993\n",
      "Epoch [33/200], Batch [300/938], Loss D: 0.5366, Loss G: 1.5823\n",
      "Epoch [33/200], Batch [400/938], Loss D: 0.2955, Loss G: 1.6434\n",
      "Epoch [33/200], Batch [500/938], Loss D: 0.5467, Loss G: 1.3492\n",
      "Epoch [33/200], Batch [600/938], Loss D: 0.5531, Loss G: 1.3820\n",
      "Epoch [33/200], Batch [700/938], Loss D: 0.3328, Loss G: 1.9904\n",
      "Epoch [33/200], Batch [800/938], Loss D: 0.5996, Loss G: 1.2516\n",
      "Epoch [33/200], Batch [900/938], Loss D: 0.5462, Loss G: 1.2988\n",
      "Epoch [34/200], Batch [0/938], Loss D: 0.5000, Loss G: 1.5478\n",
      "Epoch [34/200], Batch [100/938], Loss D: 0.6055, Loss G: 1.2422\n",
      "Epoch [34/200], Batch [200/938], Loss D: 0.7263, Loss G: 1.2146\n",
      "Epoch [34/200], Batch [300/938], Loss D: 0.4885, Loss G: 1.6130\n",
      "Epoch [34/200], Batch [400/938], Loss D: 0.4365, Loss G: 1.9771\n",
      "Epoch [34/200], Batch [500/938], Loss D: 0.4361, Loss G: 1.5365\n",
      "Epoch [34/200], Batch [600/938], Loss D: 0.5945, Loss G: 1.3555\n",
      "Epoch [34/200], Batch [700/938], Loss D: 0.4729, Loss G: 1.7594\n",
      "Epoch [34/200], Batch [800/938], Loss D: 0.6345, Loss G: 1.2605\n",
      "Epoch [34/200], Batch [900/938], Loss D: 0.4356, Loss G: 1.6147\n",
      "Epoch [35/200], Batch [0/938], Loss D: 0.4731, Loss G: 1.3626\n",
      "Epoch [35/200], Batch [100/938], Loss D: 0.4533, Loss G: 1.4287\n",
      "Epoch [35/200], Batch [200/938], Loss D: 0.5023, Loss G: 1.7936\n",
      "Epoch [35/200], Batch [300/938], Loss D: 0.4492, Loss G: 1.8558\n",
      "Epoch [35/200], Batch [400/938], Loss D: 0.4666, Loss G: 1.4683\n",
      "Epoch [35/200], Batch [500/938], Loss D: 0.3673, Loss G: 1.7095\n",
      "Epoch [35/200], Batch [600/938], Loss D: 0.3103, Loss G: 1.7563\n",
      "Epoch [35/200], Batch [700/938], Loss D: 0.2765, Loss G: 2.2045\n",
      "Epoch [35/200], Batch [800/938], Loss D: 0.4143, Loss G: 1.6402\n",
      "Epoch [35/200], Batch [900/938], Loss D: 0.1827, Loss G: 2.0198\n",
      "Epoch [36/200], Batch [0/938], Loss D: 0.5293, Loss G: 1.1403\n",
      "Epoch [36/200], Batch [100/938], Loss D: 0.5142, Loss G: 1.4616\n",
      "Epoch [36/200], Batch [200/938], Loss D: 0.2858, Loss G: 1.6015\n",
      "Epoch [36/200], Batch [300/938], Loss D: 0.4468, Loss G: 1.3077\n",
      "Epoch [36/200], Batch [400/938], Loss D: 0.6677, Loss G: 1.3618\n",
      "Epoch [36/200], Batch [500/938], Loss D: 0.4979, Loss G: 1.7843\n",
      "Epoch [36/200], Batch [600/938], Loss D: 0.4407, Loss G: 1.5873\n",
      "Epoch [36/200], Batch [700/938], Loss D: 0.4490, Loss G: 1.5748\n",
      "Epoch [36/200], Batch [800/938], Loss D: 0.3558, Loss G: 1.7980\n",
      "Epoch [36/200], Batch [900/938], Loss D: 0.3487, Loss G: 1.7050\n",
      "Epoch [37/200], Batch [0/938], Loss D: 0.4468, Loss G: 1.6520\n",
      "Epoch [37/200], Batch [100/938], Loss D: 0.3728, Loss G: 1.5538\n",
      "Epoch [37/200], Batch [200/938], Loss D: 0.6411, Loss G: 1.0554\n",
      "Epoch [37/200], Batch [300/938], Loss D: 0.5314, Loss G: 1.2720\n",
      "Epoch [37/200], Batch [400/938], Loss D: 0.5600, Loss G: 1.1265\n",
      "Epoch [37/200], Batch [500/938], Loss D: 0.3833, Loss G: 1.6310\n",
      "Epoch [37/200], Batch [600/938], Loss D: 0.4548, Loss G: 1.5473\n",
      "Epoch [37/200], Batch [700/938], Loss D: 0.5335, Loss G: 1.1656\n",
      "Epoch [37/200], Batch [800/938], Loss D: 0.6709, Loss G: 1.2560\n",
      "Epoch [37/200], Batch [900/938], Loss D: 0.4997, Loss G: 1.4171\n",
      "Epoch [38/200], Batch [0/938], Loss D: 0.6437, Loss G: 1.2167\n",
      "Epoch [38/200], Batch [100/938], Loss D: 0.5706, Loss G: 1.0622\n",
      "Epoch [38/200], Batch [200/938], Loss D: 0.6666, Loss G: 1.3381\n",
      "Epoch [38/200], Batch [300/938], Loss D: 0.5007, Loss G: 1.6471\n",
      "Epoch [38/200], Batch [400/938], Loss D: 0.7448, Loss G: 0.9930\n",
      "Epoch [38/200], Batch [500/938], Loss D: 0.5397, Loss G: 1.6224\n",
      "Epoch [38/200], Batch [600/938], Loss D: 0.4584, Loss G: 1.2873\n",
      "Epoch [38/200], Batch [700/938], Loss D: 0.5298, Loss G: 1.3732\n",
      "Epoch [38/200], Batch [800/938], Loss D: 0.5860, Loss G: 1.0517\n",
      "Epoch [38/200], Batch [900/938], Loss D: 0.5642, Loss G: 1.3951\n",
      "Epoch [39/200], Batch [0/938], Loss D: 0.6859, Loss G: 1.0983\n",
      "Epoch [39/200], Batch [100/938], Loss D: 0.6168, Loss G: 1.4303\n",
      "Epoch [39/200], Batch [200/938], Loss D: 0.3051, Loss G: 1.7379\n",
      "Epoch [39/200], Batch [300/938], Loss D: 0.4668, Loss G: 1.3056\n",
      "Epoch [39/200], Batch [400/938], Loss D: 0.4565, Loss G: 1.9037\n",
      "Epoch [39/200], Batch [500/938], Loss D: 0.3447, Loss G: 1.6971\n",
      "Epoch [39/200], Batch [600/938], Loss D: 0.4682, Loss G: 1.5313\n",
      "Epoch [39/200], Batch [700/938], Loss D: 0.5605, Loss G: 1.5596\n",
      "Epoch [39/200], Batch [800/938], Loss D: 0.4977, Loss G: 1.8799\n",
      "Epoch [39/200], Batch [900/938], Loss D: 0.4103, Loss G: 1.7542\n",
      "Epoch [40/200], Batch [0/938], Loss D: 0.6467, Loss G: 1.3326\n",
      "Epoch [40/200], Batch [100/938], Loss D: 0.5914, Loss G: 1.6877\n",
      "Epoch [40/200], Batch [200/938], Loss D: 0.6939, Loss G: 1.5117\n",
      "Epoch [40/200], Batch [300/938], Loss D: 0.5147, Loss G: 1.4551\n",
      "Epoch [40/200], Batch [400/938], Loss D: 0.4468, Loss G: 1.6901\n",
      "Epoch [40/200], Batch [500/938], Loss D: 0.5103, Loss G: 1.5291\n",
      "Epoch [40/200], Batch [600/938], Loss D: 0.4298, Loss G: 1.8148\n",
      "Epoch [40/200], Batch [700/938], Loss D: 0.4316, Loss G: 1.7016\n",
      "Epoch [40/200], Batch [800/938], Loss D: 0.3263, Loss G: 2.0584\n",
      "Epoch [40/200], Batch [900/938], Loss D: 0.4179, Loss G: 1.7876\n",
      "Epoch [41/200], Batch [0/938], Loss D: 0.3805, Loss G: 2.0687\n",
      "Epoch [41/200], Batch [100/938], Loss D: 0.4670, Loss G: 1.5084\n",
      "Epoch [41/200], Batch [200/938], Loss D: 0.3955, Loss G: 1.6420\n",
      "Epoch [41/200], Batch [300/938], Loss D: 0.3417, Loss G: 2.0093\n",
      "Epoch [41/200], Batch [400/938], Loss D: 0.3309, Loss G: 1.8003\n",
      "Epoch [41/200], Batch [500/938], Loss D: 0.4383, Loss G: 1.6226\n",
      "Epoch [41/200], Batch [600/938], Loss D: 0.4286, Loss G: 1.5391\n",
      "Epoch [41/200], Batch [700/938], Loss D: 0.5734, Loss G: 1.5316\n",
      "Epoch [41/200], Batch [800/938], Loss D: 0.7076, Loss G: 1.1976\n",
      "Epoch [41/200], Batch [900/938], Loss D: 0.3801, Loss G: 1.7017\n",
      "Epoch [42/200], Batch [0/938], Loss D: 0.4149, Loss G: 1.7468\n",
      "Epoch [42/200], Batch [100/938], Loss D: 0.3677, Loss G: 1.7733\n",
      "Epoch [42/200], Batch [200/938], Loss D: 0.6451, Loss G: 1.1543\n",
      "Epoch [42/200], Batch [300/938], Loss D: 0.4707, Loss G: 1.3698\n",
      "Epoch [42/200], Batch [400/938], Loss D: 0.3654, Loss G: 1.5949\n",
      "Epoch [42/200], Batch [500/938], Loss D: 0.5175, Loss G: 1.2397\n",
      "Epoch [42/200], Batch [600/938], Loss D: 0.6005, Loss G: 1.2177\n",
      "Epoch [42/200], Batch [700/938], Loss D: 0.3172, Loss G: 2.1143\n",
      "Epoch [42/200], Batch [800/938], Loss D: 0.3773, Loss G: 1.6358\n",
      "Epoch [42/200], Batch [900/938], Loss D: 0.5675, Loss G: 1.6212\n",
      "Epoch [43/200], Batch [0/938], Loss D: 0.4557, Loss G: 1.8474\n",
      "Epoch [43/200], Batch [100/938], Loss D: 0.6445, Loss G: 1.0756\n",
      "Epoch [43/200], Batch [200/938], Loss D: 0.3536, Loss G: 1.8913\n",
      "Epoch [43/200], Batch [300/938], Loss D: 0.4630, Loss G: 1.8293\n",
      "Epoch [43/200], Batch [400/938], Loss D: 0.5082, Loss G: 1.3270\n",
      "Epoch [43/200], Batch [500/938], Loss D: 0.7203, Loss G: 1.0045\n",
      "Epoch [43/200], Batch [600/938], Loss D: 0.4347, Loss G: 1.7815\n",
      "Epoch [43/200], Batch [700/938], Loss D: 0.5005, Loss G: 1.8083\n",
      "Epoch [43/200], Batch [800/938], Loss D: 0.3795, Loss G: 1.8164\n",
      "Epoch [43/200], Batch [900/938], Loss D: 0.5483, Loss G: 1.5001\n",
      "Epoch [44/200], Batch [0/938], Loss D: 0.6054, Loss G: 1.5711\n",
      "Epoch [44/200], Batch [100/938], Loss D: 0.5505, Loss G: 1.7840\n",
      "Epoch [44/200], Batch [200/938], Loss D: 0.3838, Loss G: 1.5832\n",
      "Epoch [44/200], Batch [300/938], Loss D: 0.4246, Loss G: 1.5241\n",
      "Epoch [44/200], Batch [400/938], Loss D: 0.3640, Loss G: 1.6795\n",
      "Epoch [44/200], Batch [500/938], Loss D: 0.5169, Loss G: 1.4863\n",
      "Epoch [44/200], Batch [600/938], Loss D: 0.5953, Loss G: 1.3902\n",
      "Epoch [44/200], Batch [700/938], Loss D: 0.6007, Loss G: 1.7590\n",
      "Epoch [44/200], Batch [800/938], Loss D: 0.5766, Loss G: 1.4428\n",
      "Epoch [44/200], Batch [900/938], Loss D: 0.3913, Loss G: 1.6782\n",
      "Epoch [45/200], Batch [0/938], Loss D: 0.3279, Loss G: 1.8506\n",
      "Epoch [45/200], Batch [100/938], Loss D: 0.4457, Loss G: 1.8806\n",
      "Epoch [45/200], Batch [200/938], Loss D: 0.4238, Loss G: 1.6835\n",
      "Epoch [45/200], Batch [300/938], Loss D: 0.3513, Loss G: 2.0528\n",
      "Epoch [45/200], Batch [400/938], Loss D: 0.3172, Loss G: 1.7477\n",
      "Epoch [45/200], Batch [500/938], Loss D: 0.5136, Loss G: 1.4046\n",
      "Epoch [45/200], Batch [600/938], Loss D: 0.3738, Loss G: 1.5936\n",
      "Epoch [45/200], Batch [700/938], Loss D: 0.4586, Loss G: 1.9544\n",
      "Epoch [45/200], Batch [800/938], Loss D: 0.5138, Loss G: 1.7939\n",
      "Epoch [45/200], Batch [900/938], Loss D: 0.3778, Loss G: 1.8560\n",
      "Epoch [46/200], Batch [0/938], Loss D: 0.5247, Loss G: 1.5700\n",
      "Epoch [46/200], Batch [100/938], Loss D: 0.5107, Loss G: 1.4201\n",
      "Epoch [46/200], Batch [200/938], Loss D: 0.4890, Loss G: 1.4437\n",
      "Epoch [46/200], Batch [300/938], Loss D: 0.4710, Loss G: 1.7376\n",
      "Epoch [46/200], Batch [400/938], Loss D: 0.6489, Loss G: 1.2357\n",
      "Epoch [46/200], Batch [500/938], Loss D: 0.3926, Loss G: 1.9458\n",
      "Epoch [46/200], Batch [600/938], Loss D: 0.5803, Loss G: 1.2935\n",
      "Epoch [46/200], Batch [700/938], Loss D: 0.4951, Loss G: 1.4800\n",
      "Epoch [46/200], Batch [800/938], Loss D: 0.4919, Loss G: 1.6429\n",
      "Epoch [46/200], Batch [900/938], Loss D: 0.5173, Loss G: 1.8756\n",
      "Epoch [47/200], Batch [0/938], Loss D: 0.6313, Loss G: 1.2612\n",
      "Epoch [47/200], Batch [100/938], Loss D: 0.4329, Loss G: 1.5530\n",
      "Epoch [47/200], Batch [200/938], Loss D: 0.8630, Loss G: 1.1528\n",
      "Epoch [47/200], Batch [300/938], Loss D: 0.4187, Loss G: 1.7553\n",
      "Epoch [47/200], Batch [400/938], Loss D: 0.3882, Loss G: 1.7963\n",
      "Epoch [47/200], Batch [500/938], Loss D: 0.4518, Loss G: 1.7191\n",
      "Epoch [47/200], Batch [600/938], Loss D: 0.4687, Loss G: 1.6389\n",
      "Epoch [47/200], Batch [700/938], Loss D: 0.4887, Loss G: 1.2527\n",
      "Epoch [47/200], Batch [800/938], Loss D: 0.4788, Loss G: 1.3672\n",
      "Epoch [47/200], Batch [900/938], Loss D: 0.3322, Loss G: 1.9447\n",
      "Epoch [48/200], Batch [0/938], Loss D: 0.2515, Loss G: 2.2002\n",
      "Epoch [48/200], Batch [100/938], Loss D: 0.3183, Loss G: 1.8469\n",
      "Epoch [48/200], Batch [200/938], Loss D: 0.5604, Loss G: 1.2475\n",
      "Epoch [48/200], Batch [300/938], Loss D: 0.5488, Loss G: 1.4404\n",
      "Epoch [48/200], Batch [400/938], Loss D: 0.3525, Loss G: 1.9036\n",
      "Epoch [48/200], Batch [500/938], Loss D: 0.4950, Loss G: 1.8092\n",
      "Epoch [48/200], Batch [600/938], Loss D: 0.4239, Loss G: 1.7135\n",
      "Epoch [48/200], Batch [700/938], Loss D: 0.3347, Loss G: 1.9284\n",
      "Epoch [48/200], Batch [800/938], Loss D: 0.3349, Loss G: 1.8501\n",
      "Epoch [48/200], Batch [900/938], Loss D: 0.4269, Loss G: 1.4257\n",
      "Epoch [49/200], Batch [0/938], Loss D: 0.2922, Loss G: 2.2738\n",
      "Epoch [49/200], Batch [100/938], Loss D: 0.3246, Loss G: 1.9685\n",
      "Epoch [49/200], Batch [200/938], Loss D: 0.3378, Loss G: 2.1739\n",
      "Epoch [49/200], Batch [300/938], Loss D: 0.5029, Loss G: 1.5869\n",
      "Epoch [49/200], Batch [400/938], Loss D: 0.2685, Loss G: 2.1693\n",
      "Epoch [49/200], Batch [500/938], Loss D: 0.2885, Loss G: 2.0315\n",
      "Epoch [49/200], Batch [600/938], Loss D: 0.3919, Loss G: 1.6335\n",
      "Epoch [49/200], Batch [700/938], Loss D: 0.4359, Loss G: 1.9295\n",
      "Epoch [49/200], Batch [800/938], Loss D: 0.3132, Loss G: 1.9219\n",
      "Epoch [49/200], Batch [900/938], Loss D: 0.4558, Loss G: 1.3802\n",
      "Epoch [50/200], Batch [0/938], Loss D: 0.6398, Loss G: 1.1207\n",
      "Epoch [50/200], Batch [100/938], Loss D: 0.3628, Loss G: 1.5116\n",
      "Epoch [50/200], Batch [200/938], Loss D: 0.4677, Loss G: 1.4622\n",
      "Epoch [50/200], Batch [300/938], Loss D: 0.6128, Loss G: 1.6195\n",
      "Epoch [50/200], Batch [400/938], Loss D: 0.7695, Loss G: 1.2367\n",
      "Epoch [50/200], Batch [500/938], Loss D: 0.5048, Loss G: 1.5196\n",
      "Epoch [50/200], Batch [600/938], Loss D: 0.6166, Loss G: 1.6570\n",
      "Epoch [50/200], Batch [700/938], Loss D: 0.5578, Loss G: 1.2597\n",
      "Epoch [50/200], Batch [800/938], Loss D: 0.5782, Loss G: 1.4764\n",
      "Epoch [50/200], Batch [900/938], Loss D: 0.6067, Loss G: 1.5092\n",
      "Epoch [51/200], Batch [0/938], Loss D: 0.6126, Loss G: 1.6614\n",
      "Epoch [51/200], Batch [100/938], Loss D: 0.4739, Loss G: 1.8462\n",
      "Epoch [51/200], Batch [200/938], Loss D: 0.4524, Loss G: 2.0789\n",
      "Epoch [51/200], Batch [300/938], Loss D: 0.6934, Loss G: 1.2649\n",
      "Epoch [51/200], Batch [400/938], Loss D: 0.5963, Loss G: 1.4608\n",
      "Epoch [51/200], Batch [500/938], Loss D: 0.5831, Loss G: 1.4752\n",
      "Epoch [51/200], Batch [600/938], Loss D: 0.6210, Loss G: 1.5031\n",
      "Epoch [51/200], Batch [700/938], Loss D: 0.5369, Loss G: 1.6847\n",
      "Epoch [51/200], Batch [800/938], Loss D: 0.4036, Loss G: 2.0235\n",
      "Epoch [51/200], Batch [900/938], Loss D: 0.6190, Loss G: 1.3150\n",
      "Epoch [52/200], Batch [0/938], Loss D: 0.5731, Loss G: 1.3585\n",
      "Epoch [52/200], Batch [100/938], Loss D: 0.3643, Loss G: 2.3756\n",
      "Epoch [52/200], Batch [200/938], Loss D: 0.4156, Loss G: 1.9035\n",
      "Epoch [52/200], Batch [300/938], Loss D: 0.4916, Loss G: 1.7878\n",
      "Epoch [52/200], Batch [400/938], Loss D: 0.3188, Loss G: 2.0115\n",
      "Epoch [52/200], Batch [500/938], Loss D: 0.5866, Loss G: 1.5041\n",
      "Epoch [52/200], Batch [600/938], Loss D: 0.6361, Loss G: 1.8961\n",
      "Epoch [52/200], Batch [700/938], Loss D: 0.3622, Loss G: 1.8967\n",
      "Epoch [52/200], Batch [800/938], Loss D: 0.6178, Loss G: 1.4523\n",
      "Epoch [52/200], Batch [900/938], Loss D: 0.6746, Loss G: 1.4676\n",
      "Epoch [53/200], Batch [0/938], Loss D: 0.5070, Loss G: 1.7525\n",
      "Epoch [53/200], Batch [100/938], Loss D: 0.7402, Loss G: 1.0726\n",
      "Epoch [53/200], Batch [200/938], Loss D: 0.3601, Loss G: 1.7131\n",
      "Epoch [53/200], Batch [300/938], Loss D: 0.4339, Loss G: 1.7319\n",
      "Epoch [53/200], Batch [400/938], Loss D: 0.6358, Loss G: 1.3626\n",
      "Epoch [53/200], Batch [500/938], Loss D: 0.3932, Loss G: 1.7424\n",
      "Epoch [53/200], Batch [600/938], Loss D: 0.5103, Loss G: 1.8538\n",
      "Epoch [53/200], Batch [700/938], Loss D: 0.3928, Loss G: 1.8934\n",
      "Epoch [53/200], Batch [800/938], Loss D: 0.4786, Loss G: 1.7062\n",
      "Epoch [53/200], Batch [900/938], Loss D: 0.3818, Loss G: 1.7896\n",
      "Epoch [54/200], Batch [0/938], Loss D: 0.4780, Loss G: 1.8689\n",
      "Epoch [54/200], Batch [100/938], Loss D: 0.5200, Loss G: 1.5760\n",
      "Epoch [54/200], Batch [200/938], Loss D: 0.3925, Loss G: 1.8404\n",
      "Epoch [54/200], Batch [300/938], Loss D: 0.6273, Loss G: 1.0758\n",
      "Epoch [54/200], Batch [400/938], Loss D: 0.5204, Loss G: 1.7654\n",
      "Epoch [54/200], Batch [500/938], Loss D: 0.5416, Loss G: 1.7360\n",
      "Epoch [54/200], Batch [600/938], Loss D: 0.4060, Loss G: 1.9210\n",
      "Epoch [54/200], Batch [700/938], Loss D: 0.4820, Loss G: 1.5901\n",
      "Epoch [54/200], Batch [800/938], Loss D: 0.6565, Loss G: 1.2095\n",
      "Epoch [54/200], Batch [900/938], Loss D: 0.3975, Loss G: 1.8970\n",
      "Epoch [55/200], Batch [0/938], Loss D: 0.6794, Loss G: 1.6383\n",
      "Epoch [55/200], Batch [100/938], Loss D: 0.7365, Loss G: 1.4207\n",
      "Epoch [55/200], Batch [200/938], Loss D: 0.5145, Loss G: 1.5381\n",
      "Epoch [55/200], Batch [300/938], Loss D: 0.4504, Loss G: 1.7390\n",
      "Epoch [55/200], Batch [400/938], Loss D: 0.6669, Loss G: 1.7273\n",
      "Epoch [55/200], Batch [500/938], Loss D: 0.5693, Loss G: 1.8933\n",
      "Epoch [55/200], Batch [600/938], Loss D: 0.4844, Loss G: 1.7681\n",
      "Epoch [55/200], Batch [700/938], Loss D: 0.4593, Loss G: 1.9923\n",
      "Epoch [55/200], Batch [800/938], Loss D: 0.4596, Loss G: 1.8640\n",
      "Epoch [55/200], Batch [900/938], Loss D: 0.4949, Loss G: 1.4760\n",
      "Epoch [56/200], Batch [0/938], Loss D: 0.4793, Loss G: 1.3565\n",
      "Epoch [56/200], Batch [100/938], Loss D: 0.5292, Loss G: 1.9082\n",
      "Epoch [56/200], Batch [200/938], Loss D: 0.5060, Loss G: 2.0780\n",
      "Epoch [56/200], Batch [300/938], Loss D: 0.6526, Loss G: 1.4557\n",
      "Epoch [56/200], Batch [400/938], Loss D: 0.6563, Loss G: 1.5411\n",
      "Epoch [56/200], Batch [500/938], Loss D: 0.3522, Loss G: 1.9130\n",
      "Epoch [56/200], Batch [600/938], Loss D: 0.5140, Loss G: 1.7994\n",
      "Epoch [56/200], Batch [700/938], Loss D: 0.3823, Loss G: 1.7519\n",
      "Epoch [56/200], Batch [800/938], Loss D: 0.4462, Loss G: 1.6995\n",
      "Epoch [56/200], Batch [900/938], Loss D: 0.4078, Loss G: 1.4566\n",
      "Epoch [57/200], Batch [0/938], Loss D: 0.4905, Loss G: 1.8414\n",
      "Epoch [57/200], Batch [100/938], Loss D: 0.3275, Loss G: 2.1070\n",
      "Epoch [57/200], Batch [200/938], Loss D: 0.4398, Loss G: 1.9503\n",
      "Epoch [57/200], Batch [300/938], Loss D: 0.3369, Loss G: 2.0425\n",
      "Epoch [57/200], Batch [400/938], Loss D: 0.5506, Loss G: 1.5381\n",
      "Epoch [57/200], Batch [500/938], Loss D: 0.7456, Loss G: 1.5394\n",
      "Epoch [57/200], Batch [600/938], Loss D: 0.6157, Loss G: 1.2244\n",
      "Epoch [57/200], Batch [700/938], Loss D: 0.4741, Loss G: 1.9730\n",
      "Epoch [57/200], Batch [800/938], Loss D: 0.6405, Loss G: 1.9010\n",
      "Epoch [57/200], Batch [900/938], Loss D: 0.5467, Loss G: 1.3592\n",
      "Epoch [58/200], Batch [0/938], Loss D: 0.5278, Loss G: 1.7230\n",
      "Epoch [58/200], Batch [100/938], Loss D: 0.5326, Loss G: 1.9138\n",
      "Epoch [58/200], Batch [200/938], Loss D: 0.5740, Loss G: 1.9417\n",
      "Epoch [58/200], Batch [300/938], Loss D: 0.6105, Loss G: 1.7111\n",
      "Epoch [58/200], Batch [400/938], Loss D: 0.3909, Loss G: 2.0612\n",
      "Epoch [58/200], Batch [500/938], Loss D: 0.4644, Loss G: 1.7557\n",
      "Epoch [58/200], Batch [600/938], Loss D: 0.5030, Loss G: 1.5228\n",
      "Epoch [58/200], Batch [700/938], Loss D: 0.4102, Loss G: 2.2375\n",
      "Epoch [58/200], Batch [800/938], Loss D: 0.6807, Loss G: 1.5266\n",
      "Epoch [58/200], Batch [900/938], Loss D: 0.4314, Loss G: 1.7689\n",
      "Epoch [59/200], Batch [0/938], Loss D: 0.4444, Loss G: 2.0941\n",
      "Epoch [59/200], Batch [100/938], Loss D: 0.3732, Loss G: 1.9607\n",
      "Epoch [59/200], Batch [200/938], Loss D: 0.5112, Loss G: 1.8420\n",
      "Epoch [59/200], Batch [300/938], Loss D: 0.3293, Loss G: 1.9556\n",
      "Epoch [59/200], Batch [400/938], Loss D: 0.3706, Loss G: 1.9823\n",
      "Epoch [59/200], Batch [500/938], Loss D: 0.5064, Loss G: 1.7670\n",
      "Epoch [59/200], Batch [600/938], Loss D: 0.4953, Loss G: 1.6410\n",
      "Epoch [59/200], Batch [700/938], Loss D: 0.5036, Loss G: 2.0053\n",
      "Epoch [59/200], Batch [800/938], Loss D: 0.4267, Loss G: 1.8028\n",
      "Epoch [59/200], Batch [900/938], Loss D: 0.5736, Loss G: 1.9144\n",
      "Epoch [60/200], Batch [0/938], Loss D: 0.5092, Loss G: 1.6641\n",
      "Epoch [60/200], Batch [100/938], Loss D: 0.5912, Loss G: 1.8194\n",
      "Epoch [60/200], Batch [200/938], Loss D: 0.4375, Loss G: 1.9483\n",
      "Epoch [60/200], Batch [300/938], Loss D: 0.2444, Loss G: 2.3324\n",
      "Epoch [60/200], Batch [400/938], Loss D: 0.4604, Loss G: 1.4930\n",
      "Epoch [60/200], Batch [500/938], Loss D: 0.4425, Loss G: 1.8316\n",
      "Epoch [60/200], Batch [600/938], Loss D: 0.5326, Loss G: 1.7785\n",
      "Epoch [60/200], Batch [700/938], Loss D: 0.6911, Loss G: 1.4435\n",
      "Epoch [60/200], Batch [800/938], Loss D: 0.4518, Loss G: 1.8913\n",
      "Epoch [60/200], Batch [900/938], Loss D: 0.6079, Loss G: 2.1428\n",
      "Epoch [61/200], Batch [0/938], Loss D: 0.7963, Loss G: 1.4244\n",
      "Epoch [61/200], Batch [100/938], Loss D: 0.5092, Loss G: 1.7890\n",
      "Epoch [61/200], Batch [200/938], Loss D: 0.4414, Loss G: 1.8996\n",
      "Epoch [61/200], Batch [300/938], Loss D: 0.5494, Loss G: 1.6381\n",
      "Epoch [61/200], Batch [400/938], Loss D: 0.4261, Loss G: 1.8340\n",
      "Epoch [61/200], Batch [500/938], Loss D: 0.5372, Loss G: 1.8836\n",
      "Epoch [61/200], Batch [600/938], Loss D: 0.3770, Loss G: 1.5765\n",
      "Epoch [61/200], Batch [700/938], Loss D: 0.3760, Loss G: 2.0148\n",
      "Epoch [61/200], Batch [800/938], Loss D: 0.4384, Loss G: 1.8118\n",
      "Epoch [61/200], Batch [900/938], Loss D: 0.4256, Loss G: 1.9520\n",
      "Epoch [62/200], Batch [0/938], Loss D: 0.4582, Loss G: 1.9831\n",
      "Epoch [62/200], Batch [100/938], Loss D: 0.4263, Loss G: 1.7361\n",
      "Epoch [62/200], Batch [200/938], Loss D: 0.3625, Loss G: 2.5508\n",
      "Epoch [62/200], Batch [300/938], Loss D: 0.3430, Loss G: 2.4222\n",
      "Epoch [62/200], Batch [400/938], Loss D: 0.3595, Loss G: 2.3336\n",
      "Epoch [62/200], Batch [500/938], Loss D: 0.5685, Loss G: 1.4475\n",
      "Epoch [62/200], Batch [600/938], Loss D: 0.4362, Loss G: 1.9428\n",
      "Epoch [62/200], Batch [700/938], Loss D: 0.2990, Loss G: 1.9842\n",
      "Epoch [62/200], Batch [800/938], Loss D: 0.4829, Loss G: 1.9282\n",
      "Epoch [62/200], Batch [900/938], Loss D: 0.4664, Loss G: 1.8830\n",
      "Epoch [63/200], Batch [0/938], Loss D: 0.2928, Loss G: 2.0608\n",
      "Epoch [63/200], Batch [100/938], Loss D: 0.4247, Loss G: 1.8956\n",
      "Epoch [63/200], Batch [200/938], Loss D: 0.6177, Loss G: 1.6216\n",
      "Epoch [63/200], Batch [300/938], Loss D: 0.4755, Loss G: 2.2083\n",
      "Epoch [63/200], Batch [400/938], Loss D: 0.4545, Loss G: 1.8529\n",
      "Epoch [63/200], Batch [500/938], Loss D: 0.4956, Loss G: 1.9441\n",
      "Epoch [63/200], Batch [600/938], Loss D: 0.6327, Loss G: 1.5837\n",
      "Epoch [63/200], Batch [700/938], Loss D: 0.4976, Loss G: 1.7076\n",
      "Epoch [63/200], Batch [800/938], Loss D: 0.4557, Loss G: 1.9276\n",
      "Epoch [63/200], Batch [900/938], Loss D: 0.2492, Loss G: 2.8744\n",
      "Epoch [64/200], Batch [0/938], Loss D: 0.5735, Loss G: 1.7101\n",
      "Epoch [64/200], Batch [100/938], Loss D: 0.4174, Loss G: 1.9836\n",
      "Epoch [64/200], Batch [200/938], Loss D: 0.4880, Loss G: 1.5883\n",
      "Epoch [64/200], Batch [300/938], Loss D: 0.4960, Loss G: 1.9391\n",
      "Epoch [64/200], Batch [400/938], Loss D: 0.3844, Loss G: 2.3056\n",
      "Epoch [64/200], Batch [500/938], Loss D: 0.3985, Loss G: 1.8852\n",
      "Epoch [64/200], Batch [600/938], Loss D: 0.4446, Loss G: 1.5247\n",
      "Epoch [64/200], Batch [700/938], Loss D: 0.5728, Loss G: 1.4117\n",
      "Epoch [64/200], Batch [800/938], Loss D: 0.4662, Loss G: 1.6988\n",
      "Epoch [64/200], Batch [900/938], Loss D: 0.4212, Loss G: 1.5392\n",
      "Epoch [65/200], Batch [0/938], Loss D: 0.3248, Loss G: 2.2604\n",
      "Epoch [65/200], Batch [100/938], Loss D: 0.4579, Loss G: 2.1225\n",
      "Epoch [65/200], Batch [200/938], Loss D: 0.3645, Loss G: 2.4021\n",
      "Epoch [65/200], Batch [300/938], Loss D: 0.5597, Loss G: 1.7590\n",
      "Epoch [65/200], Batch [400/938], Loss D: 0.4629, Loss G: 2.3069\n",
      "Epoch [65/200], Batch [500/938], Loss D: 0.4969, Loss G: 2.1333\n",
      "Epoch [65/200], Batch [600/938], Loss D: 0.3113, Loss G: 2.2724\n",
      "Epoch [65/200], Batch [700/938], Loss D: 0.5664, Loss G: 1.9471\n",
      "Epoch [65/200], Batch [800/938], Loss D: 0.6077, Loss G: 1.5255\n",
      "Epoch [65/200], Batch [900/938], Loss D: 0.3481, Loss G: 2.3203\n",
      "Epoch [66/200], Batch [0/938], Loss D: 0.4434, Loss G: 1.9593\n",
      "Epoch [66/200], Batch [100/938], Loss D: 0.5758, Loss G: 1.6767\n",
      "Epoch [66/200], Batch [200/938], Loss D: 0.3642, Loss G: 2.2330\n",
      "Epoch [66/200], Batch [300/938], Loss D: 0.4247, Loss G: 1.9396\n",
      "Epoch [66/200], Batch [400/938], Loss D: 0.2394, Loss G: 2.7651\n",
      "Epoch [66/200], Batch [500/938], Loss D: 0.4720, Loss G: 2.0332\n",
      "Epoch [66/200], Batch [600/938], Loss D: 0.4309, Loss G: 1.7677\n",
      "Epoch [66/200], Batch [700/938], Loss D: 0.8672, Loss G: 0.9928\n",
      "Epoch [66/200], Batch [800/938], Loss D: 0.3354, Loss G: 2.0229\n",
      "Epoch [66/200], Batch [900/938], Loss D: 0.5425, Loss G: 1.6124\n",
      "Epoch [67/200], Batch [0/938], Loss D: 0.2662, Loss G: 2.4193\n",
      "Epoch [67/200], Batch [100/938], Loss D: 0.4680, Loss G: 1.5620\n",
      "Epoch [67/200], Batch [200/938], Loss D: 0.5162, Loss G: 2.0616\n",
      "Epoch [67/200], Batch [300/938], Loss D: 0.5414, Loss G: 1.7482\n",
      "Epoch [67/200], Batch [400/938], Loss D: 0.3392, Loss G: 2.0723\n",
      "Epoch [67/200], Batch [500/938], Loss D: 0.5827, Loss G: 1.5195\n",
      "Epoch [67/200], Batch [600/938], Loss D: 0.3734, Loss G: 1.8435\n",
      "Epoch [67/200], Batch [700/938], Loss D: 0.4575, Loss G: 1.8298\n",
      "Epoch [67/200], Batch [800/938], Loss D: 0.4737, Loss G: 1.6626\n",
      "Epoch [67/200], Batch [900/938], Loss D: 0.5411, Loss G: 1.2609\n",
      "Epoch [68/200], Batch [0/938], Loss D: 0.2820, Loss G: 2.1151\n",
      "Epoch [68/200], Batch [100/938], Loss D: 0.4531, Loss G: 1.7056\n",
      "Epoch [68/200], Batch [200/938], Loss D: 0.3901, Loss G: 1.6490\n",
      "Epoch [68/200], Batch [300/938], Loss D: 0.6382, Loss G: 1.4373\n",
      "Epoch [68/200], Batch [400/938], Loss D: 0.4055, Loss G: 2.2205\n",
      "Epoch [68/200], Batch [500/938], Loss D: 0.5498, Loss G: 1.8337\n",
      "Epoch [68/200], Batch [600/938], Loss D: 0.3788, Loss G: 1.7847\n",
      "Epoch [68/200], Batch [700/938], Loss D: 0.3527, Loss G: 1.7757\n",
      "Epoch [68/200], Batch [800/938], Loss D: 0.5021, Loss G: 1.8506\n",
      "Epoch [68/200], Batch [900/938], Loss D: 0.5930, Loss G: 1.3379\n",
      "Epoch [69/200], Batch [0/938], Loss D: 0.3598, Loss G: 1.9713\n",
      "Epoch [69/200], Batch [100/938], Loss D: 0.3435, Loss G: 2.3011\n",
      "Epoch [69/200], Batch [200/938], Loss D: 0.3569, Loss G: 2.2799\n",
      "Epoch [69/200], Batch [300/938], Loss D: 0.5281, Loss G: 1.6446\n",
      "Epoch [69/200], Batch [400/938], Loss D: 0.4974, Loss G: 1.6474\n",
      "Epoch [69/200], Batch [500/938], Loss D: 0.5109, Loss G: 2.0835\n",
      "Epoch [69/200], Batch [600/938], Loss D: 0.3761, Loss G: 1.9757\n",
      "Epoch [69/200], Batch [700/938], Loss D: 0.3793, Loss G: 1.6433\n",
      "Epoch [69/200], Batch [800/938], Loss D: 0.6245, Loss G: 1.7743\n",
      "Epoch [69/200], Batch [900/938], Loss D: 0.5816, Loss G: 1.6291\n",
      "Epoch [70/200], Batch [0/938], Loss D: 0.3436, Loss G: 2.0523\n",
      "Epoch [70/200], Batch [100/938], Loss D: 0.5755, Loss G: 1.7218\n",
      "Epoch [70/200], Batch [200/938], Loss D: 0.4978, Loss G: 1.9855\n",
      "Epoch [70/200], Batch [300/938], Loss D: 0.6548, Loss G: 1.5761\n",
      "Epoch [70/200], Batch [400/938], Loss D: 0.3982, Loss G: 1.5392\n",
      "Epoch [70/200], Batch [500/938], Loss D: 0.4666, Loss G: 1.6751\n",
      "Epoch [70/200], Batch [600/938], Loss D: 0.5132, Loss G: 1.9911\n",
      "Epoch [70/200], Batch [700/938], Loss D: 0.3666, Loss G: 1.7406\n",
      "Epoch [70/200], Batch [800/938], Loss D: 0.5695, Loss G: 1.5116\n",
      "Epoch [70/200], Batch [900/938], Loss D: 0.6221, Loss G: 1.4854\n",
      "Epoch [71/200], Batch [0/938], Loss D: 0.5830, Loss G: 1.7780\n",
      "Epoch [71/200], Batch [100/938], Loss D: 0.7154, Loss G: 1.5419\n",
      "Epoch [71/200], Batch [200/938], Loss D: 0.4572, Loss G: 2.1446\n",
      "Epoch [71/200], Batch [300/938], Loss D: 0.4804, Loss G: 1.9697\n",
      "Epoch [71/200], Batch [400/938], Loss D: 0.4882, Loss G: 1.9648\n",
      "Epoch [71/200], Batch [500/938], Loss D: 0.3918, Loss G: 2.1835\n",
      "Epoch [71/200], Batch [600/938], Loss D: 0.6009, Loss G: 1.7514\n",
      "Epoch [71/200], Batch [700/938], Loss D: 0.6229, Loss G: 1.6946\n",
      "Epoch [71/200], Batch [800/938], Loss D: 0.4679, Loss G: 2.0010\n",
      "Epoch [71/200], Batch [900/938], Loss D: 0.5227, Loss G: 1.8033\n",
      "Epoch [72/200], Batch [0/938], Loss D: 0.5640, Loss G: 1.3932\n",
      "Epoch [72/200], Batch [100/938], Loss D: 0.5486, Loss G: 1.5600\n",
      "Epoch [72/200], Batch [200/938], Loss D: 0.6698, Loss G: 1.2521\n",
      "Epoch [72/200], Batch [300/938], Loss D: 0.3359, Loss G: 2.2033\n",
      "Epoch [72/200], Batch [400/938], Loss D: 0.3393, Loss G: 1.9331\n",
      "Epoch [72/200], Batch [500/938], Loss D: 0.4405, Loss G: 2.0564\n",
      "Epoch [72/200], Batch [600/938], Loss D: 0.3861, Loss G: 1.6676\n",
      "Epoch [72/200], Batch [700/938], Loss D: 0.3829, Loss G: 1.9853\n",
      "Epoch [72/200], Batch [800/938], Loss D: 0.4502, Loss G: 1.7104\n",
      "Epoch [72/200], Batch [900/938], Loss D: 0.2726, Loss G: 2.5128\n",
      "Epoch [73/200], Batch [0/938], Loss D: 0.4952, Loss G: 1.7675\n",
      "Epoch [73/200], Batch [100/938], Loss D: 0.4383, Loss G: 1.7970\n",
      "Epoch [73/200], Batch [200/938], Loss D: 0.3768, Loss G: 2.0118\n",
      "Epoch [73/200], Batch [300/938], Loss D: 0.4448, Loss G: 1.9671\n",
      "Epoch [73/200], Batch [400/938], Loss D: 0.4840, Loss G: 1.6252\n",
      "Epoch [73/200], Batch [500/938], Loss D: 0.4409, Loss G: 1.6465\n",
      "Epoch [73/200], Batch [600/938], Loss D: 0.3810, Loss G: 1.9153\n",
      "Epoch [73/200], Batch [700/938], Loss D: 0.3928, Loss G: 1.5625\n",
      "Epoch [73/200], Batch [800/938], Loss D: 0.4831, Loss G: 1.8204\n",
      "Epoch [73/200], Batch [900/938], Loss D: 0.5024, Loss G: 1.5758\n",
      "Epoch [74/200], Batch [0/938], Loss D: 0.4241, Loss G: 1.5239\n",
      "Epoch [74/200], Batch [100/938], Loss D: 0.5910, Loss G: 2.0796\n",
      "Epoch [74/200], Batch [200/938], Loss D: 0.7081, Loss G: 1.4861\n",
      "Epoch [74/200], Batch [300/938], Loss D: 0.5610, Loss G: 1.7634\n",
      "Epoch [74/200], Batch [400/938], Loss D: 0.4268, Loss G: 1.9742\n",
      "Epoch [74/200], Batch [500/938], Loss D: 0.6091, Loss G: 1.4918\n",
      "Epoch [74/200], Batch [600/938], Loss D: 0.4947, Loss G: 1.6643\n",
      "Epoch [74/200], Batch [700/938], Loss D: 0.5221, Loss G: 1.6051\n",
      "Epoch [74/200], Batch [800/938], Loss D: 0.5735, Loss G: 1.2242\n",
      "Epoch [74/200], Batch [900/938], Loss D: 0.3438, Loss G: 2.0743\n",
      "Epoch [75/200], Batch [0/938], Loss D: 0.4626, Loss G: 1.5035\n",
      "Epoch [75/200], Batch [100/938], Loss D: 0.4138, Loss G: 1.5800\n",
      "Epoch [75/200], Batch [200/938], Loss D: 0.4612, Loss G: 1.4891\n",
      "Epoch [75/200], Batch [300/938], Loss D: 0.6852, Loss G: 1.6359\n",
      "Epoch [75/200], Batch [400/938], Loss D: 0.3952, Loss G: 1.5516\n",
      "Epoch [75/200], Batch [500/938], Loss D: 0.5208, Loss G: 1.7133\n",
      "Epoch [75/200], Batch [600/938], Loss D: 0.6826, Loss G: 1.5375\n",
      "Epoch [75/200], Batch [700/938], Loss D: 0.4257, Loss G: 1.9274\n",
      "Epoch [75/200], Batch [800/938], Loss D: 0.6070, Loss G: 1.2804\n",
      "Epoch [75/200], Batch [900/938], Loss D: 0.5313, Loss G: 1.6132\n",
      "Epoch [76/200], Batch [0/938], Loss D: 0.3987, Loss G: 1.8643\n",
      "Epoch [76/200], Batch [100/938], Loss D: 0.5655, Loss G: 1.7475\n",
      "Epoch [76/200], Batch [200/938], Loss D: 0.5328, Loss G: 1.5008\n",
      "Epoch [76/200], Batch [300/938], Loss D: 0.5285, Loss G: 1.7261\n",
      "Epoch [76/200], Batch [400/938], Loss D: 0.9244, Loss G: 1.6797\n",
      "Epoch [76/200], Batch [500/938], Loss D: 0.6169, Loss G: 1.4765\n",
      "Epoch [76/200], Batch [600/938], Loss D: 0.4766, Loss G: 1.8275\n",
      "Epoch [76/200], Batch [700/938], Loss D: 0.5570, Loss G: 1.5598\n",
      "Epoch [76/200], Batch [800/938], Loss D: 0.4918, Loss G: 1.6350\n",
      "Epoch [76/200], Batch [900/938], Loss D: 0.3597, Loss G: 1.8012\n",
      "Epoch [77/200], Batch [0/938], Loss D: 0.4536, Loss G: 1.7930\n",
      "Epoch [77/200], Batch [100/938], Loss D: 0.7329, Loss G: 1.1284\n",
      "Epoch [77/200], Batch [200/938], Loss D: 0.4189, Loss G: 1.6516\n",
      "Epoch [77/200], Batch [300/938], Loss D: 0.4208, Loss G: 1.3290\n",
      "Epoch [77/200], Batch [400/938], Loss D: 0.3829, Loss G: 2.0242\n",
      "Epoch [77/200], Batch [500/938], Loss D: 0.4851, Loss G: 2.0293\n",
      "Epoch [77/200], Batch [600/938], Loss D: 0.5584, Loss G: 1.6507\n",
      "Epoch [77/200], Batch [700/938], Loss D: 0.4665, Loss G: 2.0398\n",
      "Epoch [77/200], Batch [800/938], Loss D: 0.7057, Loss G: 1.3055\n",
      "Epoch [77/200], Batch [900/938], Loss D: 0.3929, Loss G: 1.8146\n",
      "Epoch [78/200], Batch [0/938], Loss D: 0.4255, Loss G: 2.0205\n",
      "Epoch [78/200], Batch [100/938], Loss D: 0.5706, Loss G: 1.3897\n",
      "Epoch [78/200], Batch [200/938], Loss D: 0.5836, Loss G: 1.3151\n",
      "Epoch [78/200], Batch [300/938], Loss D: 0.3880, Loss G: 2.0426\n",
      "Epoch [78/200], Batch [400/938], Loss D: 0.4448, Loss G: 1.6287\n",
      "Epoch [78/200], Batch [500/938], Loss D: 0.5166, Loss G: 1.5062\n",
      "Epoch [78/200], Batch [600/938], Loss D: 0.3202, Loss G: 2.1319\n",
      "Epoch [78/200], Batch [700/938], Loss D: 0.3592, Loss G: 2.1670\n",
      "Epoch [78/200], Batch [800/938], Loss D: 0.7533, Loss G: 1.2859\n",
      "Epoch [78/200], Batch [900/938], Loss D: 0.3668, Loss G: 1.8451\n",
      "Epoch [79/200], Batch [0/938], Loss D: 0.4429, Loss G: 1.7955\n",
      "Epoch [79/200], Batch [100/938], Loss D: 0.5457, Loss G: 1.4465\n",
      "Epoch [79/200], Batch [200/938], Loss D: 0.4117, Loss G: 1.9571\n",
      "Epoch [79/200], Batch [300/938], Loss D: 0.5646, Loss G: 1.4245\n",
      "Epoch [79/200], Batch [400/938], Loss D: 0.6485, Loss G: 1.5099\n",
      "Epoch [79/200], Batch [500/938], Loss D: 0.5009, Loss G: 1.3396\n",
      "Epoch [79/200], Batch [600/938], Loss D: 0.5400, Loss G: 1.4952\n",
      "Epoch [79/200], Batch [700/938], Loss D: 0.3764, Loss G: 1.8233\n",
      "Epoch [79/200], Batch [800/938], Loss D: 0.5024, Loss G: 1.5812\n",
      "Epoch [79/200], Batch [900/938], Loss D: 0.5380, Loss G: 1.4412\n",
      "Epoch [80/200], Batch [0/938], Loss D: 0.4760, Loss G: 1.3746\n",
      "Epoch [80/200], Batch [100/938], Loss D: 0.7822, Loss G: 1.0828\n",
      "Epoch [80/200], Batch [200/938], Loss D: 0.4719, Loss G: 1.5771\n",
      "Epoch [80/200], Batch [300/938], Loss D: 0.9117, Loss G: 1.0881\n",
      "Epoch [80/200], Batch [400/938], Loss D: 0.6049, Loss G: 1.3553\n",
      "Epoch [80/200], Batch [500/938], Loss D: 0.4803, Loss G: 1.6569\n",
      "Epoch [80/200], Batch [600/938], Loss D: 0.7737, Loss G: 1.1630\n",
      "Epoch [80/200], Batch [700/938], Loss D: 0.6326, Loss G: 1.4503\n",
      "Epoch [80/200], Batch [800/938], Loss D: 0.8324, Loss G: 1.0214\n",
      "Epoch [80/200], Batch [900/938], Loss D: 0.3867, Loss G: 1.7831\n",
      "Epoch [81/200], Batch [0/938], Loss D: 0.6126, Loss G: 1.5672\n",
      "Epoch [81/200], Batch [100/938], Loss D: 0.4289, Loss G: 1.8281\n",
      "Epoch [81/200], Batch [200/938], Loss D: 0.5050, Loss G: 1.3398\n",
      "Epoch [81/200], Batch [300/938], Loss D: 0.5909, Loss G: 1.6352\n",
      "Epoch [81/200], Batch [400/938], Loss D: 0.5216, Loss G: 1.7161\n",
      "Epoch [81/200], Batch [500/938], Loss D: 0.4652, Loss G: 1.8500\n",
      "Epoch [81/200], Batch [600/938], Loss D: 0.4852, Loss G: 1.8436\n",
      "Epoch [81/200], Batch [700/938], Loss D: 0.5831, Loss G: 1.3843\n",
      "Epoch [81/200], Batch [800/938], Loss D: 0.7510, Loss G: 1.1397\n",
      "Epoch [81/200], Batch [900/938], Loss D: 0.4400, Loss G: 1.7606\n",
      "Epoch [82/200], Batch [0/938], Loss D: 0.3672, Loss G: 2.2415\n",
      "Epoch [82/200], Batch [100/938], Loss D: 0.6522, Loss G: 1.7679\n",
      "Epoch [82/200], Batch [200/938], Loss D: 0.6762, Loss G: 1.2481\n",
      "Epoch [82/200], Batch [300/938], Loss D: 0.5731, Loss G: 1.7149\n",
      "Epoch [82/200], Batch [400/938], Loss D: 0.5547, Loss G: 1.6219\n",
      "Epoch [82/200], Batch [500/938], Loss D: 0.8038, Loss G: 1.2636\n",
      "Epoch [82/200], Batch [600/938], Loss D: 0.4239, Loss G: 1.5087\n",
      "Epoch [82/200], Batch [700/938], Loss D: 0.6051, Loss G: 1.5024\n",
      "Epoch [82/200], Batch [800/938], Loss D: 0.3832, Loss G: 2.0222\n",
      "Epoch [82/200], Batch [900/938], Loss D: 0.4548, Loss G: 1.6494\n",
      "Epoch [83/200], Batch [0/938], Loss D: 0.3231, Loss G: 2.0901\n",
      "Epoch [83/200], Batch [100/938], Loss D: 0.5130, Loss G: 1.5547\n",
      "Epoch [83/200], Batch [200/938], Loss D: 0.4067, Loss G: 1.6925\n",
      "Epoch [83/200], Batch [300/938], Loss D: 0.4026, Loss G: 1.9124\n",
      "Epoch [83/200], Batch [400/938], Loss D: 0.8142, Loss G: 1.6219\n",
      "Epoch [83/200], Batch [500/938], Loss D: 0.6288, Loss G: 1.5290\n",
      "Epoch [83/200], Batch [600/938], Loss D: 0.5299, Loss G: 1.5117\n",
      "Epoch [83/200], Batch [700/938], Loss D: 0.4882, Loss G: 1.4511\n",
      "Epoch [83/200], Batch [800/938], Loss D: 0.4727, Loss G: 1.6668\n",
      "Epoch [83/200], Batch [900/938], Loss D: 0.5866, Loss G: 1.7115\n",
      "Epoch [84/200], Batch [0/938], Loss D: 0.4903, Loss G: 1.6086\n",
      "Epoch [84/200], Batch [100/938], Loss D: 0.2896, Loss G: 2.1363\n",
      "Epoch [84/200], Batch [200/938], Loss D: 0.3535, Loss G: 2.0921\n",
      "Epoch [84/200], Batch [300/938], Loss D: 0.4708, Loss G: 1.7205\n",
      "Epoch [84/200], Batch [400/938], Loss D: 0.3296, Loss G: 1.7830\n",
      "Epoch [84/200], Batch [500/938], Loss D: 0.3099, Loss G: 2.0787\n",
      "Epoch [84/200], Batch [600/938], Loss D: 0.3049, Loss G: 2.1618\n",
      "Epoch [84/200], Batch [700/938], Loss D: 0.3932, Loss G: 1.8182\n",
      "Epoch [84/200], Batch [800/938], Loss D: 0.4945, Loss G: 1.7673\n",
      "Epoch [84/200], Batch [900/938], Loss D: 0.5341, Loss G: 1.5487\n",
      "Epoch [85/200], Batch [0/938], Loss D: 0.4368, Loss G: 1.8685\n",
      "Epoch [85/200], Batch [100/938], Loss D: 0.4633, Loss G: 1.4507\n",
      "Epoch [85/200], Batch [200/938], Loss D: 0.3589, Loss G: 1.9479\n",
      "Epoch [85/200], Batch [300/938], Loss D: 0.5799, Loss G: 1.2255\n",
      "Epoch [85/200], Batch [400/938], Loss D: 0.5322, Loss G: 1.5076\n",
      "Epoch [85/200], Batch [500/938], Loss D: 0.4536, Loss G: 1.7786\n",
      "Epoch [85/200], Batch [600/938], Loss D: 0.4844, Loss G: 1.5644\n",
      "Epoch [85/200], Batch [700/938], Loss D: 0.5903, Loss G: 1.6294\n",
      "Epoch [85/200], Batch [800/938], Loss D: 0.5601, Loss G: 1.0913\n",
      "Epoch [85/200], Batch [900/938], Loss D: 0.6183, Loss G: 1.2998\n",
      "Epoch [86/200], Batch [0/938], Loss D: 0.4943, Loss G: 1.5520\n",
      "Epoch [86/200], Batch [100/938], Loss D: 0.3402, Loss G: 2.0815\n",
      "Epoch [86/200], Batch [200/938], Loss D: 0.3055, Loss G: 1.8398\n",
      "Epoch [86/200], Batch [300/938], Loss D: 0.5514, Loss G: 1.5992\n",
      "Epoch [86/200], Batch [400/938], Loss D: 0.6612, Loss G: 1.4687\n",
      "Epoch [86/200], Batch [500/938], Loss D: 0.4391, Loss G: 1.6555\n",
      "Epoch [86/200], Batch [600/938], Loss D: 0.3385, Loss G: 2.4200\n",
      "Epoch [86/200], Batch [700/938], Loss D: 0.5345, Loss G: 1.7362\n",
      "Epoch [86/200], Batch [800/938], Loss D: 0.7262, Loss G: 1.0578\n",
      "Epoch [86/200], Batch [900/938], Loss D: 0.4744, Loss G: 1.4990\n",
      "Epoch [87/200], Batch [0/938], Loss D: 0.2557, Loss G: 1.9972\n",
      "Epoch [87/200], Batch [100/938], Loss D: 0.4265, Loss G: 1.7522\n",
      "Epoch [87/200], Batch [200/938], Loss D: 0.5107, Loss G: 1.4219\n",
      "Epoch [87/200], Batch [300/938], Loss D: 0.5944, Loss G: 1.4881\n",
      "Epoch [87/200], Batch [400/938], Loss D: 0.7033, Loss G: 1.3459\n",
      "Epoch [87/200], Batch [500/938], Loss D: 0.5694, Loss G: 1.2977\n",
      "Epoch [87/200], Batch [600/938], Loss D: 0.5428, Loss G: 1.5082\n",
      "Epoch [87/200], Batch [700/938], Loss D: 0.7036, Loss G: 1.3952\n",
      "Epoch [87/200], Batch [800/938], Loss D: 0.7370, Loss G: 1.2910\n",
      "Epoch [87/200], Batch [900/938], Loss D: 0.5038, Loss G: 1.5219\n",
      "Epoch [88/200], Batch [0/938], Loss D: 0.6585, Loss G: 1.4019\n",
      "Epoch [88/200], Batch [100/938], Loss D: 0.5668, Loss G: 1.3503\n",
      "Epoch [88/200], Batch [200/938], Loss D: 0.4172, Loss G: 1.6856\n",
      "Epoch [88/200], Batch [300/938], Loss D: 0.3958, Loss G: 1.6737\n",
      "Epoch [88/200], Batch [400/938], Loss D: 0.4523, Loss G: 1.4798\n",
      "Epoch [88/200], Batch [500/938], Loss D: 0.4219, Loss G: 1.5938\n",
      "Epoch [88/200], Batch [600/938], Loss D: 0.5113, Loss G: 1.6087\n",
      "Epoch [88/200], Batch [700/938], Loss D: 0.6285, Loss G: 1.2891\n",
      "Epoch [88/200], Batch [800/938], Loss D: 0.4698, Loss G: 1.7036\n",
      "Epoch [88/200], Batch [900/938], Loss D: 0.5664, Loss G: 1.4909\n",
      "Epoch [89/200], Batch [0/938], Loss D: 0.6327, Loss G: 1.4436\n",
      "Epoch [89/200], Batch [100/938], Loss D: 0.4344, Loss G: 1.7914\n",
      "Epoch [89/200], Batch [200/938], Loss D: 0.5556, Loss G: 1.5022\n",
      "Epoch [89/200], Batch [300/938], Loss D: 0.6492, Loss G: 1.2429\n",
      "Epoch [89/200], Batch [400/938], Loss D: 0.6561, Loss G: 1.3564\n",
      "Epoch [89/200], Batch [500/938], Loss D: 0.5291, Loss G: 1.5748\n",
      "Epoch [89/200], Batch [600/938], Loss D: 0.6536, Loss G: 1.4980\n",
      "Epoch [89/200], Batch [700/938], Loss D: 0.3780, Loss G: 1.6290\n",
      "Epoch [89/200], Batch [800/938], Loss D: 0.4244, Loss G: 1.5841\n",
      "Epoch [89/200], Batch [900/938], Loss D: 0.4204, Loss G: 1.5311\n",
      "Epoch [90/200], Batch [0/938], Loss D: 0.4186, Loss G: 1.6452\n",
      "Epoch [90/200], Batch [100/938], Loss D: 0.4133, Loss G: 1.8750\n",
      "Epoch [90/200], Batch [200/938], Loss D: 0.7323, Loss G: 1.2669\n",
      "Epoch [90/200], Batch [300/938], Loss D: 0.3446, Loss G: 1.8056\n",
      "Epoch [90/200], Batch [400/938], Loss D: 0.3726, Loss G: 1.8465\n",
      "Epoch [90/200], Batch [500/938], Loss D: 0.4174, Loss G: 1.7612\n",
      "Epoch [90/200], Batch [600/938], Loss D: 0.4976, Loss G: 1.7404\n",
      "Epoch [90/200], Batch [700/938], Loss D: 0.3298, Loss G: 1.9673\n",
      "Epoch [90/200], Batch [800/938], Loss D: 0.5131, Loss G: 1.5970\n",
      "Epoch [90/200], Batch [900/938], Loss D: 0.4691, Loss G: 1.7320\n",
      "Epoch [91/200], Batch [0/938], Loss D: 0.3538, Loss G: 2.0163\n",
      "Epoch [91/200], Batch [100/938], Loss D: 0.4692, Loss G: 1.7855\n",
      "Epoch [91/200], Batch [200/938], Loss D: 0.6482, Loss G: 1.5295\n",
      "Epoch [91/200], Batch [300/938], Loss D: 0.5783, Loss G: 1.9010\n",
      "Epoch [91/200], Batch [400/938], Loss D: 0.5909, Loss G: 1.4152\n",
      "Epoch [91/200], Batch [500/938], Loss D: 0.4636, Loss G: 1.7181\n",
      "Epoch [91/200], Batch [600/938], Loss D: 0.4554, Loss G: 1.8986\n",
      "Epoch [91/200], Batch [700/938], Loss D: 0.5058, Loss G: 1.5410\n",
      "Epoch [91/200], Batch [800/938], Loss D: 0.3494, Loss G: 2.2019\n",
      "Epoch [91/200], Batch [900/938], Loss D: 0.5411, Loss G: 1.6700\n",
      "Epoch [92/200], Batch [0/938], Loss D: 0.4671, Loss G: 1.7282\n",
      "Epoch [92/200], Batch [100/938], Loss D: 0.3711, Loss G: 1.7840\n",
      "Epoch [92/200], Batch [200/938], Loss D: 0.5507, Loss G: 1.4901\n",
      "Epoch [92/200], Batch [300/938], Loss D: 0.4936, Loss G: 1.4162\n",
      "Epoch [92/200], Batch [400/938], Loss D: 0.4032, Loss G: 1.8057\n",
      "Epoch [92/200], Batch [500/938], Loss D: 0.5056, Loss G: 1.9179\n",
      "Epoch [92/200], Batch [600/938], Loss D: 0.5212, Loss G: 1.8435\n",
      "Epoch [92/200], Batch [700/938], Loss D: 0.4397, Loss G: 1.8252\n",
      "Epoch [92/200], Batch [800/938], Loss D: 0.3092, Loss G: 2.3572\n",
      "Epoch [92/200], Batch [900/938], Loss D: 0.3443, Loss G: 2.3181\n",
      "Epoch [93/200], Batch [0/938], Loss D: 0.3905, Loss G: 1.6933\n",
      "Epoch [93/200], Batch [100/938], Loss D: 0.5482, Loss G: 1.6666\n",
      "Epoch [93/200], Batch [200/938], Loss D: 0.5301, Loss G: 1.8573\n",
      "Epoch [93/200], Batch [300/938], Loss D: 0.6192, Loss G: 1.9646\n",
      "Epoch [93/200], Batch [400/938], Loss D: 0.3696, Loss G: 2.4529\n",
      "Epoch [93/200], Batch [500/938], Loss D: 0.2698, Loss G: 2.4625\n",
      "Epoch [93/200], Batch [600/938], Loss D: 0.4301, Loss G: 1.6043\n",
      "Epoch [93/200], Batch [700/938], Loss D: 0.5969, Loss G: 1.3753\n",
      "Epoch [93/200], Batch [800/938], Loss D: 0.4112, Loss G: 1.6009\n",
      "Epoch [93/200], Batch [900/938], Loss D: 0.4011, Loss G: 1.5758\n",
      "Epoch [94/200], Batch [0/938], Loss D: 0.4339, Loss G: 1.6727\n",
      "Epoch [94/200], Batch [100/938], Loss D: 0.4138, Loss G: 1.6674\n",
      "Epoch [94/200], Batch [200/938], Loss D: 0.3372, Loss G: 1.8241\n",
      "Epoch [94/200], Batch [300/938], Loss D: 0.3759, Loss G: 1.7819\n",
      "Epoch [94/200], Batch [400/938], Loss D: 0.3483, Loss G: 2.0796\n",
      "Epoch [94/200], Batch [500/938], Loss D: 0.2317, Loss G: 2.4695\n",
      "Epoch [94/200], Batch [600/938], Loss D: 0.4215, Loss G: 1.6796\n",
      "Epoch [94/200], Batch [700/938], Loss D: 0.4987, Loss G: 1.6970\n",
      "Epoch [94/200], Batch [800/938], Loss D: 0.6071, Loss G: 1.7101\n",
      "Epoch [94/200], Batch [900/938], Loss D: 0.2927, Loss G: 1.9339\n",
      "Epoch [95/200], Batch [0/938], Loss D: 0.3690, Loss G: 2.0673\n",
      "Epoch [95/200], Batch [100/938], Loss D: 0.3393, Loss G: 1.9477\n",
      "Epoch [95/200], Batch [200/938], Loss D: 0.4688, Loss G: 1.8360\n",
      "Epoch [95/200], Batch [300/938], Loss D: 0.3473, Loss G: 2.5208\n",
      "Epoch [95/200], Batch [400/938], Loss D: 0.3468, Loss G: 1.7716\n",
      "Epoch [95/200], Batch [500/938], Loss D: 0.4055, Loss G: 1.6753\n",
      "Epoch [95/200], Batch [600/938], Loss D: 0.5450, Loss G: 1.5351\n",
      "Epoch [95/200], Batch [700/938], Loss D: 0.3471, Loss G: 2.1183\n",
      "Epoch [95/200], Batch [800/938], Loss D: 0.3540, Loss G: 1.6280\n",
      "Epoch [95/200], Batch [900/938], Loss D: 0.3292, Loss G: 2.2261\n",
      "Epoch [96/200], Batch [0/938], Loss D: 0.4765, Loss G: 1.5108\n",
      "Epoch [96/200], Batch [100/938], Loss D: 0.3524, Loss G: 2.0236\n",
      "Epoch [96/200], Batch [200/938], Loss D: 0.5684, Loss G: 1.4787\n",
      "Epoch [96/200], Batch [300/938], Loss D: 0.3451, Loss G: 2.0156\n",
      "Epoch [96/200], Batch [400/938], Loss D: 0.4497, Loss G: 2.0749\n",
      "Epoch [96/200], Batch [500/938], Loss D: 0.5635, Loss G: 1.7854\n",
      "Epoch [96/200], Batch [600/938], Loss D: 0.4551, Loss G: 1.8312\n",
      "Epoch [96/200], Batch [700/938], Loss D: 0.6367, Loss G: 1.7248\n",
      "Epoch [96/200], Batch [800/938], Loss D: 0.3990, Loss G: 1.5164\n",
      "Epoch [96/200], Batch [900/938], Loss D: 0.3792, Loss G: 1.9200\n",
      "Epoch [97/200], Batch [0/938], Loss D: 0.4144, Loss G: 1.8660\n",
      "Epoch [97/200], Batch [100/938], Loss D: 0.4822, Loss G: 1.5804\n",
      "Epoch [97/200], Batch [200/938], Loss D: 0.4987, Loss G: 1.6975\n",
      "Epoch [97/200], Batch [300/938], Loss D: 0.4754, Loss G: 1.5651\n",
      "Epoch [97/200], Batch [400/938], Loss D: 0.3289, Loss G: 1.9061\n",
      "Epoch [97/200], Batch [500/938], Loss D: 0.4741, Loss G: 1.4493\n",
      "Epoch [97/200], Batch [600/938], Loss D: 0.5861, Loss G: 1.3747\n",
      "Epoch [97/200], Batch [700/938], Loss D: 0.4328, Loss G: 1.4861\n",
      "Epoch [97/200], Batch [800/938], Loss D: 0.6504, Loss G: 1.3134\n",
      "Epoch [97/200], Batch [900/938], Loss D: 0.5931, Loss G: 1.6009\n",
      "Epoch [98/200], Batch [0/938], Loss D: 0.6035, Loss G: 1.3895\n",
      "Epoch [98/200], Batch [100/938], Loss D: 0.5361, Loss G: 1.6255\n",
      "Epoch [98/200], Batch [200/938], Loss D: 0.7206, Loss G: 1.1887\n",
      "Epoch [98/200], Batch [300/938], Loss D: 0.6586, Loss G: 1.4671\n",
      "Epoch [98/200], Batch [400/938], Loss D: 0.4601, Loss G: 1.8515\n",
      "Epoch [98/200], Batch [500/938], Loss D: 0.5723, Loss G: 1.4837\n",
      "Epoch [98/200], Batch [600/938], Loss D: 0.4982, Loss G: 1.5741\n",
      "Epoch [98/200], Batch [700/938], Loss D: 0.3558, Loss G: 1.6081\n",
      "Epoch [98/200], Batch [800/938], Loss D: 0.4012, Loss G: 1.7815\n",
      "Epoch [98/200], Batch [900/938], Loss D: 0.4769, Loss G: 1.4865\n",
      "Epoch [99/200], Batch [0/938], Loss D: 0.6316, Loss G: 1.6742\n",
      "Epoch [99/200], Batch [100/938], Loss D: 0.6400, Loss G: 1.5450\n",
      "Epoch [99/200], Batch [200/938], Loss D: 0.6345, Loss G: 1.2951\n",
      "Epoch [99/200], Batch [300/938], Loss D: 0.4611, Loss G: 1.7320\n",
      "Epoch [99/200], Batch [400/938], Loss D: 0.3375, Loss G: 1.7922\n",
      "Epoch [99/200], Batch [500/938], Loss D: 0.3317, Loss G: 1.9859\n",
      "Epoch [99/200], Batch [600/938], Loss D: 0.3993, Loss G: 2.0013\n",
      "Epoch [99/200], Batch [700/938], Loss D: 0.4920, Loss G: 1.5408\n",
      "Epoch [99/200], Batch [800/938], Loss D: 0.4347, Loss G: 2.0902\n",
      "Epoch [99/200], Batch [900/938], Loss D: 0.3073, Loss G: 2.3549\n",
      "Epoch [100/200], Batch [0/938], Loss D: 0.3614, Loss G: 2.1672\n",
      "Epoch [100/200], Batch [100/938], Loss D: 0.4608, Loss G: 1.5537\n",
      "Epoch [100/200], Batch [200/938], Loss D: 0.4642, Loss G: 1.5801\n",
      "Epoch [100/200], Batch [300/938], Loss D: 0.5543, Loss G: 1.5147\n",
      "Epoch [100/200], Batch [400/938], Loss D: 0.4671, Loss G: 1.6905\n",
      "Epoch [100/200], Batch [500/938], Loss D: 0.5054, Loss G: 1.7978\n",
      "Epoch [100/200], Batch [600/938], Loss D: 0.5601, Loss G: 1.3184\n",
      "Epoch [100/200], Batch [700/938], Loss D: 0.6233, Loss G: 1.3779\n",
      "Epoch [100/200], Batch [800/938], Loss D: 0.6940, Loss G: 1.5827\n",
      "Epoch [100/200], Batch [900/938], Loss D: 0.3637, Loss G: 2.0050\n",
      "Epoch [101/200], Batch [0/938], Loss D: 0.5129, Loss G: 1.5030\n",
      "Epoch [101/200], Batch [100/938], Loss D: 0.2963, Loss G: 2.4352\n",
      "Epoch [101/200], Batch [200/938], Loss D: 0.2457, Loss G: 2.3012\n",
      "Epoch [101/200], Batch [300/938], Loss D: 0.3194, Loss G: 1.8197\n",
      "Epoch [101/200], Batch [400/938], Loss D: 0.4087, Loss G: 1.5429\n",
      "Epoch [101/200], Batch [500/938], Loss D: 0.5146, Loss G: 1.2493\n",
      "Epoch [101/200], Batch [600/938], Loss D: 0.5808, Loss G: 1.5482\n",
      "Epoch [101/200], Batch [700/938], Loss D: 0.4900, Loss G: 1.3617\n",
      "Epoch [101/200], Batch [800/938], Loss D: 0.7352, Loss G: 1.4453\n",
      "Epoch [101/200], Batch [900/938], Loss D: 0.4641, Loss G: 1.8454\n",
      "Epoch [102/200], Batch [0/938], Loss D: 0.5746, Loss G: 1.7393\n",
      "Epoch [102/200], Batch [100/938], Loss D: 0.5472, Loss G: 1.4584\n",
      "Epoch [102/200], Batch [200/938], Loss D: 0.5789, Loss G: 1.5831\n",
      "Epoch [102/200], Batch [300/938], Loss D: 0.5291, Loss G: 1.3930\n",
      "Epoch [102/200], Batch [400/938], Loss D: 0.5466, Loss G: 1.4422\n",
      "Epoch [102/200], Batch [500/938], Loss D: 0.6048, Loss G: 1.1702\n",
      "Epoch [102/200], Batch [600/938], Loss D: 0.5835, Loss G: 1.4904\n",
      "Epoch [102/200], Batch [700/938], Loss D: 0.4230, Loss G: 1.9964\n",
      "Epoch [102/200], Batch [800/938], Loss D: 0.6837, Loss G: 1.4102\n",
      "Epoch [102/200], Batch [900/938], Loss D: 0.4405, Loss G: 1.5785\n",
      "Epoch [103/200], Batch [0/938], Loss D: 0.5228, Loss G: 1.4250\n",
      "Epoch [103/200], Batch [100/938], Loss D: 0.6460, Loss G: 1.1522\n",
      "Epoch [103/200], Batch [200/938], Loss D: 0.6757, Loss G: 1.6036\n",
      "Epoch [103/200], Batch [300/938], Loss D: 0.6671, Loss G: 1.4566\n",
      "Epoch [103/200], Batch [400/938], Loss D: 0.5073, Loss G: 1.9664\n",
      "Epoch [103/200], Batch [500/938], Loss D: 0.4973, Loss G: 1.5623\n",
      "Epoch [103/200], Batch [600/938], Loss D: 0.5294, Loss G: 1.3299\n",
      "Epoch [103/200], Batch [700/938], Loss D: 0.5443, Loss G: 1.7597\n",
      "Epoch [103/200], Batch [800/938], Loss D: 0.6407, Loss G: 1.8085\n",
      "Epoch [103/200], Batch [900/938], Loss D: 0.4882, Loss G: 1.5216\n",
      "Epoch [104/200], Batch [0/938], Loss D: 0.4704, Loss G: 1.8286\n",
      "Epoch [104/200], Batch [100/938], Loss D: 0.5576, Loss G: 1.6576\n",
      "Epoch [104/200], Batch [200/938], Loss D: 0.4509, Loss G: 1.8109\n",
      "Epoch [104/200], Batch [300/938], Loss D: 0.5828, Loss G: 1.4404\n",
      "Epoch [104/200], Batch [400/938], Loss D: 0.4046, Loss G: 1.5618\n",
      "Epoch [104/200], Batch [500/938], Loss D: 0.4893, Loss G: 1.4161\n",
      "Epoch [104/200], Batch [600/938], Loss D: 0.4945, Loss G: 1.5873\n",
      "Epoch [104/200], Batch [700/938], Loss D: 0.5352, Loss G: 1.3057\n",
      "Epoch [104/200], Batch [800/938], Loss D: 0.4358, Loss G: 2.0033\n",
      "Epoch [104/200], Batch [900/938], Loss D: 0.4677, Loss G: 1.7380\n",
      "Epoch [105/200], Batch [0/938], Loss D: 0.6139, Loss G: 1.3965\n",
      "Epoch [105/200], Batch [100/938], Loss D: 0.6337, Loss G: 1.4290\n",
      "Epoch [105/200], Batch [200/938], Loss D: 0.5023, Loss G: 1.7656\n",
      "Epoch [105/200], Batch [300/938], Loss D: 0.4195, Loss G: 1.8016\n",
      "Epoch [105/200], Batch [400/938], Loss D: 0.5019, Loss G: 1.5365\n",
      "Epoch [105/200], Batch [500/938], Loss D: 0.5788, Loss G: 1.6121\n",
      "Epoch [105/200], Batch [600/938], Loss D: 0.4115, Loss G: 1.6892\n",
      "Epoch [105/200], Batch [700/938], Loss D: 0.5147, Loss G: 1.5668\n",
      "Epoch [105/200], Batch [800/938], Loss D: 0.5143, Loss G: 1.5586\n",
      "Epoch [105/200], Batch [900/938], Loss D: 0.3402, Loss G: 2.1366\n",
      "Epoch [106/200], Batch [0/938], Loss D: 0.4361, Loss G: 1.8342\n",
      "Epoch [106/200], Batch [100/938], Loss D: 0.3656, Loss G: 1.9807\n",
      "Epoch [106/200], Batch [200/938], Loss D: 0.3556, Loss G: 1.7916\n",
      "Epoch [106/200], Batch [300/938], Loss D: 0.8254, Loss G: 1.0554\n",
      "Epoch [106/200], Batch [400/938], Loss D: 0.5855, Loss G: 1.0386\n",
      "Epoch [106/200], Batch [500/938], Loss D: 0.4249, Loss G: 1.7322\n",
      "Epoch [106/200], Batch [600/938], Loss D: 0.6063, Loss G: 1.4851\n",
      "Epoch [106/200], Batch [700/938], Loss D: 0.4744, Loss G: 2.0322\n",
      "Epoch [106/200], Batch [800/938], Loss D: 0.6779, Loss G: 1.4083\n",
      "Epoch [106/200], Batch [900/938], Loss D: 0.5892, Loss G: 1.4030\n",
      "Epoch [107/200], Batch [0/938], Loss D: 0.6121, Loss G: 1.5316\n",
      "Epoch [107/200], Batch [100/938], Loss D: 0.6063, Loss G: 1.5518\n",
      "Epoch [107/200], Batch [200/938], Loss D: 0.4472, Loss G: 1.8300\n",
      "Epoch [107/200], Batch [300/938], Loss D: 0.5860, Loss G: 1.3961\n",
      "Epoch [107/200], Batch [400/938], Loss D: 0.5200, Loss G: 1.6768\n",
      "Epoch [107/200], Batch [500/938], Loss D: 0.6305, Loss G: 1.5176\n",
      "Epoch [107/200], Batch [600/938], Loss D: 0.4511, Loss G: 1.6125\n",
      "Epoch [107/200], Batch [700/938], Loss D: 0.7509, Loss G: 1.1005\n",
      "Epoch [107/200], Batch [800/938], Loss D: 0.5003, Loss G: 1.7035\n",
      "Epoch [107/200], Batch [900/938], Loss D: 0.5922, Loss G: 1.5034\n",
      "Epoch [108/200], Batch [0/938], Loss D: 0.4112, Loss G: 2.0299\n",
      "Epoch [108/200], Batch [100/938], Loss D: 0.5254, Loss G: 1.7467\n",
      "Epoch [108/200], Batch [200/938], Loss D: 0.5500, Loss G: 1.5362\n",
      "Epoch [108/200], Batch [300/938], Loss D: 0.5780, Loss G: 1.5974\n",
      "Epoch [108/200], Batch [400/938], Loss D: 0.5620, Loss G: 1.7755\n",
      "Epoch [108/200], Batch [500/938], Loss D: 0.4320, Loss G: 2.0614\n",
      "Epoch [108/200], Batch [600/938], Loss D: 0.7647, Loss G: 1.3305\n",
      "Epoch [108/200], Batch [700/938], Loss D: 0.5593, Loss G: 1.7152\n",
      "Epoch [108/200], Batch [800/938], Loss D: 0.6313, Loss G: 1.5082\n",
      "Epoch [108/200], Batch [900/938], Loss D: 0.6325, Loss G: 1.6969\n",
      "Epoch [109/200], Batch [0/938], Loss D: 0.8205, Loss G: 1.2973\n",
      "Epoch [109/200], Batch [100/938], Loss D: 0.6910, Loss G: 1.2870\n",
      "Epoch [109/200], Batch [200/938], Loss D: 0.7176, Loss G: 1.2970\n",
      "Epoch [109/200], Batch [300/938], Loss D: 0.4930, Loss G: 1.7176\n",
      "Epoch [109/200], Batch [400/938], Loss D: 0.4651, Loss G: 1.9325\n",
      "Epoch [109/200], Batch [500/938], Loss D: 0.6102, Loss G: 1.4112\n",
      "Epoch [109/200], Batch [600/938], Loss D: 0.6556, Loss G: 1.6018\n",
      "Epoch [109/200], Batch [700/938], Loss D: 0.6189, Loss G: 1.2196\n",
      "Epoch [109/200], Batch [800/938], Loss D: 0.5632, Loss G: 1.7385\n",
      "Epoch [109/200], Batch [900/938], Loss D: 0.6490, Loss G: 1.4248\n",
      "Epoch [110/200], Batch [0/938], Loss D: 0.5470, Loss G: 1.2562\n",
      "Epoch [110/200], Batch [100/938], Loss D: 0.4741, Loss G: 1.4399\n",
      "Epoch [110/200], Batch [200/938], Loss D: 0.5187, Loss G: 1.6330\n",
      "Epoch [110/200], Batch [300/938], Loss D: 0.6625, Loss G: 1.1373\n",
      "Epoch [110/200], Batch [400/938], Loss D: 0.4720, Loss G: 1.8616\n",
      "Epoch [110/200], Batch [500/938], Loss D: 0.8268, Loss G: 1.2089\n",
      "Epoch [110/200], Batch [600/938], Loss D: 0.7369, Loss G: 1.5007\n",
      "Epoch [110/200], Batch [700/938], Loss D: 0.5642, Loss G: 1.3957\n",
      "Epoch [110/200], Batch [800/938], Loss D: 0.5495, Loss G: 1.6654\n",
      "Epoch [110/200], Batch [900/938], Loss D: 0.5121, Loss G: 1.7394\n",
      "Epoch [111/200], Batch [0/938], Loss D: 0.6921, Loss G: 1.1640\n",
      "Epoch [111/200], Batch [100/938], Loss D: 0.6788, Loss G: 1.1978\n",
      "Epoch [111/200], Batch [200/938], Loss D: 0.6894, Loss G: 1.2352\n",
      "Epoch [111/200], Batch [300/938], Loss D: 0.6075, Loss G: 1.4378\n",
      "Epoch [111/200], Batch [400/938], Loss D: 0.4833, Loss G: 1.5591\n",
      "Epoch [111/200], Batch [500/938], Loss D: 0.7807, Loss G: 1.3618\n",
      "Epoch [111/200], Batch [600/938], Loss D: 0.6100, Loss G: 1.3440\n",
      "Epoch [111/200], Batch [700/938], Loss D: 0.7562, Loss G: 1.6112\n",
      "Epoch [111/200], Batch [800/938], Loss D: 0.6383, Loss G: 1.1589\n",
      "Epoch [111/200], Batch [900/938], Loss D: 0.4364, Loss G: 1.9789\n",
      "Epoch [112/200], Batch [0/938], Loss D: 0.5616, Loss G: 1.3394\n",
      "Epoch [112/200], Batch [100/938], Loss D: 0.6429, Loss G: 1.2971\n",
      "Epoch [112/200], Batch [200/938], Loss D: 0.6287, Loss G: 1.3619\n",
      "Epoch [112/200], Batch [300/938], Loss D: 0.5251, Loss G: 1.3476\n",
      "Epoch [112/200], Batch [400/938], Loss D: 0.6419, Loss G: 1.1547\n",
      "Epoch [112/200], Batch [500/938], Loss D: 0.5242, Loss G: 1.5494\n",
      "Epoch [112/200], Batch [600/938], Loss D: 0.5456, Loss G: 1.4558\n",
      "Epoch [112/200], Batch [700/938], Loss D: 0.6610, Loss G: 1.2359\n",
      "Epoch [112/200], Batch [800/938], Loss D: 0.6994, Loss G: 1.2182\n",
      "Epoch [112/200], Batch [900/938], Loss D: 0.4339, Loss G: 1.7648\n",
      "Epoch [113/200], Batch [0/938], Loss D: 0.4668, Loss G: 1.6118\n",
      "Epoch [113/200], Batch [100/938], Loss D: 0.6772, Loss G: 1.3988\n",
      "Epoch [113/200], Batch [200/938], Loss D: 0.6455, Loss G: 1.4647\n",
      "Epoch [113/200], Batch [300/938], Loss D: 0.6923, Loss G: 1.1687\n",
      "Epoch [113/200], Batch [400/938], Loss D: 0.6115, Loss G: 1.0680\n",
      "Epoch [113/200], Batch [500/938], Loss D: 0.5225, Loss G: 1.5317\n",
      "Epoch [113/200], Batch [600/938], Loss D: 0.4227, Loss G: 1.8749\n",
      "Epoch [113/200], Batch [700/938], Loss D: 0.5363, Loss G: 1.5084\n",
      "Epoch [113/200], Batch [800/938], Loss D: 0.6174, Loss G: 1.1514\n",
      "Epoch [113/200], Batch [900/938], Loss D: 0.5715, Loss G: 1.3135\n",
      "Epoch [114/200], Batch [0/938], Loss D: 0.6767, Loss G: 1.1681\n",
      "Epoch [114/200], Batch [100/938], Loss D: 0.4011, Loss G: 1.7873\n",
      "Epoch [114/200], Batch [200/938], Loss D: 0.6903, Loss G: 1.3139\n",
      "Epoch [114/200], Batch [300/938], Loss D: 0.5101, Loss G: 1.4372\n",
      "Epoch [114/200], Batch [400/938], Loss D: 0.4979, Loss G: 1.3605\n",
      "Epoch [114/200], Batch [500/938], Loss D: 0.4792, Loss G: 1.3741\n",
      "Epoch [114/200], Batch [600/938], Loss D: 0.6154, Loss G: 1.0784\n",
      "Epoch [114/200], Batch [700/938], Loss D: 0.6174, Loss G: 1.2471\n",
      "Epoch [114/200], Batch [800/938], Loss D: 0.6356, Loss G: 1.3773\n",
      "Epoch [114/200], Batch [900/938], Loss D: 0.6524, Loss G: 1.2889\n",
      "Epoch [115/200], Batch [0/938], Loss D: 0.5971, Loss G: 1.5082\n",
      "Epoch [115/200], Batch [100/938], Loss D: 0.5695, Loss G: 1.3594\n",
      "Epoch [115/200], Batch [200/938], Loss D: 0.5094, Loss G: 1.6577\n",
      "Epoch [115/200], Batch [300/938], Loss D: 0.4687, Loss G: 1.8014\n",
      "Epoch [115/200], Batch [400/938], Loss D: 0.6238, Loss G: 1.0875\n",
      "Epoch [115/200], Batch [500/938], Loss D: 0.4172, Loss G: 1.5310\n",
      "Epoch [115/200], Batch [600/938], Loss D: 0.8046, Loss G: 1.0010\n",
      "Epoch [115/200], Batch [700/938], Loss D: 0.6625, Loss G: 0.9815\n",
      "Epoch [115/200], Batch [800/938], Loss D: 0.6962, Loss G: 1.0776\n",
      "Epoch [115/200], Batch [900/938], Loss D: 0.5276, Loss G: 1.3412\n",
      "Epoch [116/200], Batch [0/938], Loss D: 0.5962, Loss G: 1.2855\n",
      "Epoch [116/200], Batch [100/938], Loss D: 0.7283, Loss G: 1.0876\n",
      "Epoch [116/200], Batch [200/938], Loss D: 0.6039, Loss G: 1.0226\n",
      "Epoch [116/200], Batch [300/938], Loss D: 0.4731, Loss G: 1.4823\n",
      "Epoch [116/200], Batch [400/938], Loss D: 0.8463, Loss G: 1.0368\n",
      "Epoch [116/200], Batch [500/938], Loss D: 0.7367, Loss G: 1.0232\n",
      "Epoch [116/200], Batch [600/938], Loss D: 0.7000, Loss G: 1.3006\n",
      "Epoch [116/200], Batch [700/938], Loss D: 0.5823, Loss G: 1.1864\n",
      "Epoch [116/200], Batch [800/938], Loss D: 0.6911, Loss G: 1.1585\n",
      "Epoch [116/200], Batch [900/938], Loss D: 0.5771, Loss G: 1.0853\n",
      "Epoch [117/200], Batch [0/938], Loss D: 0.5450, Loss G: 1.3035\n",
      "Epoch [117/200], Batch [100/938], Loss D: 0.5185, Loss G: 1.3956\n",
      "Epoch [117/200], Batch [200/938], Loss D: 0.5560, Loss G: 1.3949\n",
      "Epoch [117/200], Batch [300/938], Loss D: 0.6806, Loss G: 1.2146\n",
      "Epoch [117/200], Batch [400/938], Loss D: 0.5767, Loss G: 1.3421\n",
      "Epoch [117/200], Batch [500/938], Loss D: 0.7729, Loss G: 0.8995\n",
      "Epoch [117/200], Batch [600/938], Loss D: 0.5514, Loss G: 1.1763\n",
      "Epoch [117/200], Batch [700/938], Loss D: 0.4602, Loss G: 1.5083\n",
      "Epoch [117/200], Batch [800/938], Loss D: 0.6422, Loss G: 1.4752\n",
      "Epoch [117/200], Batch [900/938], Loss D: 0.7441, Loss G: 1.1525\n",
      "Epoch [118/200], Batch [0/938], Loss D: 0.7416, Loss G: 0.9774\n",
      "Epoch [118/200], Batch [100/938], Loss D: 0.7355, Loss G: 1.0429\n",
      "Epoch [118/200], Batch [200/938], Loss D: 0.7780, Loss G: 1.2211\n",
      "Epoch [118/200], Batch [300/938], Loss D: 0.6820, Loss G: 1.1931\n",
      "Epoch [118/200], Batch [400/938], Loss D: 0.5577, Loss G: 1.3165\n",
      "Epoch [118/200], Batch [500/938], Loss D: 0.5319, Loss G: 1.3112\n",
      "Epoch [118/200], Batch [600/938], Loss D: 0.7195, Loss G: 1.2821\n",
      "Epoch [118/200], Batch [700/938], Loss D: 0.6935, Loss G: 1.0409\n",
      "Epoch [118/200], Batch [800/938], Loss D: 0.6784, Loss G: 1.1737\n",
      "Epoch [118/200], Batch [900/938], Loss D: 0.7413, Loss G: 0.9429\n",
      "Epoch [119/200], Batch [0/938], Loss D: 0.6037, Loss G: 1.1869\n",
      "Epoch [119/200], Batch [100/938], Loss D: 0.5089, Loss G: 1.4971\n",
      "Epoch [119/200], Batch [200/938], Loss D: 0.5512, Loss G: 1.1342\n",
      "Epoch [119/200], Batch [300/938], Loss D: 0.7044, Loss G: 1.1003\n",
      "Epoch [119/200], Batch [400/938], Loss D: 0.6050, Loss G: 0.9822\n",
      "Epoch [119/200], Batch [500/938], Loss D: 0.5526, Loss G: 1.1013\n",
      "Epoch [119/200], Batch [600/938], Loss D: 0.4815, Loss G: 1.2617\n",
      "Epoch [119/200], Batch [700/938], Loss D: 0.6624, Loss G: 1.0324\n",
      "Epoch [119/200], Batch [800/938], Loss D: 0.6060, Loss G: 1.2633\n",
      "Epoch [119/200], Batch [900/938], Loss D: 0.5920, Loss G: 1.1123\n",
      "Epoch [120/200], Batch [0/938], Loss D: 0.6961, Loss G: 1.0521\n",
      "Epoch [120/200], Batch [100/938], Loss D: 0.8227, Loss G: 0.7457\n",
      "Epoch [120/200], Batch [200/938], Loss D: 0.5880, Loss G: 1.2797\n",
      "Epoch [120/200], Batch [300/938], Loss D: 0.9363, Loss G: 0.7655\n",
      "Epoch [120/200], Batch [400/938], Loss D: 0.8596, Loss G: 1.0033\n",
      "Epoch [120/200], Batch [500/938], Loss D: 0.5385, Loss G: 1.2084\n",
      "Epoch [120/200], Batch [600/938], Loss D: 0.4525, Loss G: 1.7114\n",
      "Epoch [120/200], Batch [700/938], Loss D: 0.6601, Loss G: 0.9821\n",
      "Epoch [120/200], Batch [800/938], Loss D: 0.7176, Loss G: 1.0413\n",
      "Epoch [120/200], Batch [900/938], Loss D: 0.5928, Loss G: 1.1719\n",
      "Epoch [121/200], Batch [0/938], Loss D: 0.7449, Loss G: 1.1050\n",
      "Epoch [121/200], Batch [100/938], Loss D: 0.6678, Loss G: 1.1448\n",
      "Epoch [121/200], Batch [200/938], Loss D: 0.6574, Loss G: 1.0833\n",
      "Epoch [121/200], Batch [300/938], Loss D: 0.5745, Loss G: 1.1890\n",
      "Epoch [121/200], Batch [400/938], Loss D: 0.5956, Loss G: 1.4372\n",
      "Epoch [121/200], Batch [500/938], Loss D: 0.6686, Loss G: 1.1346\n",
      "Epoch [121/200], Batch [600/938], Loss D: 0.6672, Loss G: 1.1707\n",
      "Epoch [121/200], Batch [700/938], Loss D: 0.6422, Loss G: 1.2447\n",
      "Epoch [121/200], Batch [800/938], Loss D: 0.6221, Loss G: 1.4453\n",
      "Epoch [121/200], Batch [900/938], Loss D: 0.3991, Loss G: 1.5557\n",
      "Epoch [122/200], Batch [0/938], Loss D: 0.5637, Loss G: 1.2451\n",
      "Epoch [122/200], Batch [100/938], Loss D: 0.7995, Loss G: 0.9445\n",
      "Epoch [122/200], Batch [200/938], Loss D: 0.7270, Loss G: 1.0949\n",
      "Epoch [122/200], Batch [300/938], Loss D: 0.7445, Loss G: 1.0698\n",
      "Epoch [122/200], Batch [400/938], Loss D: 0.8951, Loss G: 1.0595\n",
      "Epoch [122/200], Batch [500/938], Loss D: 0.6080, Loss G: 1.3113\n",
      "Epoch [122/200], Batch [600/938], Loss D: 0.4943, Loss G: 1.4742\n",
      "Epoch [122/200], Batch [700/938], Loss D: 0.5075, Loss G: 1.6722\n",
      "Epoch [122/200], Batch [800/938], Loss D: 0.5286, Loss G: 1.4845\n",
      "Epoch [122/200], Batch [900/938], Loss D: 0.4878, Loss G: 1.6833\n",
      "Epoch [123/200], Batch [0/938], Loss D: 0.5477, Loss G: 1.3873\n",
      "Epoch [123/200], Batch [100/938], Loss D: 0.5029, Loss G: 1.5182\n",
      "Epoch [123/200], Batch [200/938], Loss D: 0.7092, Loss G: 1.2752\n",
      "Epoch [123/200], Batch [300/938], Loss D: 0.6859, Loss G: 1.2268\n",
      "Epoch [123/200], Batch [400/938], Loss D: 0.4654, Loss G: 1.5031\n",
      "Epoch [123/200], Batch [500/938], Loss D: 0.4922, Loss G: 1.4488\n",
      "Epoch [123/200], Batch [600/938], Loss D: 0.7621, Loss G: 1.0902\n",
      "Epoch [123/200], Batch [700/938], Loss D: 0.6775, Loss G: 1.2270\n",
      "Epoch [123/200], Batch [800/938], Loss D: 0.6927, Loss G: 1.1860\n",
      "Epoch [123/200], Batch [900/938], Loss D: 0.7015, Loss G: 1.2541\n",
      "Epoch [124/200], Batch [0/938], Loss D: 0.7301, Loss G: 1.0481\n",
      "Epoch [124/200], Batch [100/938], Loss D: 0.6216, Loss G: 1.3343\n",
      "Epoch [124/200], Batch [200/938], Loss D: 0.5928, Loss G: 1.2346\n",
      "Epoch [124/200], Batch [300/938], Loss D: 0.6581, Loss G: 1.1568\n",
      "Epoch [124/200], Batch [400/938], Loss D: 0.5524, Loss G: 1.2255\n",
      "Epoch [124/200], Batch [500/938], Loss D: 0.7263, Loss G: 0.9496\n",
      "Epoch [124/200], Batch [600/938], Loss D: 0.6863, Loss G: 1.0177\n",
      "Epoch [124/200], Batch [700/938], Loss D: 0.5138, Loss G: 1.3013\n",
      "Epoch [124/200], Batch [800/938], Loss D: 0.6298, Loss G: 1.2055\n",
      "Epoch [124/200], Batch [900/938], Loss D: 0.5238, Loss G: 1.2691\n",
      "Epoch [125/200], Batch [0/938], Loss D: 0.5032, Loss G: 1.4024\n",
      "Epoch [125/200], Batch [100/938], Loss D: 0.7775, Loss G: 1.1018\n",
      "Epoch [125/200], Batch [200/938], Loss D: 0.6090, Loss G: 1.2357\n",
      "Epoch [125/200], Batch [300/938], Loss D: 0.6239, Loss G: 1.5751\n",
      "Epoch [125/200], Batch [400/938], Loss D: 0.7098, Loss G: 1.1601\n",
      "Epoch [125/200], Batch [500/938], Loss D: 0.7625, Loss G: 1.0452\n",
      "Epoch [125/200], Batch [600/938], Loss D: 0.6753, Loss G: 1.0426\n",
      "Epoch [125/200], Batch [700/938], Loss D: 0.5646, Loss G: 0.9487\n",
      "Epoch [125/200], Batch [800/938], Loss D: 0.6164, Loss G: 1.0947\n",
      "Epoch [125/200], Batch [900/938], Loss D: 0.6193, Loss G: 1.1553\n",
      "Epoch [126/200], Batch [0/938], Loss D: 0.6327, Loss G: 0.9021\n",
      "Epoch [126/200], Batch [100/938], Loss D: 0.6320, Loss G: 1.0655\n",
      "Epoch [126/200], Batch [200/938], Loss D: 0.5909, Loss G: 1.2027\n",
      "Epoch [126/200], Batch [300/938], Loss D: 0.5693, Loss G: 1.1574\n",
      "Epoch [126/200], Batch [400/938], Loss D: 0.7094, Loss G: 1.2229\n",
      "Epoch [126/200], Batch [500/938], Loss D: 0.5301, Loss G: 1.3210\n",
      "Epoch [126/200], Batch [600/938], Loss D: 0.6711, Loss G: 1.0632\n",
      "Epoch [126/200], Batch [700/938], Loss D: 0.6631, Loss G: 1.2808\n",
      "Epoch [126/200], Batch [800/938], Loss D: 0.6098, Loss G: 1.1663\n",
      "Epoch [126/200], Batch [900/938], Loss D: 0.5820, Loss G: 1.3225\n",
      "Epoch [127/200], Batch [0/938], Loss D: 0.5933, Loss G: 1.3106\n",
      "Epoch [127/200], Batch [100/938], Loss D: 0.6241, Loss G: 1.1456\n",
      "Epoch [127/200], Batch [200/938], Loss D: 0.7317, Loss G: 0.9931\n",
      "Epoch [127/200], Batch [300/938], Loss D: 0.5508, Loss G: 1.1440\n",
      "Epoch [127/200], Batch [400/938], Loss D: 0.5651, Loss G: 1.3839\n",
      "Epoch [127/200], Batch [500/938], Loss D: 0.7101, Loss G: 1.1094\n",
      "Epoch [127/200], Batch [600/938], Loss D: 0.6759, Loss G: 1.0016\n",
      "Epoch [127/200], Batch [700/938], Loss D: 0.6861, Loss G: 1.0852\n",
      "Epoch [127/200], Batch [800/938], Loss D: 0.6574, Loss G: 0.9842\n",
      "Epoch [127/200], Batch [900/938], Loss D: 0.6479, Loss G: 1.0602\n",
      "Epoch [128/200], Batch [0/938], Loss D: 0.7075, Loss G: 1.1404\n",
      "Epoch [128/200], Batch [100/938], Loss D: 0.6970, Loss G: 1.1270\n",
      "Epoch [128/200], Batch [200/938], Loss D: 0.5812, Loss G: 1.3498\n",
      "Epoch [128/200], Batch [300/938], Loss D: 0.5547, Loss G: 1.1889\n",
      "Epoch [128/200], Batch [400/938], Loss D: 0.5519, Loss G: 1.3579\n",
      "Epoch [128/200], Batch [500/938], Loss D: 0.5895, Loss G: 1.4865\n",
      "Epoch [128/200], Batch [600/938], Loss D: 0.4887, Loss G: 1.4903\n",
      "Epoch [128/200], Batch [700/938], Loss D: 0.5851, Loss G: 1.2489\n",
      "Epoch [128/200], Batch [800/938], Loss D: 0.6186, Loss G: 1.1728\n",
      "Epoch [128/200], Batch [900/938], Loss D: 0.6361, Loss G: 1.1947\n",
      "Epoch [129/200], Batch [0/938], Loss D: 0.6489, Loss G: 1.0865\n",
      "Epoch [129/200], Batch [100/938], Loss D: 0.5624, Loss G: 1.1907\n",
      "Epoch [129/200], Batch [200/938], Loss D: 0.5657, Loss G: 1.1360\n",
      "Epoch [129/200], Batch [300/938], Loss D: 0.6108, Loss G: 0.9915\n",
      "Epoch [129/200], Batch [400/938], Loss D: 0.4423, Loss G: 1.3411\n",
      "Epoch [129/200], Batch [500/938], Loss D: 0.6395, Loss G: 1.0746\n",
      "Epoch [129/200], Batch [600/938], Loss D: 0.5477, Loss G: 1.3849\n",
      "Epoch [129/200], Batch [700/938], Loss D: 0.6114, Loss G: 1.2772\n",
      "Epoch [129/200], Batch [800/938], Loss D: 0.5368, Loss G: 1.4907\n",
      "Epoch [129/200], Batch [900/938], Loss D: 0.4599, Loss G: 1.6763\n",
      "Epoch [130/200], Batch [0/938], Loss D: 0.5545, Loss G: 1.3870\n",
      "Epoch [130/200], Batch [100/938], Loss D: 0.5271, Loss G: 1.3514\n",
      "Epoch [130/200], Batch [200/938], Loss D: 0.7327, Loss G: 1.0006\n",
      "Epoch [130/200], Batch [300/938], Loss D: 0.5414, Loss G: 1.4687\n",
      "Epoch [130/200], Batch [400/938], Loss D: 0.8206, Loss G: 0.9394\n",
      "Epoch [130/200], Batch [500/938], Loss D: 0.5019, Loss G: 1.3749\n",
      "Epoch [130/200], Batch [600/938], Loss D: 0.7232, Loss G: 1.0138\n",
      "Epoch [130/200], Batch [700/938], Loss D: 0.6005, Loss G: 1.1220\n",
      "Epoch [130/200], Batch [800/938], Loss D: 0.5866, Loss G: 1.0861\n",
      "Epoch [130/200], Batch [900/938], Loss D: 0.5614, Loss G: 1.1138\n",
      "Epoch [131/200], Batch [0/938], Loss D: 0.6149, Loss G: 1.2325\n",
      "Epoch [131/200], Batch [100/938], Loss D: 0.5911, Loss G: 1.2539\n",
      "Epoch [131/200], Batch [200/938], Loss D: 0.6527, Loss G: 0.9790\n",
      "Epoch [131/200], Batch [300/938], Loss D: 0.5402, Loss G: 1.3830\n",
      "Epoch [131/200], Batch [400/938], Loss D: 0.6193, Loss G: 1.0645\n",
      "Epoch [131/200], Batch [500/938], Loss D: 0.8031, Loss G: 1.0196\n",
      "Epoch [131/200], Batch [600/938], Loss D: 0.7151, Loss G: 1.1984\n",
      "Epoch [131/200], Batch [700/938], Loss D: 0.5587, Loss G: 1.3120\n",
      "Epoch [131/200], Batch [800/938], Loss D: 0.6611, Loss G: 1.0588\n",
      "Epoch [131/200], Batch [900/938], Loss D: 0.7546, Loss G: 0.9049\n",
      "Epoch [132/200], Batch [0/938], Loss D: 0.7151, Loss G: 0.9188\n",
      "Epoch [132/200], Batch [100/938], Loss D: 0.6264, Loss G: 1.0586\n",
      "Epoch [132/200], Batch [200/938], Loss D: 0.5864, Loss G: 1.0699\n",
      "Epoch [132/200], Batch [300/938], Loss D: 0.5941, Loss G: 1.1162\n",
      "Epoch [132/200], Batch [400/938], Loss D: 0.5738, Loss G: 1.1685\n",
      "Epoch [132/200], Batch [500/938], Loss D: 0.7742, Loss G: 0.8459\n",
      "Epoch [132/200], Batch [600/938], Loss D: 0.4393, Loss G: 1.4484\n",
      "Epoch [132/200], Batch [700/938], Loss D: 0.6438, Loss G: 1.2039\n",
      "Epoch [132/200], Batch [800/938], Loss D: 0.5480, Loss G: 1.2890\n",
      "Epoch [132/200], Batch [900/938], Loss D: 0.6486, Loss G: 1.0513\n",
      "Epoch [133/200], Batch [0/938], Loss D: 0.6370, Loss G: 1.4248\n",
      "Epoch [133/200], Batch [100/938], Loss D: 0.6408, Loss G: 1.1452\n",
      "Epoch [133/200], Batch [200/938], Loss D: 0.5994, Loss G: 1.1556\n",
      "Epoch [133/200], Batch [300/938], Loss D: 0.6871, Loss G: 1.2125\n",
      "Epoch [133/200], Batch [400/938], Loss D: 0.4625, Loss G: 1.4553\n",
      "Epoch [133/200], Batch [500/938], Loss D: 0.6249, Loss G: 1.0943\n",
      "Epoch [133/200], Batch [600/938], Loss D: 0.7631, Loss G: 0.9527\n",
      "Epoch [133/200], Batch [700/938], Loss D: 0.6014, Loss G: 1.0329\n",
      "Epoch [133/200], Batch [800/938], Loss D: 0.6184, Loss G: 1.1556\n",
      "Epoch [133/200], Batch [900/938], Loss D: 0.8028, Loss G: 1.1250\n",
      "Epoch [134/200], Batch [0/938], Loss D: 0.6774, Loss G: 1.0819\n",
      "Epoch [134/200], Batch [100/938], Loss D: 0.5362, Loss G: 1.2863\n",
      "Epoch [134/200], Batch [200/938], Loss D: 0.5485, Loss G: 1.3490\n",
      "Epoch [134/200], Batch [300/938], Loss D: 0.9050, Loss G: 0.7136\n",
      "Epoch [134/200], Batch [400/938], Loss D: 0.8254, Loss G: 0.9088\n",
      "Epoch [134/200], Batch [500/938], Loss D: 0.5549, Loss G: 1.1402\n",
      "Epoch [134/200], Batch [600/938], Loss D: 0.6650, Loss G: 1.0496\n",
      "Epoch [134/200], Batch [700/938], Loss D: 0.6437, Loss G: 1.0248\n",
      "Epoch [134/200], Batch [800/938], Loss D: 0.6314, Loss G: 1.0683\n",
      "Epoch [134/200], Batch [900/938], Loss D: 0.7809, Loss G: 0.9607\n",
      "Epoch [135/200], Batch [0/938], Loss D: 0.8068, Loss G: 0.8593\n",
      "Epoch [135/200], Batch [100/938], Loss D: 0.8165, Loss G: 0.8894\n",
      "Epoch [135/200], Batch [200/938], Loss D: 0.7153, Loss G: 0.9131\n",
      "Epoch [135/200], Batch [300/938], Loss D: 0.6019, Loss G: 1.2022\n",
      "Epoch [135/200], Batch [400/938], Loss D: 0.5876, Loss G: 1.0954\n",
      "Epoch [135/200], Batch [500/938], Loss D: 0.6436, Loss G: 0.9853\n",
      "Epoch [135/200], Batch [600/938], Loss D: 0.6922, Loss G: 1.1393\n",
      "Epoch [135/200], Batch [700/938], Loss D: 0.7139, Loss G: 1.0317\n",
      "Epoch [135/200], Batch [800/938], Loss D: 0.8479, Loss G: 0.7261\n",
      "Epoch [135/200], Batch [900/938], Loss D: 0.6632, Loss G: 1.0997\n",
      "Epoch [136/200], Batch [0/938], Loss D: 0.5303, Loss G: 1.1689\n",
      "Epoch [136/200], Batch [100/938], Loss D: 0.6997, Loss G: 0.9642\n",
      "Epoch [136/200], Batch [200/938], Loss D: 0.7642, Loss G: 1.0774\n",
      "Epoch [136/200], Batch [300/938], Loss D: 0.6167, Loss G: 1.1380\n",
      "Epoch [136/200], Batch [400/938], Loss D: 0.4582, Loss G: 1.3776\n",
      "Epoch [136/200], Batch [500/938], Loss D: 0.6934, Loss G: 1.1352\n",
      "Epoch [136/200], Batch [600/938], Loss D: 0.8367, Loss G: 0.9403\n",
      "Epoch [136/200], Batch [700/938], Loss D: 0.6228, Loss G: 1.0485\n",
      "Epoch [136/200], Batch [800/938], Loss D: 0.6585, Loss G: 0.9974\n",
      "Epoch [136/200], Batch [900/938], Loss D: 0.5825, Loss G: 0.9859\n",
      "Epoch [137/200], Batch [0/938], Loss D: 0.8450, Loss G: 0.8006\n",
      "Epoch [137/200], Batch [100/938], Loss D: 0.6262, Loss G: 1.0196\n",
      "Epoch [137/200], Batch [200/938], Loss D: 0.6955, Loss G: 1.0040\n",
      "Epoch [137/200], Batch [300/938], Loss D: 0.7322, Loss G: 1.0013\n",
      "Epoch [137/200], Batch [400/938], Loss D: 0.6723, Loss G: 1.2286\n",
      "Epoch [137/200], Batch [500/938], Loss D: 0.6846, Loss G: 0.9278\n",
      "Epoch [137/200], Batch [600/938], Loss D: 0.6542, Loss G: 1.1470\n",
      "Epoch [137/200], Batch [700/938], Loss D: 0.5965, Loss G: 1.0476\n",
      "Epoch [137/200], Batch [800/938], Loss D: 0.5962, Loss G: 1.0847\n",
      "Epoch [137/200], Batch [900/938], Loss D: 0.7409, Loss G: 0.9481\n",
      "Epoch [138/200], Batch [0/938], Loss D: 0.7254, Loss G: 0.9366\n",
      "Epoch [138/200], Batch [100/938], Loss D: 0.7142, Loss G: 0.9135\n",
      "Epoch [138/200], Batch [200/938], Loss D: 0.7099, Loss G: 0.8615\n",
      "Epoch [138/200], Batch [300/938], Loss D: 0.4105, Loss G: 1.3767\n",
      "Epoch [138/200], Batch [400/938], Loss D: 0.6132, Loss G: 1.1137\n",
      "Epoch [138/200], Batch [500/938], Loss D: 0.7365, Loss G: 0.9946\n",
      "Epoch [138/200], Batch [600/938], Loss D: 0.6849, Loss G: 0.8969\n",
      "Epoch [138/200], Batch [700/938], Loss D: 0.5835, Loss G: 1.1563\n",
      "Epoch [138/200], Batch [800/938], Loss D: 0.7122, Loss G: 0.9273\n",
      "Epoch [138/200], Batch [900/938], Loss D: 0.7067, Loss G: 1.0196\n",
      "Epoch [139/200], Batch [0/938], Loss D: 0.4857, Loss G: 1.1540\n",
      "Epoch [139/200], Batch [100/938], Loss D: 0.5575, Loss G: 1.0833\n",
      "Epoch [139/200], Batch [200/938], Loss D: 0.5893, Loss G: 1.0660\n",
      "Epoch [139/200], Batch [300/938], Loss D: 0.6989, Loss G: 0.9645\n",
      "Epoch [139/200], Batch [400/938], Loss D: 0.6666, Loss G: 0.9482\n",
      "Epoch [139/200], Batch [500/938], Loss D: 0.5797, Loss G: 1.0477\n",
      "Epoch [139/200], Batch [600/938], Loss D: 0.6884, Loss G: 1.0803\n",
      "Epoch [139/200], Batch [700/938], Loss D: 0.6321, Loss G: 0.9324\n",
      "Epoch [139/200], Batch [800/938], Loss D: 0.6547, Loss G: 0.9421\n",
      "Epoch [139/200], Batch [900/938], Loss D: 0.6376, Loss G: 0.9755\n",
      "Epoch [140/200], Batch [0/938], Loss D: 0.6203, Loss G: 1.0444\n",
      "Epoch [140/200], Batch [100/938], Loss D: 0.5859, Loss G: 0.9311\n",
      "Epoch [140/200], Batch [200/938], Loss D: 0.7692, Loss G: 0.8884\n",
      "Epoch [140/200], Batch [300/938], Loss D: 0.6007, Loss G: 0.9640\n",
      "Epoch [140/200], Batch [400/938], Loss D: 0.6565, Loss G: 1.0036\n",
      "Epoch [140/200], Batch [500/938], Loss D: 0.7366, Loss G: 0.9465\n",
      "Epoch [140/200], Batch [600/938], Loss D: 0.7205, Loss G: 1.0734\n",
      "Epoch [140/200], Batch [700/938], Loss D: 0.7584, Loss G: 0.8224\n",
      "Epoch [140/200], Batch [800/938], Loss D: 0.7266, Loss G: 0.8331\n",
      "Epoch [140/200], Batch [900/938], Loss D: 0.6870, Loss G: 0.9094\n",
      "Epoch [141/200], Batch [0/938], Loss D: 0.7179, Loss G: 1.0054\n",
      "Epoch [141/200], Batch [100/938], Loss D: 0.6983, Loss G: 0.9982\n",
      "Epoch [141/200], Batch [200/938], Loss D: 0.7429, Loss G: 0.8796\n",
      "Epoch [141/200], Batch [300/938], Loss D: 0.5035, Loss G: 1.1974\n",
      "Epoch [141/200], Batch [400/938], Loss D: 0.7171, Loss G: 0.7996\n",
      "Epoch [141/200], Batch [500/938], Loss D: 0.7598, Loss G: 0.8070\n",
      "Epoch [141/200], Batch [600/938], Loss D: 0.6555, Loss G: 1.0156\n",
      "Epoch [141/200], Batch [700/938], Loss D: 0.6318, Loss G: 0.9838\n",
      "Epoch [141/200], Batch [800/938], Loss D: 0.7526, Loss G: 0.8620\n",
      "Epoch [141/200], Batch [900/938], Loss D: 0.5524, Loss G: 1.0296\n",
      "Epoch [142/200], Batch [0/938], Loss D: 0.6597, Loss G: 0.8567\n",
      "Epoch [142/200], Batch [100/938], Loss D: 0.6840, Loss G: 0.8524\n",
      "Epoch [142/200], Batch [200/938], Loss D: 0.5691, Loss G: 1.0423\n",
      "Epoch [142/200], Batch [300/938], Loss D: 0.6816, Loss G: 0.8214\n",
      "Epoch [142/200], Batch [400/938], Loss D: 0.5538, Loss G: 0.9448\n",
      "Epoch [142/200], Batch [500/938], Loss D: 0.6246, Loss G: 0.9420\n",
      "Epoch [142/200], Batch [600/938], Loss D: 0.5904, Loss G: 1.0134\n",
      "Epoch [142/200], Batch [700/938], Loss D: 0.7431, Loss G: 0.8730\n",
      "Epoch [142/200], Batch [800/938], Loss D: 0.7235, Loss G: 0.8208\n",
      "Epoch [142/200], Batch [900/938], Loss D: 0.6019, Loss G: 0.9994\n",
      "Epoch [143/200], Batch [0/938], Loss D: 0.6771, Loss G: 0.9371\n",
      "Epoch [143/200], Batch [100/938], Loss D: 0.5868, Loss G: 0.9464\n",
      "Epoch [143/200], Batch [200/938], Loss D: 0.5938, Loss G: 1.0891\n",
      "Epoch [143/200], Batch [300/938], Loss D: 0.7512, Loss G: 0.7975\n",
      "Epoch [143/200], Batch [400/938], Loss D: 0.7151, Loss G: 0.8126\n",
      "Epoch [143/200], Batch [500/938], Loss D: 0.5934, Loss G: 1.0160\n",
      "Epoch [143/200], Batch [600/938], Loss D: 0.6180, Loss G: 0.9345\n",
      "Epoch [143/200], Batch [700/938], Loss D: 0.6055, Loss G: 0.9313\n",
      "Epoch [143/200], Batch [800/938], Loss D: 0.6644, Loss G: 0.9408\n",
      "Epoch [143/200], Batch [900/938], Loss D: 0.7156, Loss G: 0.7894\n",
      "Epoch [144/200], Batch [0/938], Loss D: 0.5452, Loss G: 0.9927\n",
      "Epoch [144/200], Batch [100/938], Loss D: 0.6709, Loss G: 0.8395\n",
      "Epoch [144/200], Batch [200/938], Loss D: 0.5587, Loss G: 0.9877\n",
      "Epoch [144/200], Batch [300/938], Loss D: 0.5859, Loss G: 1.0091\n",
      "Epoch [144/200], Batch [400/938], Loss D: 0.7103, Loss G: 0.8600\n",
      "Epoch [144/200], Batch [500/938], Loss D: 0.6023, Loss G: 0.9021\n",
      "Epoch [144/200], Batch [600/938], Loss D: 0.6554, Loss G: 0.8737\n",
      "Epoch [144/200], Batch [700/938], Loss D: 0.5788, Loss G: 1.0154\n",
      "Epoch [144/200], Batch [800/938], Loss D: 0.6979, Loss G: 0.8852\n",
      "Epoch [144/200], Batch [900/938], Loss D: 0.6961, Loss G: 1.0842\n",
      "Epoch [145/200], Batch [0/938], Loss D: 0.5984, Loss G: 1.1257\n",
      "Epoch [145/200], Batch [100/938], Loss D: 0.5813, Loss G: 1.1310\n",
      "Epoch [145/200], Batch [200/938], Loss D: 0.6725, Loss G: 0.9186\n",
      "Epoch [145/200], Batch [300/938], Loss D: 0.6104, Loss G: 1.0520\n",
      "Epoch [145/200], Batch [400/938], Loss D: 0.6190, Loss G: 0.9114\n",
      "Epoch [145/200], Batch [500/938], Loss D: 0.5993, Loss G: 0.9744\n",
      "Epoch [145/200], Batch [600/938], Loss D: 0.6639, Loss G: 0.8889\n",
      "Epoch [145/200], Batch [700/938], Loss D: 0.6558, Loss G: 0.9481\n",
      "Epoch [145/200], Batch [800/938], Loss D: 0.5905, Loss G: 1.1062\n",
      "Epoch [145/200], Batch [900/938], Loss D: 0.8357, Loss G: 0.7127\n",
      "Epoch [146/200], Batch [0/938], Loss D: 0.7350, Loss G: 0.9126\n",
      "Epoch [146/200], Batch [100/938], Loss D: 0.5776, Loss G: 0.9767\n",
      "Epoch [146/200], Batch [200/938], Loss D: 0.5565, Loss G: 0.9979\n",
      "Epoch [146/200], Batch [300/938], Loss D: 0.5997, Loss G: 0.9514\n",
      "Epoch [146/200], Batch [400/938], Loss D: 0.6621, Loss G: 0.9342\n",
      "Epoch [146/200], Batch [500/938], Loss D: 0.8547, Loss G: 0.6999\n",
      "Epoch [146/200], Batch [600/938], Loss D: 0.6517, Loss G: 0.8606\n",
      "Epoch [146/200], Batch [700/938], Loss D: 0.7236, Loss G: 0.8238\n",
      "Epoch [146/200], Batch [800/938], Loss D: 0.6639, Loss G: 0.9013\n",
      "Epoch [146/200], Batch [900/938], Loss D: 0.5601, Loss G: 1.1103\n",
      "Epoch [147/200], Batch [0/938], Loss D: 0.6469, Loss G: 0.8838\n",
      "Epoch [147/200], Batch [100/938], Loss D: 0.6537, Loss G: 0.8618\n",
      "Epoch [147/200], Batch [200/938], Loss D: 0.6734, Loss G: 0.8883\n",
      "Epoch [147/200], Batch [300/938], Loss D: 0.6700, Loss G: 0.8081\n",
      "Epoch [147/200], Batch [400/938], Loss D: 0.7661, Loss G: 0.7393\n",
      "Epoch [147/200], Batch [500/938], Loss D: 0.6193, Loss G: 0.9820\n",
      "Epoch [147/200], Batch [600/938], Loss D: 0.7395, Loss G: 0.8982\n",
      "Epoch [147/200], Batch [700/938], Loss D: 0.7012, Loss G: 0.8838\n",
      "Epoch [147/200], Batch [800/938], Loss D: 0.6239, Loss G: 0.9206\n",
      "Epoch [147/200], Batch [900/938], Loss D: 0.6013, Loss G: 0.9765\n",
      "Epoch [148/200], Batch [0/938], Loss D: 0.6874, Loss G: 0.8707\n",
      "Epoch [148/200], Batch [100/938], Loss D: 0.6827, Loss G: 0.8855\n",
      "Epoch [148/200], Batch [200/938], Loss D: 0.6260, Loss G: 0.9379\n",
      "Epoch [148/200], Batch [300/938], Loss D: 0.6495, Loss G: 0.9739\n",
      "Epoch [148/200], Batch [400/938], Loss D: 0.6446, Loss G: 0.9555\n",
      "Epoch [148/200], Batch [500/938], Loss D: 0.6738, Loss G: 0.8361\n",
      "Epoch [148/200], Batch [600/938], Loss D: 0.6449, Loss G: 0.8550\n",
      "Epoch [148/200], Batch [700/938], Loss D: 0.7894, Loss G: 0.7291\n",
      "Epoch [148/200], Batch [800/938], Loss D: 0.6221, Loss G: 1.0164\n",
      "Epoch [148/200], Batch [900/938], Loss D: 0.7017, Loss G: 0.8334\n",
      "Epoch [149/200], Batch [0/938], Loss D: 0.6504, Loss G: 0.8690\n",
      "Epoch [149/200], Batch [100/938], Loss D: 0.8327, Loss G: 0.6786\n",
      "Epoch [149/200], Batch [200/938], Loss D: 0.7002, Loss G: 0.8530\n",
      "Epoch [149/200], Batch [300/938], Loss D: 0.6502, Loss G: 0.9262\n",
      "Epoch [149/200], Batch [400/938], Loss D: 0.7537, Loss G: 0.8633\n",
      "Epoch [149/200], Batch [500/938], Loss D: 0.6619, Loss G: 0.9030\n",
      "Epoch [149/200], Batch [600/938], Loss D: 0.6988, Loss G: 0.8453\n",
      "Epoch [149/200], Batch [700/938], Loss D: 0.7291, Loss G: 0.7647\n",
      "Epoch [149/200], Batch [800/938], Loss D: 0.5804, Loss G: 1.0465\n",
      "Epoch [149/200], Batch [900/938], Loss D: 0.5891, Loss G: 0.9842\n",
      "Epoch [150/200], Batch [0/938], Loss D: 0.5965, Loss G: 1.0081\n",
      "Epoch [150/200], Batch [100/938], Loss D: 0.5753, Loss G: 1.0347\n",
      "Epoch [150/200], Batch [200/938], Loss D: 0.7118, Loss G: 0.8907\n",
      "Epoch [150/200], Batch [300/938], Loss D: 0.6446, Loss G: 0.9601\n",
      "Epoch [150/200], Batch [400/938], Loss D: 0.5283, Loss G: 1.0438\n",
      "Epoch [150/200], Batch [500/938], Loss D: 0.5454, Loss G: 1.0901\n",
      "Epoch [150/200], Batch [600/938], Loss D: 0.6051, Loss G: 1.0824\n",
      "Epoch [150/200], Batch [700/938], Loss D: 0.5871, Loss G: 0.9250\n",
      "Epoch [150/200], Batch [800/938], Loss D: 0.7508, Loss G: 0.9168\n",
      "Epoch [150/200], Batch [900/938], Loss D: 0.6739, Loss G: 0.8706\n",
      "Epoch [151/200], Batch [0/938], Loss D: 0.7287, Loss G: 0.7616\n",
      "Epoch [151/200], Batch [100/938], Loss D: 0.6634, Loss G: 0.8615\n",
      "Epoch [151/200], Batch [200/938], Loss D: 0.5785, Loss G: 1.0572\n",
      "Epoch [151/200], Batch [300/938], Loss D: 0.5236, Loss G: 1.0349\n",
      "Epoch [151/200], Batch [400/938], Loss D: 0.7665, Loss G: 0.7966\n",
      "Epoch [151/200], Batch [500/938], Loss D: 0.8423, Loss G: 0.7182\n",
      "Epoch [151/200], Batch [600/938], Loss D: 0.6694, Loss G: 0.9276\n",
      "Epoch [151/200], Batch [700/938], Loss D: 0.6314, Loss G: 0.9560\n",
      "Epoch [151/200], Batch [800/938], Loss D: 0.6133, Loss G: 1.0404\n",
      "Epoch [151/200], Batch [900/938], Loss D: 0.6296, Loss G: 0.8951\n",
      "Epoch [152/200], Batch [0/938], Loss D: 0.6640, Loss G: 0.8331\n",
      "Epoch [152/200], Batch [100/938], Loss D: 0.6849, Loss G: 0.7641\n",
      "Epoch [152/200], Batch [200/938], Loss D: 0.7245, Loss G: 0.7183\n",
      "Epoch [152/200], Batch [300/938], Loss D: 0.6263, Loss G: 0.9505\n",
      "Epoch [152/200], Batch [400/938], Loss D: 0.6709, Loss G: 0.7604\n",
      "Epoch [152/200], Batch [500/938], Loss D: 0.7463, Loss G: 0.6894\n",
      "Epoch [152/200], Batch [600/938], Loss D: 0.8861, Loss G: 0.6732\n",
      "Epoch [152/200], Batch [700/938], Loss D: 0.6307, Loss G: 0.8770\n",
      "Epoch [152/200], Batch [800/938], Loss D: 0.6632, Loss G: 0.8611\n",
      "Epoch [152/200], Batch [900/938], Loss D: 0.5892, Loss G: 0.8643\n",
      "Epoch [153/200], Batch [0/938], Loss D: 0.6117, Loss G: 0.9723\n",
      "Epoch [153/200], Batch [100/938], Loss D: 0.6680, Loss G: 0.9917\n",
      "Epoch [153/200], Batch [200/938], Loss D: 0.6537, Loss G: 0.8932\n",
      "Epoch [153/200], Batch [300/938], Loss D: 0.7140, Loss G: 0.7652\n",
      "Epoch [153/200], Batch [400/938], Loss D: 0.7242, Loss G: 0.7699\n",
      "Epoch [153/200], Batch [500/938], Loss D: 0.6636, Loss G: 0.7761\n",
      "Epoch [153/200], Batch [600/938], Loss D: 0.7209, Loss G: 0.7854\n",
      "Epoch [153/200], Batch [700/938], Loss D: 0.8429, Loss G: 0.7693\n",
      "Epoch [153/200], Batch [800/938], Loss D: 0.7405, Loss G: 0.7813\n",
      "Epoch [153/200], Batch [900/938], Loss D: 0.6151, Loss G: 0.9677\n",
      "Epoch [154/200], Batch [0/938], Loss D: 0.5639, Loss G: 1.0956\n",
      "Epoch [154/200], Batch [100/938], Loss D: 0.6497, Loss G: 0.8481\n",
      "Epoch [154/200], Batch [200/938], Loss D: 0.6586, Loss G: 0.8098\n",
      "Epoch [154/200], Batch [300/938], Loss D: 0.5844, Loss G: 0.9217\n",
      "Epoch [154/200], Batch [400/938], Loss D: 0.6147, Loss G: 0.9184\n",
      "Epoch [154/200], Batch [500/938], Loss D: 0.6670, Loss G: 0.8273\n",
      "Epoch [154/200], Batch [600/938], Loss D: 0.7205, Loss G: 0.8205\n",
      "Epoch [154/200], Batch [700/938], Loss D: 0.6086, Loss G: 0.8938\n",
      "Epoch [154/200], Batch [800/938], Loss D: 0.7553, Loss G: 0.8164\n",
      "Epoch [154/200], Batch [900/938], Loss D: 0.5808, Loss G: 0.9144\n",
      "Epoch [155/200], Batch [0/938], Loss D: 0.6563, Loss G: 0.8805\n",
      "Epoch [155/200], Batch [100/938], Loss D: 0.7943, Loss G: 0.7709\n",
      "Epoch [155/200], Batch [200/938], Loss D: 0.6918, Loss G: 0.8522\n",
      "Epoch [155/200], Batch [300/938], Loss D: 0.7905, Loss G: 0.8064\n",
      "Epoch [155/200], Batch [400/938], Loss D: 0.7602, Loss G: 0.7346\n",
      "Epoch [155/200], Batch [500/938], Loss D: 0.6952, Loss G: 0.7887\n",
      "Epoch [155/200], Batch [600/938], Loss D: 0.7989, Loss G: 0.7251\n",
      "Epoch [155/200], Batch [700/938], Loss D: 0.6717, Loss G: 0.8458\n",
      "Epoch [155/200], Batch [800/938], Loss D: 0.7092, Loss G: 0.7746\n",
      "Epoch [155/200], Batch [900/938], Loss D: 0.5756, Loss G: 0.9422\n",
      "Epoch [156/200], Batch [0/938], Loss D: 0.6230, Loss G: 0.9031\n",
      "Epoch [156/200], Batch [100/938], Loss D: 0.6760, Loss G: 0.7976\n",
      "Epoch [156/200], Batch [200/938], Loss D: 0.6386, Loss G: 0.9646\n",
      "Epoch [156/200], Batch [300/938], Loss D: 0.6999, Loss G: 0.8786\n",
      "Epoch [156/200], Batch [400/938], Loss D: 0.7181, Loss G: 0.7607\n",
      "Epoch [156/200], Batch [500/938], Loss D: 0.7155, Loss G: 0.7637\n",
      "Epoch [156/200], Batch [600/938], Loss D: 0.5769, Loss G: 0.9833\n",
      "Epoch [156/200], Batch [700/938], Loss D: 0.7536, Loss G: 0.6629\n",
      "Epoch [156/200], Batch [800/938], Loss D: 0.6488, Loss G: 0.9087\n",
      "Epoch [156/200], Batch [900/938], Loss D: 0.7197, Loss G: 0.7117\n",
      "Epoch [157/200], Batch [0/938], Loss D: 0.6570, Loss G: 0.8904\n",
      "Epoch [157/200], Batch [100/938], Loss D: 0.7219, Loss G: 0.7897\n",
      "Epoch [157/200], Batch [200/938], Loss D: 0.6814, Loss G: 0.8365\n",
      "Epoch [157/200], Batch [300/938], Loss D: 0.6805, Loss G: 0.8341\n",
      "Epoch [157/200], Batch [400/938], Loss D: 0.5618, Loss G: 0.9897\n",
      "Epoch [157/200], Batch [500/938], Loss D: 0.6119, Loss G: 0.8931\n",
      "Epoch [157/200], Batch [600/938], Loss D: 0.6324, Loss G: 0.8957\n",
      "Epoch [157/200], Batch [700/938], Loss D: 0.6852, Loss G: 0.8066\n",
      "Epoch [157/200], Batch [800/938], Loss D: 0.6325, Loss G: 0.8302\n",
      "Epoch [157/200], Batch [900/938], Loss D: 0.4354, Loss G: 1.1676\n",
      "Epoch [158/200], Batch [0/938], Loss D: 0.5374, Loss G: 1.0871\n",
      "Epoch [158/200], Batch [100/938], Loss D: 0.6036, Loss G: 0.8860\n",
      "Epoch [158/200], Batch [200/938], Loss D: 0.5861, Loss G: 0.8385\n",
      "Epoch [158/200], Batch [300/938], Loss D: 0.7323, Loss G: 0.7394\n",
      "Epoch [158/200], Batch [400/938], Loss D: 0.7707, Loss G: 0.6750\n",
      "Epoch [158/200], Batch [500/938], Loss D: 0.5569, Loss G: 0.9184\n",
      "Epoch [158/200], Batch [600/938], Loss D: 0.6223, Loss G: 0.9119\n",
      "Epoch [158/200], Batch [700/938], Loss D: 0.6741, Loss G: 0.8528\n",
      "Epoch [158/200], Batch [800/938], Loss D: 0.7472, Loss G: 0.7561\n",
      "Epoch [158/200], Batch [900/938], Loss D: 0.6687, Loss G: 0.9262\n",
      "Epoch [159/200], Batch [0/938], Loss D: 0.6943, Loss G: 0.7840\n",
      "Epoch [159/200], Batch [100/938], Loss D: 0.5377, Loss G: 0.9236\n",
      "Epoch [159/200], Batch [200/938], Loss D: 0.7424, Loss G: 0.8887\n",
      "Epoch [159/200], Batch [300/938], Loss D: 0.5826, Loss G: 0.9588\n",
      "Epoch [159/200], Batch [400/938], Loss D: 0.5141, Loss G: 1.0070\n",
      "Epoch [159/200], Batch [500/938], Loss D: 0.4730, Loss G: 1.1574\n",
      "Epoch [159/200], Batch [600/938], Loss D: 0.6681, Loss G: 0.8864\n",
      "Epoch [159/200], Batch [700/938], Loss D: 0.5843, Loss G: 1.0819\n",
      "Epoch [159/200], Batch [800/938], Loss D: 0.7350, Loss G: 0.8070\n",
      "Epoch [159/200], Batch [900/938], Loss D: 0.6328, Loss G: 1.0107\n",
      "Epoch [160/200], Batch [0/938], Loss D: 0.6618, Loss G: 0.9137\n",
      "Epoch [160/200], Batch [100/938], Loss D: 0.7743, Loss G: 0.8011\n",
      "Epoch [160/200], Batch [200/938], Loss D: 0.7130, Loss G: 0.8999\n",
      "Epoch [160/200], Batch [300/938], Loss D: 0.5990, Loss G: 1.0042\n",
      "Epoch [160/200], Batch [400/938], Loss D: 0.6701, Loss G: 0.9527\n",
      "Epoch [160/200], Batch [500/938], Loss D: 0.7601, Loss G: 0.7324\n",
      "Epoch [160/200], Batch [600/938], Loss D: 0.6385, Loss G: 1.0114\n",
      "Epoch [160/200], Batch [700/938], Loss D: 0.7353, Loss G: 0.8090\n",
      "Epoch [160/200], Batch [800/938], Loss D: 0.6809, Loss G: 0.7741\n",
      "Epoch [160/200], Batch [900/938], Loss D: 0.6358, Loss G: 0.9077\n",
      "Epoch [161/200], Batch [0/938], Loss D: 0.6770, Loss G: 0.8723\n",
      "Epoch [161/200], Batch [100/938], Loss D: 0.7416, Loss G: 0.7659\n",
      "Epoch [161/200], Batch [200/938], Loss D: 0.6030, Loss G: 0.9076\n",
      "Epoch [161/200], Batch [300/938], Loss D: 0.7536, Loss G: 0.8320\n",
      "Epoch [161/200], Batch [400/938], Loss D: 0.6741, Loss G: 0.8278\n",
      "Epoch [161/200], Batch [500/938], Loss D: 0.6680, Loss G: 0.8858\n",
      "Epoch [161/200], Batch [600/938], Loss D: 0.5428, Loss G: 1.0335\n",
      "Epoch [161/200], Batch [700/938], Loss D: 0.5874, Loss G: 0.9665\n",
      "Epoch [161/200], Batch [800/938], Loss D: 0.6100, Loss G: 0.8593\n",
      "Epoch [161/200], Batch [900/938], Loss D: 0.4999, Loss G: 1.0280\n",
      "Epoch [162/200], Batch [0/938], Loss D: 0.6734, Loss G: 0.9058\n",
      "Epoch [162/200], Batch [100/938], Loss D: 0.6146, Loss G: 0.9332\n",
      "Epoch [162/200], Batch [200/938], Loss D: 0.5711, Loss G: 0.9629\n",
      "Epoch [162/200], Batch [300/938], Loss D: 0.6750, Loss G: 0.8961\n",
      "Epoch [162/200], Batch [400/938], Loss D: 0.7377, Loss G: 0.8548\n",
      "Epoch [162/200], Batch [500/938], Loss D: 0.6321, Loss G: 0.9501\n",
      "Epoch [162/200], Batch [600/938], Loss D: 0.6360, Loss G: 0.9444\n",
      "Epoch [162/200], Batch [700/938], Loss D: 0.5541, Loss G: 1.1140\n",
      "Epoch [162/200], Batch [800/938], Loss D: 0.6861, Loss G: 0.8460\n",
      "Epoch [162/200], Batch [900/938], Loss D: 0.6904, Loss G: 0.7925\n",
      "Epoch [163/200], Batch [0/938], Loss D: 0.6360, Loss G: 0.9213\n",
      "Epoch [163/200], Batch [100/938], Loss D: 0.6177, Loss G: 0.9122\n",
      "Epoch [163/200], Batch [200/938], Loss D: 0.7386, Loss G: 0.7713\n",
      "Epoch [163/200], Batch [300/938], Loss D: 0.6178, Loss G: 0.9416\n",
      "Epoch [163/200], Batch [400/938], Loss D: 0.7996, Loss G: 0.7383\n",
      "Epoch [163/200], Batch [500/938], Loss D: 0.6423, Loss G: 0.9309\n",
      "Epoch [163/200], Batch [600/938], Loss D: 0.6182, Loss G: 0.8992\n",
      "Epoch [163/200], Batch [700/938], Loss D: 0.6350, Loss G: 0.8416\n",
      "Epoch [163/200], Batch [800/938], Loss D: 0.7785, Loss G: 0.6929\n",
      "Epoch [163/200], Batch [900/938], Loss D: 0.6865, Loss G: 0.8000\n",
      "Epoch [164/200], Batch [0/938], Loss D: 0.6933, Loss G: 0.7624\n",
      "Epoch [164/200], Batch [100/938], Loss D: 0.6443, Loss G: 0.9289\n",
      "Epoch [164/200], Batch [200/938], Loss D: 0.6691, Loss G: 0.8019\n",
      "Epoch [164/200], Batch [300/938], Loss D: 0.6066, Loss G: 0.8889\n",
      "Epoch [164/200], Batch [400/938], Loss D: 0.5632, Loss G: 0.9475\n",
      "Epoch [164/200], Batch [500/938], Loss D: 0.7064, Loss G: 0.7206\n",
      "Epoch [164/200], Batch [600/938], Loss D: 0.6460, Loss G: 0.8553\n",
      "Epoch [164/200], Batch [700/938], Loss D: 0.7121, Loss G: 0.7758\n",
      "Epoch [164/200], Batch [800/938], Loss D: 0.7211, Loss G: 0.8472\n",
      "Epoch [164/200], Batch [900/938], Loss D: 0.5864, Loss G: 0.9225\n",
      "Epoch [165/200], Batch [0/938], Loss D: 0.6973, Loss G: 0.7837\n",
      "Epoch [165/200], Batch [100/938], Loss D: 0.7282, Loss G: 0.7720\n",
      "Epoch [165/200], Batch [200/938], Loss D: 0.6296, Loss G: 0.8942\n",
      "Epoch [165/200], Batch [300/938], Loss D: 0.7108, Loss G: 0.8530\n",
      "Epoch [165/200], Batch [400/938], Loss D: 0.6406, Loss G: 0.8817\n",
      "Epoch [165/200], Batch [500/938], Loss D: 0.6645, Loss G: 0.8064\n",
      "Epoch [165/200], Batch [600/938], Loss D: 0.6354, Loss G: 0.8509\n",
      "Epoch [165/200], Batch [700/938], Loss D: 0.6622, Loss G: 0.7508\n",
      "Epoch [165/200], Batch [800/938], Loss D: 0.7099, Loss G: 0.7423\n",
      "Epoch [165/200], Batch [900/938], Loss D: 0.8174, Loss G: 0.6764\n",
      "Epoch [166/200], Batch [0/938], Loss D: 0.7380, Loss G: 0.7207\n",
      "Epoch [166/200], Batch [100/938], Loss D: 0.6761, Loss G: 0.8473\n",
      "Epoch [166/200], Batch [200/938], Loss D: 0.5381, Loss G: 1.0164\n",
      "Epoch [166/200], Batch [300/938], Loss D: 0.7248, Loss G: 0.7810\n",
      "Epoch [166/200], Batch [400/938], Loss D: 0.5987, Loss G: 0.9333\n",
      "Epoch [166/200], Batch [500/938], Loss D: 0.6479, Loss G: 0.8726\n",
      "Epoch [166/200], Batch [600/938], Loss D: 0.5915, Loss G: 0.9486\n",
      "Epoch [166/200], Batch [700/938], Loss D: 0.6263, Loss G: 0.8600\n",
      "Epoch [166/200], Batch [800/938], Loss D: 0.7141, Loss G: 0.8469\n",
      "Epoch [166/200], Batch [900/938], Loss D: 0.6958, Loss G: 0.7850\n",
      "Epoch [167/200], Batch [0/938], Loss D: 0.6615, Loss G: 0.8135\n",
      "Epoch [167/200], Batch [100/938], Loss D: 0.7021, Loss G: 0.7930\n",
      "Epoch [167/200], Batch [200/938], Loss D: 0.6122, Loss G: 0.8673\n",
      "Epoch [167/200], Batch [300/938], Loss D: 0.6205, Loss G: 0.8736\n",
      "Epoch [167/200], Batch [400/938], Loss D: 0.7006, Loss G: 0.7191\n",
      "Epoch [167/200], Batch [500/938], Loss D: 0.6525, Loss G: 0.8955\n",
      "Epoch [167/200], Batch [600/938], Loss D: 0.6127, Loss G: 0.8784\n",
      "Epoch [167/200], Batch [700/938], Loss D: 0.7873, Loss G: 0.7444\n",
      "Epoch [167/200], Batch [800/938], Loss D: 0.7225, Loss G: 0.7801\n",
      "Epoch [167/200], Batch [900/938], Loss D: 0.5842, Loss G: 0.9641\n",
      "Epoch [168/200], Batch [0/938], Loss D: 0.6947, Loss G: 0.8923\n",
      "Epoch [168/200], Batch [100/938], Loss D: 0.5924, Loss G: 0.9342\n",
      "Epoch [168/200], Batch [200/938], Loss D: 0.7525, Loss G: 0.7020\n",
      "Epoch [168/200], Batch [300/938], Loss D: 0.6469, Loss G: 0.9290\n",
      "Epoch [168/200], Batch [400/938], Loss D: 0.6827, Loss G: 0.7776\n",
      "Epoch [168/200], Batch [500/938], Loss D: 0.6871, Loss G: 0.7691\n",
      "Epoch [168/200], Batch [600/938], Loss D: 0.6013, Loss G: 0.9844\n",
      "Epoch [168/200], Batch [700/938], Loss D: 0.5816, Loss G: 0.9862\n",
      "Epoch [168/200], Batch [800/938], Loss D: 0.6613, Loss G: 0.8809\n",
      "Epoch [168/200], Batch [900/938], Loss D: 0.6898, Loss G: 0.7844\n",
      "Epoch [169/200], Batch [0/938], Loss D: 0.6883, Loss G: 0.7997\n",
      "Epoch [169/200], Batch [100/938], Loss D: 0.6986, Loss G: 0.8192\n",
      "Epoch [169/200], Batch [200/938], Loss D: 0.6993, Loss G: 0.8235\n",
      "Epoch [169/200], Batch [300/938], Loss D: 0.6862, Loss G: 0.8676\n",
      "Epoch [169/200], Batch [400/938], Loss D: 0.6320, Loss G: 0.9126\n",
      "Epoch [169/200], Batch [500/938], Loss D: 0.6927, Loss G: 0.8246\n",
      "Epoch [169/200], Batch [600/938], Loss D: 0.6583, Loss G: 0.8028\n",
      "Epoch [169/200], Batch [700/938], Loss D: 0.6275, Loss G: 0.8250\n",
      "Epoch [169/200], Batch [800/938], Loss D: 0.5962, Loss G: 0.9215\n",
      "Epoch [169/200], Batch [900/938], Loss D: 0.6965, Loss G: 0.7782\n",
      "Epoch [170/200], Batch [0/938], Loss D: 0.6799, Loss G: 0.7947\n",
      "Epoch [170/200], Batch [100/938], Loss D: 0.5882, Loss G: 0.8859\n",
      "Epoch [170/200], Batch [200/938], Loss D: 0.6508, Loss G: 0.7956\n",
      "Epoch [170/200], Batch [300/938], Loss D: 0.6717, Loss G: 0.8342\n",
      "Epoch [170/200], Batch [400/938], Loss D: 0.7018, Loss G: 0.8662\n",
      "Epoch [170/200], Batch [500/938], Loss D: 0.6833, Loss G: 0.8165\n",
      "Epoch [170/200], Batch [600/938], Loss D: 0.6324, Loss G: 0.8652\n",
      "Epoch [170/200], Batch [700/938], Loss D: 0.6950, Loss G: 0.8645\n",
      "Epoch [170/200], Batch [800/938], Loss D: 0.6737, Loss G: 0.8331\n",
      "Epoch [170/200], Batch [900/938], Loss D: 0.5838, Loss G: 0.9799\n",
      "Epoch [171/200], Batch [0/938], Loss D: 0.7128, Loss G: 0.7944\n",
      "Epoch [171/200], Batch [100/938], Loss D: 0.6770, Loss G: 0.8285\n",
      "Epoch [171/200], Batch [200/938], Loss D: 0.6893, Loss G: 0.9491\n",
      "Epoch [171/200], Batch [300/938], Loss D: 0.6405, Loss G: 0.9122\n",
      "Epoch [171/200], Batch [400/938], Loss D: 0.6460, Loss G: 0.9359\n",
      "Epoch [171/200], Batch [500/938], Loss D: 0.6967, Loss G: 0.8261\n",
      "Epoch [171/200], Batch [600/938], Loss D: 0.6986, Loss G: 0.7800\n",
      "Epoch [171/200], Batch [700/938], Loss D: 0.7016, Loss G: 0.8169\n",
      "Epoch [171/200], Batch [800/938], Loss D: 0.6480, Loss G: 0.8476\n",
      "Epoch [171/200], Batch [900/938], Loss D: 0.6611, Loss G: 0.8980\n",
      "Epoch [172/200], Batch [0/938], Loss D: 0.7424, Loss G: 0.7859\n",
      "Epoch [172/200], Batch [100/938], Loss D: 0.6958, Loss G: 0.8550\n",
      "Epoch [172/200], Batch [200/938], Loss D: 0.7171, Loss G: 0.7345\n",
      "Epoch [172/200], Batch [300/938], Loss D: 0.8267, Loss G: 0.6474\n",
      "Epoch [172/200], Batch [400/938], Loss D: 0.6965, Loss G: 0.7719\n",
      "Epoch [172/200], Batch [500/938], Loss D: 0.7035, Loss G: 0.7537\n",
      "Epoch [172/200], Batch [600/938], Loss D: 0.6755, Loss G: 0.7481\n",
      "Epoch [172/200], Batch [700/938], Loss D: 0.7049, Loss G: 0.8023\n",
      "Epoch [172/200], Batch [800/938], Loss D: 0.6674, Loss G: 0.8003\n",
      "Epoch [172/200], Batch [900/938], Loss D: 0.5491, Loss G: 0.9211\n",
      "Epoch [173/200], Batch [0/938], Loss D: 0.7143, Loss G: 0.7699\n",
      "Epoch [173/200], Batch [100/938], Loss D: 0.8286, Loss G: 0.7261\n",
      "Epoch [173/200], Batch [200/938], Loss D: 0.6340, Loss G: 0.8405\n",
      "Epoch [173/200], Batch [300/938], Loss D: 0.6845, Loss G: 0.7813\n",
      "Epoch [173/200], Batch [400/938], Loss D: 0.6754, Loss G: 0.7888\n",
      "Epoch [173/200], Batch [500/938], Loss D: 0.6687, Loss G: 0.7995\n",
      "Epoch [173/200], Batch [600/938], Loss D: 0.7518, Loss G: 0.7627\n",
      "Epoch [173/200], Batch [700/938], Loss D: 0.6448, Loss G: 0.8609\n",
      "Epoch [173/200], Batch [800/938], Loss D: 0.6637, Loss G: 0.8442\n",
      "Epoch [173/200], Batch [900/938], Loss D: 0.6499, Loss G: 0.8692\n",
      "Epoch [174/200], Batch [0/938], Loss D: 0.7577, Loss G: 0.7333\n",
      "Epoch [174/200], Batch [100/938], Loss D: 0.7558, Loss G: 0.7353\n",
      "Epoch [174/200], Batch [200/938], Loss D: 0.6894, Loss G: 0.8643\n",
      "Epoch [174/200], Batch [300/938], Loss D: 0.7942, Loss G: 0.7003\n",
      "Epoch [174/200], Batch [400/938], Loss D: 0.6378, Loss G: 0.8706\n",
      "Epoch [174/200], Batch [500/938], Loss D: 0.6635, Loss G: 0.8010\n",
      "Epoch [174/200], Batch [600/938], Loss D: 0.6035, Loss G: 0.8760\n",
      "Epoch [174/200], Batch [700/938], Loss D: 0.7357, Loss G: 0.7240\n",
      "Epoch [174/200], Batch [800/938], Loss D: 0.7593, Loss G: 0.7271\n",
      "Epoch [174/200], Batch [900/938], Loss D: 0.7239, Loss G: 0.7765\n",
      "Epoch [175/200], Batch [0/938], Loss D: 0.5956, Loss G: 0.8655\n",
      "Epoch [175/200], Batch [100/938], Loss D: 0.6328, Loss G: 0.8374\n",
      "Epoch [175/200], Batch [200/938], Loss D: 0.6910, Loss G: 0.7701\n",
      "Epoch [175/200], Batch [300/938], Loss D: 0.6047, Loss G: 0.9004\n",
      "Epoch [175/200], Batch [400/938], Loss D: 0.6448, Loss G: 0.8172\n",
      "Epoch [175/200], Batch [500/938], Loss D: 0.6312, Loss G: 0.8703\n",
      "Epoch [175/200], Batch [600/938], Loss D: 0.7599, Loss G: 0.6682\n",
      "Epoch [175/200], Batch [700/938], Loss D: 0.6140, Loss G: 0.9049\n",
      "Epoch [175/200], Batch [800/938], Loss D: 0.6126, Loss G: 0.8509\n",
      "Epoch [175/200], Batch [900/938], Loss D: 0.6191, Loss G: 0.8853\n",
      "Epoch [176/200], Batch [0/938], Loss D: 0.6359, Loss G: 0.8527\n",
      "Epoch [176/200], Batch [100/938], Loss D: 0.7568, Loss G: 0.7127\n",
      "Epoch [176/200], Batch [200/938], Loss D: 0.6448, Loss G: 0.8432\n",
      "Epoch [176/200], Batch [300/938], Loss D: 0.6849, Loss G: 0.8452\n",
      "Epoch [176/200], Batch [400/938], Loss D: 0.5979, Loss G: 0.8834\n",
      "Epoch [176/200], Batch [500/938], Loss D: 0.6620, Loss G: 0.8217\n",
      "Epoch [176/200], Batch [600/938], Loss D: 0.7163, Loss G: 0.8071\n",
      "Epoch [176/200], Batch [700/938], Loss D: 0.7130, Loss G: 0.7318\n",
      "Epoch [176/200], Batch [800/938], Loss D: 0.7593, Loss G: 0.7049\n",
      "Epoch [176/200], Batch [900/938], Loss D: 0.6753, Loss G: 0.8439\n",
      "Epoch [177/200], Batch [0/938], Loss D: 0.6944, Loss G: 0.7435\n",
      "Epoch [177/200], Batch [100/938], Loss D: 0.6566, Loss G: 0.7442\n",
      "Epoch [177/200], Batch [200/938], Loss D: 0.6891, Loss G: 0.7680\n",
      "Epoch [177/200], Batch [300/938], Loss D: 0.7349, Loss G: 0.7388\n",
      "Epoch [177/200], Batch [400/938], Loss D: 0.6665, Loss G: 0.8175\n",
      "Epoch [177/200], Batch [500/938], Loss D: 0.6579, Loss G: 0.7551\n",
      "Epoch [177/200], Batch [600/938], Loss D: 0.6806, Loss G: 0.8152\n",
      "Epoch [177/200], Batch [700/938], Loss D: 0.6455, Loss G: 0.8541\n",
      "Epoch [177/200], Batch [800/938], Loss D: 0.6430, Loss G: 0.8192\n",
      "Epoch [177/200], Batch [900/938], Loss D: 0.6679, Loss G: 0.7616\n",
      "Epoch [178/200], Batch [0/938], Loss D: 0.7469, Loss G: 0.7497\n",
      "Epoch [178/200], Batch [100/938], Loss D: 0.6710, Loss G: 0.8400\n",
      "Epoch [178/200], Batch [200/938], Loss D: 0.6127, Loss G: 0.8716\n",
      "Epoch [178/200], Batch [300/938], Loss D: 0.6980, Loss G: 0.7470\n",
      "Epoch [178/200], Batch [400/938], Loss D: 0.7552, Loss G: 0.6992\n",
      "Epoch [178/200], Batch [500/938], Loss D: 0.6937, Loss G: 0.8303\n",
      "Epoch [178/200], Batch [600/938], Loss D: 0.7317, Loss G: 0.7037\n",
      "Epoch [178/200], Batch [700/938], Loss D: 0.6609, Loss G: 0.8226\n",
      "Epoch [178/200], Batch [800/938], Loss D: 0.7016, Loss G: 0.7455\n",
      "Epoch [178/200], Batch [900/938], Loss D: 0.6175, Loss G: 0.8309\n",
      "Epoch [179/200], Batch [0/938], Loss D: 0.6811, Loss G: 0.7719\n",
      "Epoch [179/200], Batch [100/938], Loss D: 0.6036, Loss G: 0.8805\n",
      "Epoch [179/200], Batch [200/938], Loss D: 0.6066, Loss G: 0.8962\n",
      "Epoch [179/200], Batch [300/938], Loss D: 0.6898, Loss G: 0.7485\n",
      "Epoch [179/200], Batch [400/938], Loss D: 0.6466, Loss G: 0.8459\n",
      "Epoch [179/200], Batch [500/938], Loss D: 0.7055, Loss G: 0.7161\n",
      "Epoch [179/200], Batch [600/938], Loss D: 0.6531, Loss G: 0.8483\n",
      "Epoch [179/200], Batch [700/938], Loss D: 0.6338, Loss G: 0.8203\n",
      "Epoch [179/200], Batch [800/938], Loss D: 0.6482, Loss G: 0.7584\n",
      "Epoch [179/200], Batch [900/938], Loss D: 0.6692, Loss G: 0.7681\n",
      "Epoch [180/200], Batch [0/938], Loss D: 0.6716, Loss G: 0.7449\n",
      "Epoch [180/200], Batch [100/938], Loss D: 0.7139, Loss G: 0.7873\n",
      "Epoch [180/200], Batch [200/938], Loss D: 0.6554, Loss G: 0.8184\n",
      "Epoch [180/200], Batch [300/938], Loss D: 0.7538, Loss G: 0.6623\n",
      "Epoch [180/200], Batch [400/938], Loss D: 0.6047, Loss G: 0.8583\n",
      "Epoch [180/200], Batch [500/938], Loss D: 0.6987, Loss G: 0.7617\n",
      "Epoch [180/200], Batch [600/938], Loss D: 0.6176, Loss G: 0.8147\n",
      "Epoch [180/200], Batch [700/938], Loss D: 0.7024, Loss G: 0.7763\n",
      "Epoch [180/200], Batch [800/938], Loss D: 0.7226, Loss G: 0.7202\n",
      "Epoch [180/200], Batch [900/938], Loss D: 0.6912, Loss G: 0.7463\n",
      "Epoch [181/200], Batch [0/938], Loss D: 0.6989, Loss G: 0.7423\n",
      "Epoch [181/200], Batch [100/938], Loss D: 0.6901, Loss G: 0.7201\n",
      "Epoch [181/200], Batch [200/938], Loss D: 0.6350, Loss G: 0.8163\n",
      "Epoch [181/200], Batch [300/938], Loss D: 0.6519, Loss G: 0.7771\n",
      "Epoch [181/200], Batch [400/938], Loss D: 0.6288, Loss G: 0.8583\n",
      "Epoch [181/200], Batch [500/938], Loss D: 0.6163, Loss G: 0.8884\n",
      "Epoch [181/200], Batch [600/938], Loss D: 0.6483, Loss G: 0.7885\n",
      "Epoch [181/200], Batch [700/938], Loss D: 0.6639, Loss G: 0.8698\n",
      "Epoch [181/200], Batch [800/938], Loss D: 0.6217, Loss G: 0.8126\n",
      "Epoch [181/200], Batch [900/938], Loss D: 0.6579, Loss G: 0.8361\n",
      "Epoch [182/200], Batch [0/938], Loss D: 0.6564, Loss G: 0.7374\n",
      "Epoch [182/200], Batch [100/938], Loss D: 0.6066, Loss G: 0.8483\n",
      "Epoch [182/200], Batch [200/938], Loss D: 0.6474, Loss G: 0.8129\n",
      "Epoch [182/200], Batch [300/938], Loss D: 0.6433, Loss G: 0.8394\n",
      "Epoch [182/200], Batch [400/938], Loss D: 0.6603, Loss G: 0.7634\n",
      "Epoch [182/200], Batch [500/938], Loss D: 0.6770, Loss G: 0.7690\n",
      "Epoch [182/200], Batch [600/938], Loss D: 0.6546, Loss G: 0.7810\n",
      "Epoch [182/200], Batch [700/938], Loss D: 0.6660, Loss G: 0.8266\n",
      "Epoch [182/200], Batch [800/938], Loss D: 0.6211, Loss G: 0.9822\n",
      "Epoch [182/200], Batch [900/938], Loss D: 0.6523, Loss G: 0.8270\n",
      "Epoch [183/200], Batch [0/938], Loss D: 0.6823, Loss G: 0.7528\n",
      "Epoch [183/200], Batch [100/938], Loss D: 0.6840, Loss G: 0.7715\n",
      "Epoch [183/200], Batch [200/938], Loss D: 0.6061, Loss G: 0.8431\n",
      "Epoch [183/200], Batch [300/938], Loss D: 0.6253, Loss G: 0.8545\n",
      "Epoch [183/200], Batch [400/938], Loss D: 0.6307, Loss G: 0.9074\n",
      "Epoch [183/200], Batch [500/938], Loss D: 0.6038, Loss G: 0.8100\n",
      "Epoch [183/200], Batch [600/938], Loss D: 0.6256, Loss G: 0.8415\n",
      "Epoch [183/200], Batch [700/938], Loss D: 0.6947, Loss G: 0.8184\n",
      "Epoch [183/200], Batch [800/938], Loss D: 0.6997, Loss G: 0.8040\n",
      "Epoch [183/200], Batch [900/938], Loss D: 0.6642, Loss G: 0.7861\n",
      "Epoch [184/200], Batch [0/938], Loss D: 0.6430, Loss G: 0.8187\n",
      "Epoch [184/200], Batch [100/938], Loss D: 0.6478, Loss G: 0.8642\n",
      "Epoch [184/200], Batch [200/938], Loss D: 0.6773, Loss G: 0.8246\n",
      "Epoch [184/200], Batch [300/938], Loss D: 0.6911, Loss G: 0.7876\n",
      "Epoch [184/200], Batch [400/938], Loss D: 0.6329, Loss G: 0.8268\n",
      "Epoch [184/200], Batch [500/938], Loss D: 0.5901, Loss G: 0.8609\n",
      "Epoch [184/200], Batch [600/938], Loss D: 0.7265, Loss G: 0.7443\n",
      "Epoch [184/200], Batch [700/938], Loss D: 0.7076, Loss G: 0.7527\n",
      "Epoch [184/200], Batch [800/938], Loss D: 0.7452, Loss G: 0.7392\n",
      "Epoch [184/200], Batch [900/938], Loss D: 0.6793, Loss G: 0.8541\n",
      "Epoch [185/200], Batch [0/938], Loss D: 0.6594, Loss G: 0.8153\n",
      "Epoch [185/200], Batch [100/938], Loss D: 0.6891, Loss G: 0.7890\n",
      "Epoch [185/200], Batch [200/938], Loss D: 0.6047, Loss G: 0.8646\n",
      "Epoch [185/200], Batch [300/938], Loss D: 0.6571, Loss G: 0.7839\n",
      "Epoch [185/200], Batch [400/938], Loss D: 0.6470, Loss G: 0.8410\n",
      "Epoch [185/200], Batch [500/938], Loss D: 0.6383, Loss G: 0.8113\n",
      "Epoch [185/200], Batch [600/938], Loss D: 0.7470, Loss G: 0.6729\n",
      "Epoch [185/200], Batch [700/938], Loss D: 0.6519, Loss G: 0.8197\n",
      "Epoch [185/200], Batch [800/938], Loss D: 0.6375, Loss G: 0.8270\n",
      "Epoch [185/200], Batch [900/938], Loss D: 0.7496, Loss G: 0.6890\n",
      "Epoch [186/200], Batch [0/938], Loss D: 0.6527, Loss G: 0.8828\n",
      "Epoch [186/200], Batch [100/938], Loss D: 0.7204, Loss G: 0.7477\n",
      "Epoch [186/200], Batch [200/938], Loss D: 0.6652, Loss G: 0.8207\n",
      "Epoch [186/200], Batch [300/938], Loss D: 0.6605, Loss G: 0.8274\n",
      "Epoch [186/200], Batch [400/938], Loss D: 0.7331, Loss G: 0.7130\n",
      "Epoch [186/200], Batch [500/938], Loss D: 0.6761, Loss G: 0.7920\n",
      "Epoch [186/200], Batch [600/938], Loss D: 0.7427, Loss G: 0.7212\n",
      "Epoch [186/200], Batch [700/938], Loss D: 0.7042, Loss G: 0.7221\n",
      "Epoch [186/200], Batch [800/938], Loss D: 0.6347, Loss G: 0.8023\n",
      "Epoch [186/200], Batch [900/938], Loss D: 0.6168, Loss G: 0.9206\n",
      "Epoch [187/200], Batch [0/938], Loss D: 0.6615, Loss G: 0.7787\n",
      "Epoch [187/200], Batch [100/938], Loss D: 0.6899, Loss G: 0.7620\n",
      "Epoch [187/200], Batch [200/938], Loss D: 0.5870, Loss G: 0.9452\n",
      "Epoch [187/200], Batch [300/938], Loss D: 0.6911, Loss G: 0.7434\n",
      "Epoch [187/200], Batch [400/938], Loss D: 0.6808, Loss G: 0.7506\n",
      "Epoch [187/200], Batch [500/938], Loss D: 0.6873, Loss G: 0.7190\n",
      "Epoch [187/200], Batch [600/938], Loss D: 0.6565, Loss G: 0.7848\n",
      "Epoch [187/200], Batch [700/938], Loss D: 0.6646, Loss G: 0.7932\n",
      "Epoch [187/200], Batch [800/938], Loss D: 0.7289, Loss G: 0.7529\n",
      "Epoch [187/200], Batch [900/938], Loss D: 0.6872, Loss G: 0.7760\n",
      "Epoch [188/200], Batch [0/938], Loss D: 0.6855, Loss G: 0.7355\n",
      "Epoch [188/200], Batch [100/938], Loss D: 0.7088, Loss G: 0.7640\n",
      "Epoch [188/200], Batch [200/938], Loss D: 0.6098, Loss G: 0.8831\n",
      "Epoch [188/200], Batch [300/938], Loss D: 0.6363, Loss G: 0.8199\n",
      "Epoch [188/200], Batch [400/938], Loss D: 0.6929, Loss G: 0.7681\n",
      "Epoch [188/200], Batch [500/938], Loss D: 0.7002, Loss G: 0.8121\n",
      "Epoch [188/200], Batch [600/938], Loss D: 0.7304, Loss G: 0.7692\n",
      "Epoch [188/200], Batch [700/938], Loss D: 0.6219, Loss G: 0.8512\n",
      "Epoch [188/200], Batch [800/938], Loss D: 0.6822, Loss G: 0.7718\n",
      "Epoch [188/200], Batch [900/938], Loss D: 0.6936, Loss G: 0.7477\n",
      "Epoch [189/200], Batch [0/938], Loss D: 0.6633, Loss G: 0.8272\n",
      "Epoch [189/200], Batch [100/938], Loss D: 0.7067, Loss G: 0.7640\n",
      "Epoch [189/200], Batch [200/938], Loss D: 0.6932, Loss G: 0.8201\n",
      "Epoch [189/200], Batch [300/938], Loss D: 0.6821, Loss G: 0.8146\n",
      "Epoch [189/200], Batch [400/938], Loss D: 0.6325, Loss G: 0.8300\n",
      "Epoch [189/200], Batch [500/938], Loss D: 0.7236, Loss G: 0.7312\n",
      "Epoch [189/200], Batch [600/938], Loss D: 0.6814, Loss G: 0.7679\n",
      "Epoch [189/200], Batch [700/938], Loss D: 0.6210, Loss G: 0.8839\n",
      "Epoch [189/200], Batch [800/938], Loss D: 0.7038, Loss G: 0.7578\n",
      "Epoch [189/200], Batch [900/938], Loss D: 0.6632, Loss G: 0.7968\n",
      "Epoch [190/200], Batch [0/938], Loss D: 0.6131, Loss G: 0.9083\n",
      "Epoch [190/200], Batch [100/938], Loss D: 0.6916, Loss G: 0.7469\n",
      "Epoch [190/200], Batch [200/938], Loss D: 0.7269, Loss G: 0.7454\n",
      "Epoch [190/200], Batch [300/938], Loss D: 0.6550, Loss G: 0.8007\n",
      "Epoch [190/200], Batch [400/938], Loss D: 0.6539, Loss G: 0.8190\n",
      "Epoch [190/200], Batch [500/938], Loss D: 0.6913, Loss G: 0.7555\n",
      "Epoch [190/200], Batch [600/938], Loss D: 0.6988, Loss G: 0.7318\n",
      "Epoch [190/200], Batch [700/938], Loss D: 0.6762, Loss G: 0.7757\n",
      "Epoch [190/200], Batch [800/938], Loss D: 0.6379, Loss G: 0.8485\n",
      "Epoch [190/200], Batch [900/938], Loss D: 0.6216, Loss G: 0.8263\n",
      "Epoch [191/200], Batch [0/938], Loss D: 0.7064, Loss G: 0.8068\n",
      "Epoch [191/200], Batch [100/938], Loss D: 0.6547, Loss G: 0.8598\n",
      "Epoch [191/200], Batch [200/938], Loss D: 0.6968, Loss G: 0.7334\n",
      "Epoch [191/200], Batch [300/938], Loss D: 0.6313, Loss G: 0.8168\n",
      "Epoch [191/200], Batch [400/938], Loss D: 0.6528, Loss G: 0.8647\n",
      "Epoch [191/200], Batch [500/938], Loss D: 0.6454, Loss G: 0.8453\n",
      "Epoch [191/200], Batch [600/938], Loss D: 0.6305, Loss G: 0.7705\n",
      "Epoch [191/200], Batch [700/938], Loss D: 0.7392, Loss G: 0.6809\n",
      "Epoch [191/200], Batch [800/938], Loss D: 0.6536, Loss G: 0.8338\n",
      "Epoch [191/200], Batch [900/938], Loss D: 0.7001, Loss G: 0.7283\n",
      "Epoch [192/200], Batch [0/938], Loss D: 0.6751, Loss G: 0.7910\n",
      "Epoch [192/200], Batch [100/938], Loss D: 0.6621, Loss G: 0.8253\n",
      "Epoch [192/200], Batch [200/938], Loss D: 0.5299, Loss G: 0.9988\n",
      "Epoch [192/200], Batch [300/938], Loss D: 0.6554, Loss G: 0.8407\n",
      "Epoch [192/200], Batch [400/938], Loss D: 0.6790, Loss G: 0.7790\n",
      "Epoch [192/200], Batch [500/938], Loss D: 0.6170, Loss G: 0.8346\n",
      "Epoch [192/200], Batch [600/938], Loss D: 0.6185, Loss G: 0.8573\n",
      "Epoch [192/200], Batch [700/938], Loss D: 0.6843, Loss G: 0.7806\n",
      "Epoch [192/200], Batch [800/938], Loss D: 0.6234, Loss G: 0.8386\n",
      "Epoch [192/200], Batch [900/938], Loss D: 0.7169, Loss G: 0.7952\n",
      "Epoch [193/200], Batch [0/938], Loss D: 0.6020, Loss G: 0.9721\n",
      "Epoch [193/200], Batch [100/938], Loss D: 0.6297, Loss G: 0.8823\n",
      "Epoch [193/200], Batch [200/938], Loss D: 0.6830, Loss G: 0.8069\n",
      "Epoch [193/200], Batch [300/938], Loss D: 0.6898, Loss G: 0.7887\n",
      "Epoch [193/200], Batch [400/938], Loss D: 0.6381, Loss G: 0.8195\n",
      "Epoch [193/200], Batch [500/938], Loss D: 0.7148, Loss G: 0.7731\n",
      "Epoch [193/200], Batch [600/938], Loss D: 0.6725, Loss G: 0.8787\n",
      "Epoch [193/200], Batch [700/938], Loss D: 0.6473, Loss G: 0.8762\n",
      "Epoch [193/200], Batch [800/938], Loss D: 0.6807, Loss G: 0.7823\n",
      "Epoch [193/200], Batch [900/938], Loss D: 0.6534, Loss G: 0.8070\n",
      "Epoch [194/200], Batch [0/938], Loss D: 0.6446, Loss G: 0.8080\n",
      "Epoch [194/200], Batch [100/938], Loss D: 0.7281, Loss G: 0.7367\n",
      "Epoch [194/200], Batch [200/938], Loss D: 0.6006, Loss G: 0.8535\n",
      "Epoch [194/200], Batch [300/938], Loss D: 0.6522, Loss G: 0.8029\n",
      "Epoch [194/200], Batch [400/938], Loss D: 0.7066, Loss G: 0.7367\n",
      "Epoch [194/200], Batch [500/938], Loss D: 0.7341, Loss G: 0.6752\n",
      "Epoch [194/200], Batch [600/938], Loss D: 0.6875, Loss G: 0.7464\n",
      "Epoch [194/200], Batch [700/938], Loss D: 0.6490, Loss G: 0.8933\n",
      "Epoch [194/200], Batch [800/938], Loss D: 0.7139, Loss G: 0.7619\n",
      "Epoch [194/200], Batch [900/938], Loss D: 0.6743, Loss G: 0.7900\n",
      "Epoch [195/200], Batch [0/938], Loss D: 0.7394, Loss G: 0.7659\n",
      "Epoch [195/200], Batch [100/938], Loss D: 0.6623, Loss G: 0.8285\n",
      "Epoch [195/200], Batch [200/938], Loss D: 0.7088, Loss G: 0.6745\n",
      "Epoch [195/200], Batch [300/938], Loss D: 0.6620, Loss G: 0.8116\n",
      "Epoch [195/200], Batch [400/938], Loss D: 0.6235, Loss G: 0.8119\n",
      "Epoch [195/200], Batch [500/938], Loss D: 0.6838, Loss G: 0.8259\n",
      "Epoch [195/200], Batch [600/938], Loss D: 0.6843, Loss G: 0.7771\n",
      "Epoch [195/200], Batch [700/938], Loss D: 0.6602, Loss G: 0.7865\n",
      "Epoch [195/200], Batch [800/938], Loss D: 0.6668, Loss G: 0.8220\n",
      "Epoch [195/200], Batch [900/938], Loss D: 0.7121, Loss G: 0.7684\n",
      "Epoch [196/200], Batch [0/938], Loss D: 0.6093, Loss G: 0.8378\n",
      "Epoch [196/200], Batch [100/938], Loss D: 0.6338, Loss G: 0.8910\n",
      "Epoch [196/200], Batch [200/938], Loss D: 0.7006, Loss G: 0.7357\n",
      "Epoch [196/200], Batch [300/938], Loss D: 0.7405, Loss G: 0.7691\n",
      "Epoch [196/200], Batch [400/938], Loss D: 0.6506, Loss G: 0.8050\n",
      "Epoch [196/200], Batch [500/938], Loss D: 0.6986, Loss G: 0.7563\n",
      "Epoch [196/200], Batch [600/938], Loss D: 0.6143, Loss G: 0.8356\n",
      "Epoch [196/200], Batch [700/938], Loss D: 0.6214, Loss G: 0.7815\n",
      "Epoch [196/200], Batch [800/938], Loss D: 0.7143, Loss G: 0.7884\n",
      "Epoch [196/200], Batch [900/938], Loss D: 0.6341, Loss G: 0.8073\n",
      "Epoch [197/200], Batch [0/938], Loss D: 0.6846, Loss G: 0.7584\n",
      "Epoch [197/200], Batch [100/938], Loss D: 0.5446, Loss G: 0.9819\n",
      "Epoch [197/200], Batch [200/938], Loss D: 0.6235, Loss G: 0.8588\n",
      "Epoch [197/200], Batch [300/938], Loss D: 0.6122, Loss G: 0.8294\n",
      "Epoch [197/200], Batch [400/938], Loss D: 0.6299, Loss G: 0.8612\n",
      "Epoch [197/200], Batch [500/938], Loss D: 0.6091, Loss G: 0.8953\n",
      "Epoch [197/200], Batch [600/938], Loss D: 0.6263, Loss G: 0.8478\n",
      "Epoch [197/200], Batch [700/938], Loss D: 0.7369, Loss G: 0.7971\n",
      "Epoch [197/200], Batch [800/938], Loss D: 0.6105, Loss G: 0.8315\n",
      "Epoch [197/200], Batch [900/938], Loss D: 0.7034, Loss G: 0.7750\n",
      "Epoch [198/200], Batch [0/938], Loss D: 0.6672, Loss G: 0.8749\n",
      "Epoch [198/200], Batch [100/938], Loss D: 0.6124, Loss G: 0.8748\n",
      "Epoch [198/200], Batch [200/938], Loss D: 0.6493, Loss G: 0.7887\n",
      "Epoch [198/200], Batch [300/938], Loss D: 0.6643, Loss G: 0.8316\n",
      "Epoch [198/200], Batch [400/938], Loss D: 0.6906, Loss G: 0.7859\n",
      "Epoch [198/200], Batch [500/938], Loss D: 0.6255, Loss G: 0.8477\n",
      "Epoch [198/200], Batch [600/938], Loss D: 0.6462, Loss G: 0.8252\n",
      "Epoch [198/200], Batch [700/938], Loss D: 0.6750, Loss G: 0.7606\n",
      "Epoch [198/200], Batch [800/938], Loss D: 0.6213, Loss G: 0.8937\n",
      "Epoch [198/200], Batch [900/938], Loss D: 0.6435, Loss G: 0.8664\n",
      "Epoch [199/200], Batch [0/938], Loss D: 0.6847, Loss G: 0.8039\n",
      "Epoch [199/200], Batch [100/938], Loss D: 0.6475, Loss G: 0.8141\n",
      "Epoch [199/200], Batch [200/938], Loss D: 0.6456, Loss G: 0.8268\n",
      "Epoch [199/200], Batch [300/938], Loss D: 0.6630, Loss G: 0.8012\n",
      "Epoch [199/200], Batch [400/938], Loss D: 0.7163, Loss G: 0.7229\n",
      "Epoch [199/200], Batch [500/938], Loss D: 0.6477, Loss G: 0.8531\n",
      "Epoch [199/200], Batch [600/938], Loss D: 0.6353, Loss G: 0.8694\n",
      "Epoch [199/200], Batch [700/938], Loss D: 0.6693, Loss G: 0.8089\n",
      "Epoch [199/200], Batch [800/938], Loss D: 0.6796, Loss G: 0.7782\n",
      "Epoch [199/200], Batch [900/938], Loss D: 0.6052, Loss G: 0.9148\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    out = 0.5 * (x + 0.5)\n",
    "    out = out.clamp(0, 1)  # Clamp函数可以将随机变化的数值限制在一个给定的区间[min, max]内：\n",
    "    out = out.view(-1, 1, 28, 28)  # view()函数作用是将一个多行的Tensor,拼接成一行\n",
    "    return out\n",
    "\n",
    "# 定义生成器网络结构\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        x = torch.cat((z, y), dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "\n",
    "# 定义判别器网络结构\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# 定义条件GAN模型\n",
    "class cGAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(cGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        fake_images = self.generator(z, y)\n",
    "        fake_outputs = self.discriminator(fake_images, y)\n",
    "        return fake_images, fake_outputs\n",
    "\n",
    "# 参数设置\n",
    "input_size = 100  # 随机噪声z的维度\n",
    "output_size = 784  # 图像的维度（28x28）\n",
    "num_classes = 10  # 类别数量\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "num_epochs = 200\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# 加载MNIST数据集\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = MNIST(root='/data/mwj/data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "generator = Generator(input_size + num_classes, output_size)\n",
    "discriminator = Discriminator(output_size + num_classes, 1)\n",
    "\n",
    "# 初始化条件GAN模型\n",
    "cgan = cGAN(generator, discriminator).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real_images, labels) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        # 生成随机噪声和条件信息\n",
    "        z = torch.randn(real_images.size(0), input_size).to(device)\n",
    "        y = torch.eye(num_classes)[labels].to(device)  # 将类别转换为one-hot编码\n",
    "        \n",
    "        # 训练判别器\n",
    "        optimizer_d.zero_grad()\n",
    "        fake_images, fake_outputs = cgan(z, y)\n",
    "        real_outputs = discriminator(real_images.view(-1, output_size), y)\n",
    "        loss_d_real = criterion(real_outputs, torch.ones_like(real_outputs))  # 真实图像的判别器损失\n",
    "        loss_d_fake = criterion(fake_outputs, torch.zeros_like(fake_outputs))  # 生成图像的判别器损失\n",
    "        loss_d = (loss_d_real + loss_d_fake) / 2\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # 训练生成器\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_images, fake_outputs = cgan(z, y)\n",
    "        loss_g = criterion(fake_outputs, torch.ones_like(fake_outputs))  # 生成器损失\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # 打印训练信息\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], \"\n",
    "                  f\"Loss D: {loss_d:.4f}, Loss G: {loss_g:.4f}\")\n",
    "        if epoch == 0 and batch_idx==len(train_loader)-1:\n",
    "            real_images = to_img(real_images.cuda().data)\n",
    "            save_image(real_images, str(epoch)+'_real_images.png')\n",
    "        if batch_idx==len(train_loader)-1:\n",
    "            fake_images = to_img(fake_images.cuda().data)\n",
    "            save_image(fake_images, str(epoch)+'_fake_images.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "generator.eval()\n",
    "batch_s = 32\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "generator.to(device)\n",
    "z = torch.randn(batch_s, input_size).to(device)\n",
    "# labels = torch.ones(batch_s, 1, dtype=int)\n",
    "# 创建维度为[100, 1]的张量，所有元素的值都为3，意味着生成类别index为3的样本\n",
    "labels = torch.full((batch_s, 1), 6)\n",
    "y = torch.eye(num_classes)[labels].squeeze().to(device)  # 将类别转换为one-hot编码\n",
    "print(y)\n",
    "fake_images = generator(z, y)\n",
    "fake_images = to_img(fake_images)\n",
    "save_image(fake_images, 'example_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/200], Batch [0/938], Loss D: 0.7021, Loss G: 0.7621\n",
      "Epoch [200/200], Batch [100/938], Loss D: 0.6511, Loss G: 0.8138\n",
      "Epoch [200/200], Batch [200/938], Loss D: 0.6636, Loss G: 0.8971\n",
      "Epoch [200/200], Batch [300/938], Loss D: 0.6571, Loss G: 0.9199\n",
      "Epoch [200/200], Batch [400/938], Loss D: 0.6867, Loss G: 0.7655\n",
      "Epoch [200/200], Batch [500/938], Loss D: 0.7100, Loss G: 0.6794\n",
      "Epoch [200/200], Batch [600/938], Loss D: 0.6513, Loss G: 0.8016\n",
      "Epoch [200/200], Batch [700/938], Loss D: 0.6737, Loss G: 0.8111\n",
      "Epoch [200/200], Batch [800/938], Loss D: 0.7063, Loss G: 0.7915\n",
      "Epoch [200/200], Batch [900/938], Loss D: 0.6177, Loss G: 0.8832\n",
      "Epoch [201/200], Batch [0/938], Loss D: 0.6869, Loss G: 0.7334\n",
      "Epoch [201/200], Batch [100/938], Loss D: 0.7220, Loss G: 0.7356\n",
      "Epoch [201/200], Batch [200/938], Loss D: 0.6520, Loss G: 0.7828\n",
      "Epoch [201/200], Batch [300/938], Loss D: 0.6572, Loss G: 0.7868\n",
      "Epoch [201/200], Batch [400/938], Loss D: 0.6880, Loss G: 0.7401\n",
      "Epoch [201/200], Batch [500/938], Loss D: 0.7412, Loss G: 0.7243\n",
      "Epoch [201/200], Batch [600/938], Loss D: 0.7060, Loss G: 0.7759\n",
      "Epoch [201/200], Batch [700/938], Loss D: 0.6826, Loss G: 0.7909\n",
      "Epoch [201/200], Batch [800/938], Loss D: 0.6108, Loss G: 0.9103\n",
      "Epoch [201/200], Batch [900/938], Loss D: 0.7751, Loss G: 0.7386\n",
      "Epoch [202/200], Batch [0/938], Loss D: 0.6190, Loss G: 0.9129\n",
      "Epoch [202/200], Batch [100/938], Loss D: 0.6864, Loss G: 0.7949\n",
      "Epoch [202/200], Batch [200/938], Loss D: 0.6564, Loss G: 0.8453\n",
      "Epoch [202/200], Batch [300/938], Loss D: 0.5475, Loss G: 0.9397\n",
      "Epoch [202/200], Batch [400/938], Loss D: 0.6990, Loss G: 0.7307\n",
      "Epoch [202/200], Batch [500/938], Loss D: 0.7228, Loss G: 0.6750\n",
      "Epoch [202/200], Batch [600/938], Loss D: 0.6443, Loss G: 0.8256\n",
      "Epoch [202/200], Batch [700/938], Loss D: 0.7163, Loss G: 0.7718\n",
      "Epoch [202/200], Batch [800/938], Loss D: 0.6340, Loss G: 0.8277\n",
      "Epoch [202/200], Batch [900/938], Loss D: 0.6901, Loss G: 0.7899\n",
      "Epoch [203/200], Batch [0/938], Loss D: 0.7021, Loss G: 0.8049\n",
      "Epoch [203/200], Batch [100/938], Loss D: 0.7079, Loss G: 0.7263\n",
      "Epoch [203/200], Batch [200/938], Loss D: 0.6731, Loss G: 0.7546\n",
      "Epoch [203/200], Batch [300/938], Loss D: 0.6969, Loss G: 0.8041\n",
      "Epoch [203/200], Batch [400/938], Loss D: 0.6273, Loss G: 0.8518\n",
      "Epoch [203/200], Batch [500/938], Loss D: 0.6378, Loss G: 0.8251\n",
      "Epoch [203/200], Batch [600/938], Loss D: 0.6789, Loss G: 0.8079\n",
      "Epoch [203/200], Batch [700/938], Loss D: 0.6461, Loss G: 0.7993\n",
      "Epoch [203/200], Batch [800/938], Loss D: 0.6579, Loss G: 0.8337\n",
      "Epoch [203/200], Batch [900/938], Loss D: 0.7374, Loss G: 0.7006\n",
      "Epoch [204/200], Batch [0/938], Loss D: 0.6160, Loss G: 0.8838\n",
      "Epoch [204/200], Batch [100/938], Loss D: 0.6725, Loss G: 0.7896\n",
      "Epoch [204/200], Batch [200/938], Loss D: 0.6415, Loss G: 0.8068\n",
      "Epoch [204/200], Batch [300/938], Loss D: 0.6373, Loss G: 0.8279\n",
      "Epoch [204/200], Batch [400/938], Loss D: 0.6601, Loss G: 0.8396\n",
      "Epoch [204/200], Batch [500/938], Loss D: 0.7098, Loss G: 0.6931\n",
      "Epoch [204/200], Batch [600/938], Loss D: 0.6620, Loss G: 0.8482\n",
      "Epoch [204/200], Batch [700/938], Loss D: 0.7151, Loss G: 0.7129\n",
      "Epoch [204/200], Batch [800/938], Loss D: 0.7220, Loss G: 0.7746\n",
      "Epoch [204/200], Batch [900/938], Loss D: 0.6665, Loss G: 0.8016\n",
      "Epoch [205/200], Batch [0/938], Loss D: 0.6997, Loss G: 0.7471\n",
      "Epoch [205/200], Batch [100/938], Loss D: 0.6125, Loss G: 0.8704\n",
      "Epoch [205/200], Batch [200/938], Loss D: 0.6897, Loss G: 0.7279\n",
      "Epoch [205/200], Batch [300/938], Loss D: 0.6721, Loss G: 0.7923\n",
      "Epoch [205/200], Batch [400/938], Loss D: 0.6860, Loss G: 0.8024\n",
      "Epoch [205/200], Batch [500/938], Loss D: 0.6990, Loss G: 0.7895\n",
      "Epoch [205/200], Batch [600/938], Loss D: 0.6771, Loss G: 0.7941\n",
      "Epoch [205/200], Batch [700/938], Loss D: 0.6061, Loss G: 0.8655\n",
      "Epoch [205/200], Batch [800/938], Loss D: 0.6989, Loss G: 0.7773\n",
      "Epoch [205/200], Batch [900/938], Loss D: 0.6428, Loss G: 0.8014\n",
      "Epoch [206/200], Batch [0/938], Loss D: 0.6971, Loss G: 0.7383\n",
      "Epoch [206/200], Batch [100/938], Loss D: 0.7198, Loss G: 0.7137\n",
      "Epoch [206/200], Batch [200/938], Loss D: 0.6201, Loss G: 0.8660\n",
      "Epoch [206/200], Batch [300/938], Loss D: 0.6533, Loss G: 0.7983\n",
      "Epoch [206/200], Batch [400/938], Loss D: 0.7112, Loss G: 0.7262\n",
      "Epoch [206/200], Batch [500/938], Loss D: 0.6645, Loss G: 0.7550\n",
      "Epoch [206/200], Batch [600/938], Loss D: 0.7283, Loss G: 0.7540\n",
      "Epoch [206/200], Batch [700/938], Loss D: 0.6136, Loss G: 0.8534\n",
      "Epoch [206/200], Batch [800/938], Loss D: 0.6553, Loss G: 0.7646\n",
      "Epoch [206/200], Batch [900/938], Loss D: 0.7403, Loss G: 0.7016\n",
      "Epoch [207/200], Batch [0/938], Loss D: 0.7597, Loss G: 0.6766\n",
      "Epoch [207/200], Batch [100/938], Loss D: 0.6585, Loss G: 0.7859\n",
      "Epoch [207/200], Batch [200/938], Loss D: 0.6434, Loss G: 0.8647\n",
      "Epoch [207/200], Batch [300/938], Loss D: 0.7691, Loss G: 0.6944\n",
      "Epoch [207/200], Batch [400/938], Loss D: 0.6516, Loss G: 0.7631\n",
      "Epoch [207/200], Batch [500/938], Loss D: 0.6471, Loss G: 0.7647\n",
      "Epoch [207/200], Batch [600/938], Loss D: 0.7455, Loss G: 0.7001\n",
      "Epoch [207/200], Batch [700/938], Loss D: 0.6723, Loss G: 0.8249\n",
      "Epoch [207/200], Batch [800/938], Loss D: 0.6665, Loss G: 0.8109\n",
      "Epoch [207/200], Batch [900/938], Loss D: 0.6720, Loss G: 0.7568\n",
      "Epoch [208/200], Batch [0/938], Loss D: 0.6815, Loss G: 0.7258\n",
      "Epoch [208/200], Batch [100/938], Loss D: 0.6638, Loss G: 0.8190\n",
      "Epoch [208/200], Batch [200/938], Loss D: 0.6260, Loss G: 0.8141\n",
      "Epoch [208/200], Batch [300/938], Loss D: 0.7494, Loss G: 0.7511\n",
      "Epoch [208/200], Batch [400/938], Loss D: 0.6275, Loss G: 0.8353\n",
      "Epoch [208/200], Batch [500/938], Loss D: 0.7749, Loss G: 0.7002\n",
      "Epoch [208/200], Batch [600/938], Loss D: 0.6498, Loss G: 0.7769\n",
      "Epoch [208/200], Batch [700/938], Loss D: 0.6910, Loss G: 0.7373\n",
      "Epoch [208/200], Batch [800/938], Loss D: 0.6034, Loss G: 0.8644\n",
      "Epoch [208/200], Batch [900/938], Loss D: 0.6174, Loss G: 0.8543\n",
      "Epoch [209/200], Batch [0/938], Loss D: 0.6623, Loss G: 0.7831\n",
      "Epoch [209/200], Batch [100/938], Loss D: 0.5227, Loss G: 0.9880\n",
      "Epoch [209/200], Batch [200/938], Loss D: 0.6757, Loss G: 0.8671\n",
      "Epoch [209/200], Batch [300/938], Loss D: 0.7016, Loss G: 0.8012\n",
      "Epoch [209/200], Batch [400/938], Loss D: 0.6705, Loss G: 0.8767\n",
      "Epoch [209/200], Batch [500/938], Loss D: 0.7466, Loss G: 0.7051\n",
      "Epoch [209/200], Batch [600/938], Loss D: 0.6959, Loss G: 0.7930\n",
      "Epoch [209/200], Batch [700/938], Loss D: 0.7345, Loss G: 0.7110\n",
      "Epoch [209/200], Batch [800/938], Loss D: 0.6918, Loss G: 0.7913\n",
      "Epoch [209/200], Batch [900/938], Loss D: 0.6254, Loss G: 0.8622\n",
      "Epoch [210/200], Batch [0/938], Loss D: 0.6690, Loss G: 0.7966\n",
      "Epoch [210/200], Batch [100/938], Loss D: 0.6747, Loss G: 0.7874\n",
      "Epoch [210/200], Batch [200/938], Loss D: 0.6490, Loss G: 0.8162\n",
      "Epoch [210/200], Batch [300/938], Loss D: 0.7720, Loss G: 0.7044\n",
      "Epoch [210/200], Batch [400/938], Loss D: 0.6513, Loss G: 0.8218\n",
      "Epoch [210/200], Batch [500/938], Loss D: 0.6923, Loss G: 0.7936\n",
      "Epoch [210/200], Batch [600/938], Loss D: 0.6432, Loss G: 0.8485\n",
      "Epoch [210/200], Batch [700/938], Loss D: 0.7263, Loss G: 0.7370\n",
      "Epoch [210/200], Batch [800/938], Loss D: 0.6701, Loss G: 0.7864\n",
      "Epoch [210/200], Batch [900/938], Loss D: 0.6457, Loss G: 0.8148\n",
      "Epoch [211/200], Batch [0/938], Loss D: 0.6472, Loss G: 0.7875\n",
      "Epoch [211/200], Batch [100/938], Loss D: 0.6661, Loss G: 0.7624\n",
      "Epoch [211/200], Batch [200/938], Loss D: 0.6612, Loss G: 0.7381\n",
      "Epoch [211/200], Batch [300/938], Loss D: 0.6461, Loss G: 0.8283\n",
      "Epoch [211/200], Batch [400/938], Loss D: 0.6526, Loss G: 0.8662\n",
      "Epoch [211/200], Batch [500/938], Loss D: 0.6825, Loss G: 0.7693\n",
      "Epoch [211/200], Batch [600/938], Loss D: 0.7042, Loss G: 0.7575\n",
      "Epoch [211/200], Batch [700/938], Loss D: 0.6250, Loss G: 0.8445\n",
      "Epoch [211/200], Batch [800/938], Loss D: 0.6409, Loss G: 0.7982\n",
      "Epoch [211/200], Batch [900/938], Loss D: 0.6689, Loss G: 0.8459\n",
      "Epoch [212/200], Batch [0/938], Loss D: 0.6771, Loss G: 0.8283\n",
      "Epoch [212/200], Batch [100/938], Loss D: 0.6645, Loss G: 0.8907\n",
      "Epoch [212/200], Batch [200/938], Loss D: 0.6201, Loss G: 0.8538\n",
      "Epoch [212/200], Batch [300/938], Loss D: 0.7099, Loss G: 0.7370\n",
      "Epoch [212/200], Batch [400/938], Loss D: 0.6302, Loss G: 0.8341\n",
      "Epoch [212/200], Batch [500/938], Loss D: 0.6627, Loss G: 0.7582\n",
      "Epoch [212/200], Batch [600/938], Loss D: 0.6826, Loss G: 0.7429\n",
      "Epoch [212/200], Batch [700/938], Loss D: 0.6558, Loss G: 0.8287\n",
      "Epoch [212/200], Batch [800/938], Loss D: 0.7006, Loss G: 0.7497\n",
      "Epoch [212/200], Batch [900/938], Loss D: 0.6511, Loss G: 0.8337\n",
      "Epoch [213/200], Batch [0/938], Loss D: 0.7107, Loss G: 0.8613\n",
      "Epoch [213/200], Batch [100/938], Loss D: 0.6589, Loss G: 0.8336\n",
      "Epoch [213/200], Batch [200/938], Loss D: 0.6210, Loss G: 0.9060\n",
      "Epoch [213/200], Batch [300/938], Loss D: 0.6818, Loss G: 0.8124\n",
      "Epoch [213/200], Batch [400/938], Loss D: 0.6230, Loss G: 0.8855\n",
      "Epoch [213/200], Batch [500/938], Loss D: 0.6351, Loss G: 0.8738\n",
      "Epoch [213/200], Batch [600/938], Loss D: 0.6457, Loss G: 0.8578\n",
      "Epoch [213/200], Batch [700/938], Loss D: 0.6318, Loss G: 0.8302\n",
      "Epoch [213/200], Batch [800/938], Loss D: 0.6627, Loss G: 0.8593\n",
      "Epoch [213/200], Batch [900/938], Loss D: 0.6804, Loss G: 0.7715\n",
      "Epoch [214/200], Batch [0/938], Loss D: 0.8047, Loss G: 0.6717\n",
      "Epoch [214/200], Batch [100/938], Loss D: 0.7537, Loss G: 0.6882\n",
      "Epoch [214/200], Batch [200/938], Loss D: 0.6566, Loss G: 0.8513\n",
      "Epoch [214/200], Batch [300/938], Loss D: 0.6801, Loss G: 0.8409\n",
      "Epoch [214/200], Batch [400/938], Loss D: 0.5820, Loss G: 0.9148\n",
      "Epoch [214/200], Batch [500/938], Loss D: 0.6346, Loss G: 0.8270\n",
      "Epoch [214/200], Batch [600/938], Loss D: 0.7232, Loss G: 0.7457\n",
      "Epoch [214/200], Batch [700/938], Loss D: 0.6427, Loss G: 0.8632\n",
      "Epoch [214/200], Batch [800/938], Loss D: 0.6480, Loss G: 0.8204\n",
      "Epoch [214/200], Batch [900/938], Loss D: 0.6137, Loss G: 0.9239\n",
      "Epoch [215/200], Batch [0/938], Loss D: 0.6678, Loss G: 0.7868\n",
      "Epoch [215/200], Batch [100/938], Loss D: 0.6396, Loss G: 0.8295\n",
      "Epoch [215/200], Batch [200/938], Loss D: 0.6407, Loss G: 0.7978\n",
      "Epoch [215/200], Batch [300/938], Loss D: 0.6767, Loss G: 0.7277\n",
      "Epoch [215/200], Batch [400/938], Loss D: 0.7814, Loss G: 0.7488\n",
      "Epoch [215/200], Batch [500/938], Loss D: 0.6284, Loss G: 0.8445\n",
      "Epoch [215/200], Batch [600/938], Loss D: 0.6280, Loss G: 0.8577\n",
      "Epoch [215/200], Batch [700/938], Loss D: 0.7434, Loss G: 0.7892\n",
      "Epoch [215/200], Batch [800/938], Loss D: 0.7233, Loss G: 0.6878\n",
      "Epoch [215/200], Batch [900/938], Loss D: 0.6226, Loss G: 0.8962\n",
      "Epoch [216/200], Batch [0/938], Loss D: 0.6696, Loss G: 0.7816\n",
      "Epoch [216/200], Batch [100/938], Loss D: 0.5956, Loss G: 0.8849\n",
      "Epoch [216/200], Batch [200/938], Loss D: 0.6814, Loss G: 0.7630\n",
      "Epoch [216/200], Batch [300/938], Loss D: 0.6516, Loss G: 0.8269\n",
      "Epoch [216/200], Batch [400/938], Loss D: 0.7016, Loss G: 0.7200\n",
      "Epoch [216/200], Batch [500/938], Loss D: 0.6859, Loss G: 0.8221\n",
      "Epoch [216/200], Batch [600/938], Loss D: 0.6654, Loss G: 0.8243\n",
      "Epoch [216/200], Batch [700/938], Loss D: 0.6462, Loss G: 0.7716\n",
      "Epoch [216/200], Batch [800/938], Loss D: 0.6868, Loss G: 0.7622\n",
      "Epoch [216/200], Batch [900/938], Loss D: 0.7140, Loss G: 0.7369\n",
      "Epoch [217/200], Batch [0/938], Loss D: 0.7124, Loss G: 0.7585\n",
      "Epoch [217/200], Batch [100/938], Loss D: 0.6490, Loss G: 0.8220\n",
      "Epoch [217/200], Batch [200/938], Loss D: 0.6519, Loss G: 0.8335\n",
      "Epoch [217/200], Batch [300/938], Loss D: 0.6999, Loss G: 0.7636\n",
      "Epoch [217/200], Batch [400/938], Loss D: 0.7212, Loss G: 0.7387\n",
      "Epoch [217/200], Batch [500/938], Loss D: 0.6878, Loss G: 0.7704\n",
      "Epoch [217/200], Batch [600/938], Loss D: 0.6468, Loss G: 0.8110\n",
      "Epoch [217/200], Batch [700/938], Loss D: 0.6905, Loss G: 0.7397\n",
      "Epoch [217/200], Batch [800/938], Loss D: 0.6238, Loss G: 0.8471\n",
      "Epoch [217/200], Batch [900/938], Loss D: 0.6432, Loss G: 0.8201\n",
      "Epoch [218/200], Batch [0/938], Loss D: 0.6482, Loss G: 0.7825\n",
      "Epoch [218/200], Batch [100/938], Loss D: 0.6568, Loss G: 0.7751\n",
      "Epoch [218/200], Batch [200/938], Loss D: 0.7428, Loss G: 0.7334\n",
      "Epoch [218/200], Batch [300/938], Loss D: 0.6967, Loss G: 0.7583\n",
      "Epoch [218/200], Batch [400/938], Loss D: 0.6983, Loss G: 0.8001\n",
      "Epoch [218/200], Batch [500/938], Loss D: 0.6587, Loss G: 0.8032\n",
      "Epoch [218/200], Batch [600/938], Loss D: 0.6732, Loss G: 0.7670\n",
      "Epoch [218/200], Batch [700/938], Loss D: 0.6555, Loss G: 0.8721\n",
      "Epoch [218/200], Batch [800/938], Loss D: 0.6506, Loss G: 0.8510\n",
      "Epoch [218/200], Batch [900/938], Loss D: 0.6294, Loss G: 0.8064\n",
      "Epoch [219/200], Batch [0/938], Loss D: 0.6332, Loss G: 0.8402\n",
      "Epoch [219/200], Batch [100/938], Loss D: 0.6979, Loss G: 0.7551\n",
      "Epoch [219/200], Batch [200/938], Loss D: 0.7166, Loss G: 0.7461\n",
      "Epoch [219/200], Batch [300/938], Loss D: 0.6608, Loss G: 0.7811\n",
      "Epoch [219/200], Batch [400/938], Loss D: 0.6822, Loss G: 0.7769\n",
      "Epoch [219/200], Batch [500/938], Loss D: 0.7962, Loss G: 0.6921\n",
      "Epoch [219/200], Batch [600/938], Loss D: 0.7543, Loss G: 0.6881\n",
      "Epoch [219/200], Batch [700/938], Loss D: 0.6408, Loss G: 0.8098\n",
      "Epoch [219/200], Batch [800/938], Loss D: 0.6745, Loss G: 0.7630\n",
      "Epoch [219/200], Batch [900/938], Loss D: 0.6770, Loss G: 0.7965\n",
      "Epoch [220/200], Batch [0/938], Loss D: 0.6557, Loss G: 0.8161\n",
      "Epoch [220/200], Batch [100/938], Loss D: 0.6269, Loss G: 0.8444\n",
      "Epoch [220/200], Batch [200/938], Loss D: 0.6840, Loss G: 0.7732\n",
      "Epoch [220/200], Batch [300/938], Loss D: 0.7045, Loss G: 0.7629\n",
      "Epoch [220/200], Batch [400/938], Loss D: 0.6018, Loss G: 0.9802\n",
      "Epoch [220/200], Batch [500/938], Loss D: 0.6543, Loss G: 0.8539\n",
      "Epoch [220/200], Batch [600/938], Loss D: 0.6806, Loss G: 0.7749\n",
      "Epoch [220/200], Batch [700/938], Loss D: 0.6859, Loss G: 0.7503\n",
      "Epoch [220/200], Batch [800/938], Loss D: 0.6450, Loss G: 0.7801\n",
      "Epoch [220/200], Batch [900/938], Loss D: 0.6668, Loss G: 0.7645\n",
      "Epoch [221/200], Batch [0/938], Loss D: 0.7257, Loss G: 0.6696\n",
      "Epoch [221/200], Batch [100/938], Loss D: 0.5948, Loss G: 0.8777\n",
      "Epoch [221/200], Batch [200/938], Loss D: 0.7085, Loss G: 0.8065\n",
      "Epoch [221/200], Batch [300/938], Loss D: 0.7005, Loss G: 0.7613\n",
      "Epoch [221/200], Batch [400/938], Loss D: 0.6327, Loss G: 0.8767\n",
      "Epoch [221/200], Batch [500/938], Loss D: 0.6554, Loss G: 0.7755\n",
      "Epoch [221/200], Batch [600/938], Loss D: 0.7170, Loss G: 0.8379\n",
      "Epoch [221/200], Batch [700/938], Loss D: 0.6415, Loss G: 0.8621\n",
      "Epoch [221/200], Batch [800/938], Loss D: 0.7119, Loss G: 0.7413\n",
      "Epoch [221/200], Batch [900/938], Loss D: 0.7074, Loss G: 0.6782\n",
      "Epoch [222/200], Batch [0/938], Loss D: 0.6727, Loss G: 0.7990\n",
      "Epoch [222/200], Batch [100/938], Loss D: 0.7021, Loss G: 0.7390\n",
      "Epoch [222/200], Batch [200/938], Loss D: 0.6238, Loss G: 0.8586\n",
      "Epoch [222/200], Batch [300/938], Loss D: 0.6576, Loss G: 0.8232\n",
      "Epoch [222/200], Batch [400/938], Loss D: 0.6902, Loss G: 0.7662\n",
      "Epoch [222/200], Batch [500/938], Loss D: 0.6168, Loss G: 0.8281\n",
      "Epoch [222/200], Batch [600/938], Loss D: 0.6268, Loss G: 0.7847\n",
      "Epoch [222/200], Batch [700/938], Loss D: 0.6989, Loss G: 0.7803\n",
      "Epoch [222/200], Batch [800/938], Loss D: 0.7314, Loss G: 0.7511\n",
      "Epoch [222/200], Batch [900/938], Loss D: 0.6591, Loss G: 0.7960\n",
      "Epoch [223/200], Batch [0/938], Loss D: 0.7296, Loss G: 0.6786\n",
      "Epoch [223/200], Batch [100/938], Loss D: 0.7171, Loss G: 0.7348\n",
      "Epoch [223/200], Batch [200/938], Loss D: 0.6555, Loss G: 0.8532\n",
      "Epoch [223/200], Batch [300/938], Loss D: 0.7388, Loss G: 0.6977\n",
      "Epoch [223/200], Batch [400/938], Loss D: 0.6681, Loss G: 0.8482\n",
      "Epoch [223/200], Batch [500/938], Loss D: 0.6696, Loss G: 0.7667\n",
      "Epoch [223/200], Batch [600/938], Loss D: 0.6471, Loss G: 0.8598\n",
      "Epoch [223/200], Batch [700/938], Loss D: 0.7246, Loss G: 0.7417\n",
      "Epoch [223/200], Batch [800/938], Loss D: 0.7181, Loss G: 0.7427\n",
      "Epoch [223/200], Batch [900/938], Loss D: 0.6640, Loss G: 0.7913\n",
      "Epoch [224/200], Batch [0/938], Loss D: 0.6934, Loss G: 0.7809\n",
      "Epoch [224/200], Batch [100/938], Loss D: 0.7198, Loss G: 0.7369\n",
      "Epoch [224/200], Batch [200/938], Loss D: 0.6882, Loss G: 0.7733\n",
      "Epoch [224/200], Batch [300/938], Loss D: 0.7947, Loss G: 0.6352\n",
      "Epoch [224/200], Batch [400/938], Loss D: 0.6309, Loss G: 0.8245\n",
      "Epoch [224/200], Batch [500/938], Loss D: 0.7497, Loss G: 0.7443\n",
      "Epoch [224/200], Batch [600/938], Loss D: 0.5969, Loss G: 0.8984\n",
      "Epoch [224/200], Batch [700/938], Loss D: 0.6829, Loss G: 0.7774\n",
      "Epoch [224/200], Batch [800/938], Loss D: 0.6655, Loss G: 0.7817\n",
      "Epoch [224/200], Batch [900/938], Loss D: 0.6534, Loss G: 0.8713\n",
      "Epoch [225/200], Batch [0/938], Loss D: 0.6702, Loss G: 0.7345\n",
      "Epoch [225/200], Batch [100/938], Loss D: 0.7627, Loss G: 0.7071\n",
      "Epoch [225/200], Batch [200/938], Loss D: 0.7118, Loss G: 0.7240\n",
      "Epoch [225/200], Batch [300/938], Loss D: 0.6265, Loss G: 0.9090\n",
      "Epoch [225/200], Batch [400/938], Loss D: 0.6733, Loss G: 0.7650\n",
      "Epoch [225/200], Batch [500/938], Loss D: 0.6962, Loss G: 0.7591\n",
      "Epoch [225/200], Batch [600/938], Loss D: 0.7348, Loss G: 0.7096\n",
      "Epoch [225/200], Batch [700/938], Loss D: 0.6742, Loss G: 0.7916\n",
      "Epoch [225/200], Batch [800/938], Loss D: 0.6333, Loss G: 0.8326\n",
      "Epoch [225/200], Batch [900/938], Loss D: 0.6700, Loss G: 0.8979\n",
      "Epoch [226/200], Batch [0/938], Loss D: 0.7117, Loss G: 0.7529\n",
      "Epoch [226/200], Batch [100/938], Loss D: 0.6002, Loss G: 0.9827\n",
      "Epoch [226/200], Batch [200/938], Loss D: 0.7445, Loss G: 0.7472\n",
      "Epoch [226/200], Batch [300/938], Loss D: 0.6135, Loss G: 0.9186\n",
      "Epoch [226/200], Batch [400/938], Loss D: 0.6066, Loss G: 0.8710\n",
      "Epoch [226/200], Batch [500/938], Loss D: 0.6866, Loss G: 0.8134\n",
      "Epoch [226/200], Batch [600/938], Loss D: 0.7543, Loss G: 0.7402\n",
      "Epoch [226/200], Batch [700/938], Loss D: 0.6671, Loss G: 0.8232\n",
      "Epoch [226/200], Batch [800/938], Loss D: 0.6548, Loss G: 0.7886\n",
      "Epoch [226/200], Batch [900/938], Loss D: 0.6904, Loss G: 0.7569\n",
      "Epoch [227/200], Batch [0/938], Loss D: 0.6764, Loss G: 0.7647\n",
      "Epoch [227/200], Batch [100/938], Loss D: 0.6853, Loss G: 0.8194\n",
      "Epoch [227/200], Batch [200/938], Loss D: 0.7025, Loss G: 0.7922\n",
      "Epoch [227/200], Batch [300/938], Loss D: 0.7792, Loss G: 0.7204\n",
      "Epoch [227/200], Batch [400/938], Loss D: 0.6305, Loss G: 0.7972\n",
      "Epoch [227/200], Batch [500/938], Loss D: 0.6146, Loss G: 0.9178\n",
      "Epoch [227/200], Batch [600/938], Loss D: 0.6532, Loss G: 0.8652\n",
      "Epoch [227/200], Batch [700/938], Loss D: 0.6086, Loss G: 0.8505\n",
      "Epoch [227/200], Batch [800/938], Loss D: 0.6158, Loss G: 0.8779\n",
      "Epoch [227/200], Batch [900/938], Loss D: 0.6239, Loss G: 0.8898\n",
      "Epoch [228/200], Batch [0/938], Loss D: 0.6013, Loss G: 0.8114\n",
      "Epoch [228/200], Batch [100/938], Loss D: 0.6842, Loss G: 0.7587\n",
      "Epoch [228/200], Batch [200/938], Loss D: 0.7413, Loss G: 0.7756\n",
      "Epoch [228/200], Batch [300/938], Loss D: 0.7098, Loss G: 0.7951\n",
      "Epoch [228/200], Batch [400/938], Loss D: 0.6194, Loss G: 0.7802\n",
      "Epoch [228/200], Batch [500/938], Loss D: 0.6142, Loss G: 0.8907\n",
      "Epoch [228/200], Batch [600/938], Loss D: 0.6544, Loss G: 0.8654\n",
      "Epoch [228/200], Batch [700/938], Loss D: 0.6348, Loss G: 0.8427\n",
      "Epoch [228/200], Batch [800/938], Loss D: 0.6342, Loss G: 0.8558\n",
      "Epoch [228/200], Batch [900/938], Loss D: 0.6609, Loss G: 0.8293\n",
      "Epoch [229/200], Batch [0/938], Loss D: 0.6282, Loss G: 0.8409\n",
      "Epoch [229/200], Batch [100/938], Loss D: 0.6562, Loss G: 0.8373\n",
      "Epoch [229/200], Batch [200/938], Loss D: 0.6154, Loss G: 0.8504\n",
      "Epoch [229/200], Batch [300/938], Loss D: 0.5501, Loss G: 1.0001\n",
      "Epoch [229/200], Batch [400/938], Loss D: 0.7516, Loss G: 0.7378\n",
      "Epoch [229/200], Batch [500/938], Loss D: 0.6863, Loss G: 0.8008\n",
      "Epoch [229/200], Batch [600/938], Loss D: 0.6552, Loss G: 0.8472\n",
      "Epoch [229/200], Batch [700/938], Loss D: 0.6507, Loss G: 0.8126\n",
      "Epoch [229/200], Batch [800/938], Loss D: 0.6446, Loss G: 0.8275\n",
      "Epoch [229/200], Batch [900/938], Loss D: 0.6724, Loss G: 0.8424\n",
      "Epoch [230/200], Batch [0/938], Loss D: 0.6482, Loss G: 0.8341\n",
      "Epoch [230/200], Batch [100/938], Loss D: 0.6627, Loss G: 0.7782\n",
      "Epoch [230/200], Batch [200/938], Loss D: 0.6235, Loss G: 0.8740\n",
      "Epoch [230/200], Batch [300/938], Loss D: 0.7320, Loss G: 0.7695\n",
      "Epoch [230/200], Batch [400/938], Loss D: 0.6359, Loss G: 0.8143\n",
      "Epoch [230/200], Batch [500/938], Loss D: 0.6257, Loss G: 0.9153\n",
      "Epoch [230/200], Batch [600/938], Loss D: 0.6566, Loss G: 0.8275\n",
      "Epoch [230/200], Batch [700/938], Loss D: 0.6345, Loss G: 0.8658\n",
      "Epoch [230/200], Batch [800/938], Loss D: 0.6759, Loss G: 0.8002\n",
      "Epoch [230/200], Batch [900/938], Loss D: 0.6594, Loss G: 0.8782\n",
      "Epoch [231/200], Batch [0/938], Loss D: 0.6329, Loss G: 0.8377\n",
      "Epoch [231/200], Batch [100/938], Loss D: 0.6024, Loss G: 0.8347\n",
      "Epoch [231/200], Batch [200/938], Loss D: 0.6551, Loss G: 0.7999\n",
      "Epoch [231/200], Batch [300/938], Loss D: 0.6873, Loss G: 0.7684\n",
      "Epoch [231/200], Batch [400/938], Loss D: 0.6457, Loss G: 0.8945\n",
      "Epoch [231/200], Batch [500/938], Loss D: 0.7059, Loss G: 0.8038\n",
      "Epoch [231/200], Batch [600/938], Loss D: 0.6968, Loss G: 0.8044\n",
      "Epoch [231/200], Batch [700/938], Loss D: 0.6404, Loss G: 0.8088\n",
      "Epoch [231/200], Batch [800/938], Loss D: 0.6737, Loss G: 0.7530\n",
      "Epoch [231/200], Batch [900/938], Loss D: 0.6568, Loss G: 0.7964\n",
      "Epoch [232/200], Batch [0/938], Loss D: 0.6812, Loss G: 0.7820\n",
      "Epoch [232/200], Batch [100/938], Loss D: 0.8198, Loss G: 0.6513\n",
      "Epoch [232/200], Batch [200/938], Loss D: 0.6582, Loss G: 0.8237\n",
      "Epoch [232/200], Batch [300/938], Loss D: 0.7559, Loss G: 0.7528\n",
      "Epoch [232/200], Batch [400/938], Loss D: 0.7629, Loss G: 0.6992\n",
      "Epoch [232/200], Batch [500/938], Loss D: 0.7105, Loss G: 0.7396\n",
      "Epoch [232/200], Batch [600/938], Loss D: 0.6694, Loss G: 0.7867\n",
      "Epoch [232/200], Batch [700/938], Loss D: 0.6214, Loss G: 0.8545\n",
      "Epoch [232/200], Batch [800/938], Loss D: 0.6538, Loss G: 0.8579\n",
      "Epoch [232/200], Batch [900/938], Loss D: 0.6196, Loss G: 0.8696\n",
      "Epoch [233/200], Batch [0/938], Loss D: 0.7311, Loss G: 0.7436\n",
      "Epoch [233/200], Batch [100/938], Loss D: 0.6347, Loss G: 0.8396\n",
      "Epoch [233/200], Batch [200/938], Loss D: 0.6842, Loss G: 0.7326\n",
      "Epoch [233/200], Batch [300/938], Loss D: 0.6513, Loss G: 0.8518\n",
      "Epoch [233/200], Batch [400/938], Loss D: 0.7028, Loss G: 0.7000\n",
      "Epoch [233/200], Batch [500/938], Loss D: 0.7236, Loss G: 0.7154\n",
      "Epoch [233/200], Batch [600/938], Loss D: 0.6294, Loss G: 0.8532\n",
      "Epoch [233/200], Batch [700/938], Loss D: 0.5888, Loss G: 0.8679\n",
      "Epoch [233/200], Batch [800/938], Loss D: 0.6744, Loss G: 0.7883\n",
      "Epoch [233/200], Batch [900/938], Loss D: 0.6634, Loss G: 0.8236\n",
      "Epoch [234/200], Batch [0/938], Loss D: 0.5636, Loss G: 0.9150\n",
      "Epoch [234/200], Batch [100/938], Loss D: 0.6308, Loss G: 0.8776\n",
      "Epoch [234/200], Batch [200/938], Loss D: 0.7037, Loss G: 0.7422\n",
      "Epoch [234/200], Batch [300/938], Loss D: 0.6578, Loss G: 0.7852\n",
      "Epoch [234/200], Batch [400/938], Loss D: 0.6271, Loss G: 0.7955\n",
      "Epoch [234/200], Batch [500/938], Loss D: 0.7665, Loss G: 0.6885\n",
      "Epoch [234/200], Batch [600/938], Loss D: 0.6303, Loss G: 0.8227\n",
      "Epoch [234/200], Batch [700/938], Loss D: 0.6875, Loss G: 0.7604\n",
      "Epoch [234/200], Batch [800/938], Loss D: 0.6899, Loss G: 0.7912\n",
      "Epoch [234/200], Batch [900/938], Loss D: 0.7138, Loss G: 0.6860\n",
      "Epoch [235/200], Batch [0/938], Loss D: 0.6767, Loss G: 0.8553\n",
      "Epoch [235/200], Batch [100/938], Loss D: 0.6786, Loss G: 0.7790\n",
      "Epoch [235/200], Batch [200/938], Loss D: 0.6229, Loss G: 0.9064\n",
      "Epoch [235/200], Batch [300/938], Loss D: 0.6407, Loss G: 0.8471\n",
      "Epoch [235/200], Batch [400/938], Loss D: 0.6302, Loss G: 0.8173\n",
      "Epoch [235/200], Batch [500/938], Loss D: 0.6773, Loss G: 0.8037\n",
      "Epoch [235/200], Batch [600/938], Loss D: 0.7012, Loss G: 0.8125\n",
      "Epoch [235/200], Batch [700/938], Loss D: 0.6606, Loss G: 0.8181\n",
      "Epoch [235/200], Batch [800/938], Loss D: 0.6681, Loss G: 0.7874\n",
      "Epoch [235/200], Batch [900/938], Loss D: 0.6811, Loss G: 0.7344\n",
      "Epoch [236/200], Batch [0/938], Loss D: 0.6385, Loss G: 0.8343\n",
      "Epoch [236/200], Batch [100/938], Loss D: 0.6095, Loss G: 0.8323\n",
      "Epoch [236/200], Batch [200/938], Loss D: 0.7509, Loss G: 0.7444\n",
      "Epoch [236/200], Batch [300/938], Loss D: 0.6751, Loss G: 0.7418\n",
      "Epoch [236/200], Batch [400/938], Loss D: 0.7025, Loss G: 0.7400\n",
      "Epoch [236/200], Batch [500/938], Loss D: 0.6134, Loss G: 0.8422\n",
      "Epoch [236/200], Batch [600/938], Loss D: 0.7015, Loss G: 0.7041\n",
      "Epoch [236/200], Batch [700/938], Loss D: 0.6908, Loss G: 0.7483\n",
      "Epoch [236/200], Batch [800/938], Loss D: 0.6510, Loss G: 0.8031\n",
      "Epoch [236/200], Batch [900/938], Loss D: 0.6889, Loss G: 0.7598\n",
      "Epoch [237/200], Batch [0/938], Loss D: 0.7276, Loss G: 0.6963\n",
      "Epoch [237/200], Batch [100/938], Loss D: 0.7111, Loss G: 0.7369\n",
      "Epoch [237/200], Batch [200/938], Loss D: 0.6660, Loss G: 0.7889\n",
      "Epoch [237/200], Batch [300/938], Loss D: 0.6779, Loss G: 0.7485\n",
      "Epoch [237/200], Batch [400/938], Loss D: 0.6305, Loss G: 0.9087\n",
      "Epoch [237/200], Batch [500/938], Loss D: 0.7594, Loss G: 0.6909\n",
      "Epoch [237/200], Batch [600/938], Loss D: 0.6653, Loss G: 0.7678\n",
      "Epoch [237/200], Batch [700/938], Loss D: 0.7012, Loss G: 0.7142\n",
      "Epoch [237/200], Batch [800/938], Loss D: 0.6921, Loss G: 0.8055\n",
      "Epoch [237/200], Batch [900/938], Loss D: 0.6334, Loss G: 0.8017\n",
      "Epoch [238/200], Batch [0/938], Loss D: 0.6600, Loss G: 0.7974\n",
      "Epoch [238/200], Batch [100/938], Loss D: 0.6376, Loss G: 0.8232\n",
      "Epoch [238/200], Batch [200/938], Loss D: 0.6777, Loss G: 0.8148\n",
      "Epoch [238/200], Batch [300/938], Loss D: 0.6735, Loss G: 0.7283\n",
      "Epoch [238/200], Batch [400/938], Loss D: 0.7048, Loss G: 0.7335\n",
      "Epoch [238/200], Batch [500/938], Loss D: 0.5800, Loss G: 0.9137\n",
      "Epoch [238/200], Batch [600/938], Loss D: 0.6781, Loss G: 0.7846\n",
      "Epoch [238/200], Batch [700/938], Loss D: 0.6974, Loss G: 0.7954\n",
      "Epoch [238/200], Batch [800/938], Loss D: 0.6482, Loss G: 0.8018\n",
      "Epoch [238/200], Batch [900/938], Loss D: 0.6520, Loss G: 0.8520\n",
      "Epoch [239/200], Batch [0/938], Loss D: 0.6597, Loss G: 0.8448\n",
      "Epoch [239/200], Batch [100/938], Loss D: 0.6314, Loss G: 0.8595\n",
      "Epoch [239/200], Batch [200/938], Loss D: 0.6257, Loss G: 0.9098\n",
      "Epoch [239/200], Batch [300/938], Loss D: 0.7106, Loss G: 0.7534\n",
      "Epoch [239/200], Batch [400/938], Loss D: 0.7432, Loss G: 0.6963\n",
      "Epoch [239/200], Batch [500/938], Loss D: 0.6929, Loss G: 0.7425\n",
      "Epoch [239/200], Batch [600/938], Loss D: 0.6803, Loss G: 0.7810\n",
      "Epoch [239/200], Batch [700/938], Loss D: 0.7279, Loss G: 0.8018\n",
      "Epoch [239/200], Batch [800/938], Loss D: 0.6152, Loss G: 0.8775\n",
      "Epoch [239/200], Batch [900/938], Loss D: 0.6843, Loss G: 0.7306\n",
      "Epoch [240/200], Batch [0/938], Loss D: 0.6554, Loss G: 0.7381\n",
      "Epoch [240/200], Batch [100/938], Loss D: 0.6335, Loss G: 0.7873\n",
      "Epoch [240/200], Batch [200/938], Loss D: 0.6588, Loss G: 0.8182\n",
      "Epoch [240/200], Batch [300/938], Loss D: 0.5933, Loss G: 0.8720\n",
      "Epoch [240/200], Batch [400/938], Loss D: 0.6323, Loss G: 0.8356\n",
      "Epoch [240/200], Batch [500/938], Loss D: 0.6770, Loss G: 0.7649\n",
      "Epoch [240/200], Batch [600/938], Loss D: 0.7245, Loss G: 0.7021\n",
      "Epoch [240/200], Batch [700/938], Loss D: 0.6666, Loss G: 0.8110\n",
      "Epoch [240/200], Batch [800/938], Loss D: 0.6675, Loss G: 0.7988\n",
      "Epoch [240/200], Batch [900/938], Loss D: 0.6460, Loss G: 0.7959\n",
      "Epoch [241/200], Batch [0/938], Loss D: 0.6766, Loss G: 0.7101\n",
      "Epoch [241/200], Batch [100/938], Loss D: 0.6687, Loss G: 0.7911\n",
      "Epoch [241/200], Batch [200/938], Loss D: 0.6929, Loss G: 0.7485\n",
      "Epoch [241/200], Batch [300/938], Loss D: 0.6551, Loss G: 0.7752\n",
      "Epoch [241/200], Batch [400/938], Loss D: 0.6076, Loss G: 0.8234\n",
      "Epoch [241/200], Batch [500/938], Loss D: 0.6224, Loss G: 0.8045\n",
      "Epoch [241/200], Batch [600/938], Loss D: 0.6704, Loss G: 0.8571\n",
      "Epoch [241/200], Batch [700/938], Loss D: 0.6672, Loss G: 0.7982\n",
      "Epoch [241/200], Batch [800/938], Loss D: 0.6885, Loss G: 0.7681\n",
      "Epoch [241/200], Batch [900/938], Loss D: 0.6457, Loss G: 0.8294\n",
      "Epoch [242/200], Batch [0/938], Loss D: 0.7032, Loss G: 0.7457\n",
      "Epoch [242/200], Batch [100/938], Loss D: 0.5976, Loss G: 0.9019\n",
      "Epoch [242/200], Batch [200/938], Loss D: 0.7039, Loss G: 0.7707\n",
      "Epoch [242/200], Batch [300/938], Loss D: 0.7332, Loss G: 0.7096\n",
      "Epoch [242/200], Batch [400/938], Loss D: 0.7382, Loss G: 0.7153\n",
      "Epoch [242/200], Batch [500/938], Loss D: 0.6527, Loss G: 0.8331\n",
      "Epoch [242/200], Batch [600/938], Loss D: 0.6792, Loss G: 0.8154\n",
      "Epoch [242/200], Batch [700/938], Loss D: 0.6661, Loss G: 0.7549\n",
      "Epoch [242/200], Batch [800/938], Loss D: 0.6012, Loss G: 0.8195\n",
      "Epoch [242/200], Batch [900/938], Loss D: 0.6602, Loss G: 0.7627\n",
      "Epoch [243/200], Batch [0/938], Loss D: 0.6906, Loss G: 0.8426\n",
      "Epoch [243/200], Batch [100/938], Loss D: 0.7009, Loss G: 0.8018\n",
      "Epoch [243/200], Batch [200/938], Loss D: 0.6495, Loss G: 0.8140\n",
      "Epoch [243/200], Batch [300/938], Loss D: 0.6636, Loss G: 0.7716\n",
      "Epoch [243/200], Batch [400/938], Loss D: 0.6592, Loss G: 0.8666\n",
      "Epoch [243/200], Batch [500/938], Loss D: 0.6726, Loss G: 0.7778\n",
      "Epoch [243/200], Batch [600/938], Loss D: 0.6703, Loss G: 0.7566\n",
      "Epoch [243/200], Batch [700/938], Loss D: 0.6598, Loss G: 0.8219\n",
      "Epoch [243/200], Batch [800/938], Loss D: 0.7073, Loss G: 0.7119\n",
      "Epoch [243/200], Batch [900/938], Loss D: 0.6799, Loss G: 0.7415\n",
      "Epoch [244/200], Batch [0/938], Loss D: 0.6969, Loss G: 0.7824\n",
      "Epoch [244/200], Batch [100/938], Loss D: 0.7063, Loss G: 0.7789\n",
      "Epoch [244/200], Batch [200/938], Loss D: 0.6898, Loss G: 0.8059\n",
      "Epoch [244/200], Batch [300/938], Loss D: 0.7443, Loss G: 0.7456\n",
      "Epoch [244/200], Batch [400/938], Loss D: 0.6751, Loss G: 0.7830\n",
      "Epoch [244/200], Batch [500/938], Loss D: 0.6336, Loss G: 0.8346\n",
      "Epoch [244/200], Batch [600/938], Loss D: 0.7023, Loss G: 0.7848\n",
      "Epoch [244/200], Batch [700/938], Loss D: 0.6766, Loss G: 0.7954\n",
      "Epoch [244/200], Batch [800/938], Loss D: 0.6736, Loss G: 0.7606\n",
      "Epoch [244/200], Batch [900/938], Loss D: 0.7022, Loss G: 0.7681\n",
      "Epoch [245/200], Batch [0/938], Loss D: 0.6564, Loss G: 0.7803\n",
      "Epoch [245/200], Batch [100/938], Loss D: 0.6835, Loss G: 0.8155\n",
      "Epoch [245/200], Batch [200/938], Loss D: 0.7209, Loss G: 0.7881\n",
      "Epoch [245/200], Batch [300/938], Loss D: 0.6517, Loss G: 0.7646\n",
      "Epoch [245/200], Batch [400/938], Loss D: 0.6556, Loss G: 0.7888\n",
      "Epoch [245/200], Batch [500/938], Loss D: 0.7218, Loss G: 0.7547\n",
      "Epoch [245/200], Batch [600/938], Loss D: 0.7165, Loss G: 0.7586\n",
      "Epoch [245/200], Batch [700/938], Loss D: 0.6965, Loss G: 0.7277\n",
      "Epoch [245/200], Batch [800/938], Loss D: 0.6243, Loss G: 0.8443\n",
      "Epoch [245/200], Batch [900/938], Loss D: 0.6741, Loss G: 0.7744\n",
      "Epoch [246/200], Batch [0/938], Loss D: 0.6857, Loss G: 0.7768\n",
      "Epoch [246/200], Batch [100/938], Loss D: 0.6825, Loss G: 0.7843\n",
      "Epoch [246/200], Batch [200/938], Loss D: 0.6194, Loss G: 0.8319\n",
      "Epoch [246/200], Batch [300/938], Loss D: 0.6301, Loss G: 0.8935\n",
      "Epoch [246/200], Batch [400/938], Loss D: 0.6554, Loss G: 0.8302\n",
      "Epoch [246/200], Batch [500/938], Loss D: 0.6300, Loss G: 0.7831\n",
      "Epoch [246/200], Batch [600/938], Loss D: 0.6689, Loss G: 0.8654\n",
      "Epoch [246/200], Batch [700/938], Loss D: 0.6616, Loss G: 0.8043\n",
      "Epoch [246/200], Batch [800/938], Loss D: 0.6719, Loss G: 0.7818\n",
      "Epoch [246/200], Batch [900/938], Loss D: 0.6516, Loss G: 0.8044\n",
      "Epoch [247/200], Batch [0/938], Loss D: 0.6637, Loss G: 0.8017\n",
      "Epoch [247/200], Batch [100/938], Loss D: 0.7498, Loss G: 0.7519\n",
      "Epoch [247/200], Batch [200/938], Loss D: 0.6667, Loss G: 0.8205\n",
      "Epoch [247/200], Batch [300/938], Loss D: 0.7186, Loss G: 0.6905\n",
      "Epoch [247/200], Batch [400/938], Loss D: 0.7196, Loss G: 0.7316\n",
      "Epoch [247/200], Batch [500/938], Loss D: 0.7096, Loss G: 0.7833\n",
      "Epoch [247/200], Batch [600/938], Loss D: 0.6487, Loss G: 0.8428\n",
      "Epoch [247/200], Batch [700/938], Loss D: 0.6726, Loss G: 0.8458\n",
      "Epoch [247/200], Batch [800/938], Loss D: 0.7101, Loss G: 0.7145\n",
      "Epoch [247/200], Batch [900/938], Loss D: 0.7282, Loss G: 0.7394\n",
      "Epoch [248/200], Batch [0/938], Loss D: 0.6543, Loss G: 0.7839\n",
      "Epoch [248/200], Batch [100/938], Loss D: 0.6648, Loss G: 0.7927\n",
      "Epoch [248/200], Batch [200/938], Loss D: 0.6414, Loss G: 0.7932\n",
      "Epoch [248/200], Batch [300/938], Loss D: 0.6250, Loss G: 0.7718\n",
      "Epoch [248/200], Batch [400/938], Loss D: 0.6779, Loss G: 0.7584\n",
      "Epoch [248/200], Batch [500/938], Loss D: 0.6698, Loss G: 0.7715\n",
      "Epoch [248/200], Batch [600/938], Loss D: 0.7134, Loss G: 0.7211\n",
      "Epoch [248/200], Batch [700/938], Loss D: 0.6238, Loss G: 0.8428\n",
      "Epoch [248/200], Batch [800/938], Loss D: 0.7116, Loss G: 0.7117\n",
      "Epoch [248/200], Batch [900/938], Loss D: 0.6804, Loss G: 0.7571\n",
      "Epoch [249/200], Batch [0/938], Loss D: 0.6779, Loss G: 0.7417\n",
      "Epoch [249/200], Batch [100/938], Loss D: 0.6455, Loss G: 0.8432\n",
      "Epoch [249/200], Batch [200/938], Loss D: 0.6603, Loss G: 0.8307\n",
      "Epoch [249/200], Batch [300/938], Loss D: 0.6356, Loss G: 0.8120\n",
      "Epoch [249/200], Batch [400/938], Loss D: 0.6245, Loss G: 0.8174\n",
      "Epoch [249/200], Batch [500/938], Loss D: 0.6764, Loss G: 0.7824\n",
      "Epoch [249/200], Batch [600/938], Loss D: 0.6729, Loss G: 0.7696\n",
      "Epoch [249/200], Batch [700/938], Loss D: 0.5990, Loss G: 0.8235\n",
      "Epoch [249/200], Batch [800/938], Loss D: 0.6564, Loss G: 0.7705\n",
      "Epoch [249/200], Batch [900/938], Loss D: 0.7088, Loss G: 0.8082\n",
      "Epoch [250/200], Batch [0/938], Loss D: 0.6258, Loss G: 0.8135\n",
      "Epoch [250/200], Batch [100/938], Loss D: 0.7492, Loss G: 0.7025\n",
      "Epoch [250/200], Batch [200/938], Loss D: 0.5888, Loss G: 0.9053\n",
      "Epoch [250/200], Batch [300/938], Loss D: 0.6353, Loss G: 0.8380\n",
      "Epoch [250/200], Batch [400/938], Loss D: 0.5759, Loss G: 0.9079\n",
      "Epoch [250/200], Batch [500/938], Loss D: 0.6987, Loss G: 0.7597\n",
      "Epoch [250/200], Batch [600/938], Loss D: 0.6948, Loss G: 0.7809\n",
      "Epoch [250/200], Batch [700/938], Loss D: 0.7165, Loss G: 0.7161\n",
      "Epoch [250/200], Batch [800/938], Loss D: 0.6373, Loss G: 0.8390\n",
      "Epoch [250/200], Batch [900/938], Loss D: 0.6928, Loss G: 0.7694\n",
      "Epoch [251/200], Batch [0/938], Loss D: 0.6042, Loss G: 0.9321\n",
      "Epoch [251/200], Batch [100/938], Loss D: 0.5928, Loss G: 0.9085\n",
      "Epoch [251/200], Batch [200/938], Loss D: 0.5845, Loss G: 0.8372\n",
      "Epoch [251/200], Batch [300/938], Loss D: 0.6575, Loss G: 0.7811\n",
      "Epoch [251/200], Batch [400/938], Loss D: 0.6314, Loss G: 0.8234\n",
      "Epoch [251/200], Batch [500/938], Loss D: 0.6419, Loss G: 0.7802\n",
      "Epoch [251/200], Batch [600/938], Loss D: 0.6095, Loss G: 0.8575\n",
      "Epoch [251/200], Batch [700/938], Loss D: 0.6107, Loss G: 0.8678\n",
      "Epoch [251/200], Batch [800/938], Loss D: 0.6409, Loss G: 0.7818\n",
      "Epoch [251/200], Batch [900/938], Loss D: 0.6644, Loss G: 0.7908\n",
      "Epoch [252/200], Batch [0/938], Loss D: 0.7063, Loss G: 0.7987\n",
      "Epoch [252/200], Batch [100/938], Loss D: 0.7142, Loss G: 0.7778\n",
      "Epoch [252/200], Batch [200/938], Loss D: 0.6910, Loss G: 0.7503\n",
      "Epoch [252/200], Batch [300/938], Loss D: 0.6893, Loss G: 0.7405\n",
      "Epoch [252/200], Batch [400/938], Loss D: 0.7432, Loss G: 0.6540\n",
      "Epoch [252/200], Batch [500/938], Loss D: 0.7149, Loss G: 0.7719\n",
      "Epoch [252/200], Batch [600/938], Loss D: 0.6480, Loss G: 0.8135\n",
      "Epoch [252/200], Batch [700/938], Loss D: 0.6025, Loss G: 0.8847\n",
      "Epoch [252/200], Batch [800/938], Loss D: 0.6971, Loss G: 0.7606\n",
      "Epoch [252/200], Batch [900/938], Loss D: 0.6576, Loss G: 0.8309\n",
      "Epoch [253/200], Batch [0/938], Loss D: 0.6413, Loss G: 0.8017\n",
      "Epoch [253/200], Batch [100/938], Loss D: 0.6807, Loss G: 0.8037\n",
      "Epoch [253/200], Batch [200/938], Loss D: 0.6538, Loss G: 0.8246\n",
      "Epoch [253/200], Batch [300/938], Loss D: 0.6632, Loss G: 0.8316\n",
      "Epoch [253/200], Batch [400/938], Loss D: 0.6396, Loss G: 0.8250\n",
      "Epoch [253/200], Batch [500/938], Loss D: 0.7292, Loss G: 0.7345\n",
      "Epoch [253/200], Batch [600/938], Loss D: 0.6501, Loss G: 0.7438\n",
      "Epoch [253/200], Batch [700/938], Loss D: 0.6315, Loss G: 0.8078\n",
      "Epoch [253/200], Batch [800/938], Loss D: 0.7107, Loss G: 0.7424\n",
      "Epoch [253/200], Batch [900/938], Loss D: 0.7038, Loss G: 0.6641\n",
      "Epoch [254/200], Batch [0/938], Loss D: 0.6690, Loss G: 0.8176\n",
      "Epoch [254/200], Batch [100/938], Loss D: 0.6144, Loss G: 0.9056\n",
      "Epoch [254/200], Batch [200/938], Loss D: 0.6409, Loss G: 0.7934\n",
      "Epoch [254/200], Batch [300/938], Loss D: 0.6925, Loss G: 0.7312\n",
      "Epoch [254/200], Batch [400/938], Loss D: 0.6922, Loss G: 0.7519\n",
      "Epoch [254/200], Batch [500/938], Loss D: 0.6912, Loss G: 0.7867\n",
      "Epoch [254/200], Batch [600/938], Loss D: 0.7053, Loss G: 0.7840\n",
      "Epoch [254/200], Batch [700/938], Loss D: 0.6189, Loss G: 0.8671\n",
      "Epoch [254/200], Batch [800/938], Loss D: 0.5525, Loss G: 0.9275\n",
      "Epoch [254/200], Batch [900/938], Loss D: 0.6412, Loss G: 0.7867\n",
      "Epoch [255/200], Batch [0/938], Loss D: 0.6854, Loss G: 0.7315\n",
      "Epoch [255/200], Batch [100/938], Loss D: 0.7270, Loss G: 0.7130\n",
      "Epoch [255/200], Batch [200/938], Loss D: 0.6685, Loss G: 0.7694\n",
      "Epoch [255/200], Batch [300/938], Loss D: 0.6787, Loss G: 0.7247\n",
      "Epoch [255/200], Batch [400/938], Loss D: 0.6315, Loss G: 0.8219\n",
      "Epoch [255/200], Batch [500/938], Loss D: 0.6745, Loss G: 0.8132\n",
      "Epoch [255/200], Batch [600/938], Loss D: 0.7126, Loss G: 0.7083\n",
      "Epoch [255/200], Batch [700/938], Loss D: 0.6848, Loss G: 0.7247\n",
      "Epoch [255/200], Batch [800/938], Loss D: 0.6897, Loss G: 0.7610\n",
      "Epoch [255/200], Batch [900/938], Loss D: 0.7223, Loss G: 0.7556\n",
      "Epoch [256/200], Batch [0/938], Loss D: 0.6356, Loss G: 0.8355\n",
      "Epoch [256/200], Batch [100/938], Loss D: 0.6976, Loss G: 0.7691\n",
      "Epoch [256/200], Batch [200/938], Loss D: 0.6549, Loss G: 0.8429\n",
      "Epoch [256/200], Batch [300/938], Loss D: 0.6202, Loss G: 0.8626\n",
      "Epoch [256/200], Batch [400/938], Loss D: 0.6096, Loss G: 0.8451\n",
      "Epoch [256/200], Batch [500/938], Loss D: 0.6412, Loss G: 0.8137\n",
      "Epoch [256/200], Batch [600/938], Loss D: 0.6526, Loss G: 0.8364\n",
      "Epoch [256/200], Batch [700/938], Loss D: 0.6982, Loss G: 0.7552\n",
      "Epoch [256/200], Batch [800/938], Loss D: 0.6912, Loss G: 0.8550\n",
      "Epoch [256/200], Batch [900/938], Loss D: 0.6291, Loss G: 0.8443\n",
      "Epoch [257/200], Batch [0/938], Loss D: 0.6733, Loss G: 0.7635\n",
      "Epoch [257/200], Batch [100/938], Loss D: 0.7234, Loss G: 0.7334\n",
      "Epoch [257/200], Batch [200/938], Loss D: 0.7104, Loss G: 0.7296\n",
      "Epoch [257/200], Batch [300/938], Loss D: 0.6938, Loss G: 0.7582\n",
      "Epoch [257/200], Batch [400/938], Loss D: 0.6775, Loss G: 0.7667\n",
      "Epoch [257/200], Batch [500/938], Loss D: 0.6818, Loss G: 0.7520\n",
      "Epoch [257/200], Batch [600/938], Loss D: 0.6888, Loss G: 0.7404\n",
      "Epoch [257/200], Batch [700/938], Loss D: 0.6994, Loss G: 0.7167\n",
      "Epoch [257/200], Batch [800/938], Loss D: 0.6879, Loss G: 0.7541\n",
      "Epoch [257/200], Batch [900/938], Loss D: 0.7159, Loss G: 0.7192\n",
      "Epoch [258/200], Batch [0/938], Loss D: 0.6281, Loss G: 0.8190\n",
      "Epoch [258/200], Batch [100/938], Loss D: 0.6738, Loss G: 0.7994\n",
      "Epoch [258/200], Batch [200/938], Loss D: 0.6979, Loss G: 0.7464\n",
      "Epoch [258/200], Batch [300/938], Loss D: 0.7145, Loss G: 0.7520\n",
      "Epoch [258/200], Batch [400/938], Loss D: 0.6902, Loss G: 0.7167\n",
      "Epoch [258/200], Batch [500/938], Loss D: 0.6283, Loss G: 0.8182\n",
      "Epoch [258/200], Batch [600/938], Loss D: 0.6910, Loss G: 0.8039\n",
      "Epoch [258/200], Batch [700/938], Loss D: 0.6419, Loss G: 0.8369\n",
      "Epoch [258/200], Batch [800/938], Loss D: 0.7418, Loss G: 0.7257\n",
      "Epoch [258/200], Batch [900/938], Loss D: 0.6944, Loss G: 0.7844\n",
      "Epoch [259/200], Batch [0/938], Loss D: 0.6284, Loss G: 0.8840\n",
      "Epoch [259/200], Batch [100/938], Loss D: 0.6714, Loss G: 0.7688\n",
      "Epoch [259/200], Batch [200/938], Loss D: 0.6109, Loss G: 0.8245\n",
      "Epoch [259/200], Batch [300/938], Loss D: 0.6659, Loss G: 0.7922\n",
      "Epoch [259/200], Batch [400/938], Loss D: 0.7035, Loss G: 0.7728\n",
      "Epoch [259/200], Batch [500/938], Loss D: 0.6790, Loss G: 0.8205\n",
      "Epoch [259/200], Batch [600/938], Loss D: 0.6539, Loss G: 0.8163\n",
      "Epoch [259/200], Batch [700/938], Loss D: 0.6371, Loss G: 0.8621\n",
      "Epoch [259/200], Batch [800/938], Loss D: 0.7128, Loss G: 0.7896\n",
      "Epoch [259/200], Batch [900/938], Loss D: 0.7284, Loss G: 0.7196\n",
      "Epoch [260/200], Batch [0/938], Loss D: 0.6658, Loss G: 0.7268\n",
      "Epoch [260/200], Batch [100/938], Loss D: 0.7852, Loss G: 0.6431\n",
      "Epoch [260/200], Batch [200/938], Loss D: 0.6846, Loss G: 0.8175\n",
      "Epoch [260/200], Batch [300/938], Loss D: 0.7369, Loss G: 0.7235\n",
      "Epoch [260/200], Batch [400/938], Loss D: 0.6658, Loss G: 0.8327\n",
      "Epoch [260/200], Batch [500/938], Loss D: 0.6383, Loss G: 0.8525\n",
      "Epoch [260/200], Batch [600/938], Loss D: 0.6668, Loss G: 0.7943\n",
      "Epoch [260/200], Batch [700/938], Loss D: 0.6745, Loss G: 0.7632\n",
      "Epoch [260/200], Batch [800/938], Loss D: 0.6789, Loss G: 0.7804\n",
      "Epoch [260/200], Batch [900/938], Loss D: 0.6350, Loss G: 0.8418\n",
      "Epoch [261/200], Batch [0/938], Loss D: 0.6456, Loss G: 0.8395\n",
      "Epoch [261/200], Batch [100/938], Loss D: 0.6245, Loss G: 0.8279\n",
      "Epoch [261/200], Batch [200/938], Loss D: 0.6783, Loss G: 0.7693\n",
      "Epoch [261/200], Batch [300/938], Loss D: 0.6345, Loss G: 0.7900\n",
      "Epoch [261/200], Batch [400/938], Loss D: 0.6687, Loss G: 0.7652\n",
      "Epoch [261/200], Batch [500/938], Loss D: 0.6939, Loss G: 0.7945\n",
      "Epoch [261/200], Batch [600/938], Loss D: 0.6554, Loss G: 0.8159\n",
      "Epoch [261/200], Batch [700/938], Loss D: 0.7025, Loss G: 0.7310\n",
      "Epoch [261/200], Batch [800/938], Loss D: 0.7036, Loss G: 0.8111\n",
      "Epoch [261/200], Batch [900/938], Loss D: 0.6794, Loss G: 0.7281\n",
      "Epoch [262/200], Batch [0/938], Loss D: 0.6282, Loss G: 0.8763\n",
      "Epoch [262/200], Batch [100/938], Loss D: 0.6941, Loss G: 0.7304\n",
      "Epoch [262/200], Batch [200/938], Loss D: 0.6737, Loss G: 0.8169\n",
      "Epoch [262/200], Batch [300/938], Loss D: 0.6971, Loss G: 0.7751\n",
      "Epoch [262/200], Batch [400/938], Loss D: 0.7307, Loss G: 0.7293\n",
      "Epoch [262/200], Batch [500/938], Loss D: 0.6499, Loss G: 0.7983\n",
      "Epoch [262/200], Batch [600/938], Loss D: 0.6645, Loss G: 0.7523\n",
      "Epoch [262/200], Batch [700/938], Loss D: 0.6105, Loss G: 0.8832\n",
      "Epoch [262/200], Batch [800/938], Loss D: 0.6865, Loss G: 0.7168\n",
      "Epoch [262/200], Batch [900/938], Loss D: 0.6826, Loss G: 0.8542\n",
      "Epoch [263/200], Batch [0/938], Loss D: 0.7169, Loss G: 0.7206\n",
      "Epoch [263/200], Batch [100/938], Loss D: 0.6128, Loss G: 0.8703\n",
      "Epoch [263/200], Batch [200/938], Loss D: 0.6603, Loss G: 0.7944\n",
      "Epoch [263/200], Batch [300/938], Loss D: 0.7386, Loss G: 0.7253\n",
      "Epoch [263/200], Batch [400/938], Loss D: 0.6723, Loss G: 0.7761\n",
      "Epoch [263/200], Batch [500/938], Loss D: 0.6871, Loss G: 0.7877\n",
      "Epoch [263/200], Batch [600/938], Loss D: 0.7125, Loss G: 0.7653\n",
      "Epoch [263/200], Batch [700/938], Loss D: 0.6544, Loss G: 0.8415\n",
      "Epoch [263/200], Batch [800/938], Loss D: 0.6794, Loss G: 0.7516\n",
      "Epoch [263/200], Batch [900/938], Loss D: 0.6322, Loss G: 0.7895\n",
      "Epoch [264/200], Batch [0/938], Loss D: 0.6766, Loss G: 0.7665\n",
      "Epoch [264/200], Batch [100/938], Loss D: 0.6344, Loss G: 0.7968\n",
      "Epoch [264/200], Batch [200/938], Loss D: 0.6262, Loss G: 0.8513\n",
      "Epoch [264/200], Batch [300/938], Loss D: 0.6473, Loss G: 0.7749\n",
      "Epoch [264/200], Batch [400/938], Loss D: 0.6267, Loss G: 0.7853\n",
      "Epoch [264/200], Batch [500/938], Loss D: 0.6179, Loss G: 0.8593\n",
      "Epoch [264/200], Batch [600/938], Loss D: 0.6848, Loss G: 0.7758\n",
      "Epoch [264/200], Batch [700/938], Loss D: 0.6514, Loss G: 0.8356\n",
      "Epoch [264/200], Batch [800/938], Loss D: 0.6325, Loss G: 0.8285\n",
      "Epoch [264/200], Batch [900/938], Loss D: 0.7081, Loss G: 0.7665\n",
      "Epoch [265/200], Batch [0/938], Loss D: 0.6546, Loss G: 0.7918\n",
      "Epoch [265/200], Batch [100/938], Loss D: 0.7044, Loss G: 0.7419\n",
      "Epoch [265/200], Batch [200/938], Loss D: 0.7411, Loss G: 0.6761\n",
      "Epoch [265/200], Batch [300/938], Loss D: 0.7044, Loss G: 0.7215\n",
      "Epoch [265/200], Batch [400/938], Loss D: 0.6038, Loss G: 0.8744\n",
      "Epoch [265/200], Batch [500/938], Loss D: 0.6761, Loss G: 0.8291\n",
      "Epoch [265/200], Batch [600/938], Loss D: 0.7040, Loss G: 0.7585\n",
      "Epoch [265/200], Batch [700/938], Loss D: 0.6809, Loss G: 0.7134\n",
      "Epoch [265/200], Batch [800/938], Loss D: 0.6883, Loss G: 0.7551\n",
      "Epoch [265/200], Batch [900/938], Loss D: 0.6578, Loss G: 0.8094\n",
      "Epoch [266/200], Batch [0/938], Loss D: 0.6615, Loss G: 0.7717\n",
      "Epoch [266/200], Batch [100/938], Loss D: 0.6933, Loss G: 0.7996\n",
      "Epoch [266/200], Batch [200/938], Loss D: 0.6509, Loss G: 0.7989\n",
      "Epoch [266/200], Batch [300/938], Loss D: 0.7070, Loss G: 0.6943\n",
      "Epoch [266/200], Batch [400/938], Loss D: 0.6306, Loss G: 0.8243\n",
      "Epoch [266/200], Batch [500/938], Loss D: 0.6183, Loss G: 0.8086\n",
      "Epoch [266/200], Batch [600/938], Loss D: 0.5899, Loss G: 0.8974\n",
      "Epoch [266/200], Batch [700/938], Loss D: 0.6699, Loss G: 0.8034\n",
      "Epoch [266/200], Batch [800/938], Loss D: 0.6976, Loss G: 0.7749\n",
      "Epoch [266/200], Batch [900/938], Loss D: 0.7264, Loss G: 0.7190\n",
      "Epoch [267/200], Batch [0/938], Loss D: 0.6251, Loss G: 0.8732\n",
      "Epoch [267/200], Batch [100/938], Loss D: 0.6154, Loss G: 0.8563\n",
      "Epoch [267/200], Batch [200/938], Loss D: 0.7220, Loss G: 0.7371\n",
      "Epoch [267/200], Batch [300/938], Loss D: 0.6763, Loss G: 0.7210\n",
      "Epoch [267/200], Batch [400/938], Loss D: 0.6788, Loss G: 0.7624\n",
      "Epoch [267/200], Batch [500/938], Loss D: 0.6923, Loss G: 0.7239\n",
      "Epoch [267/200], Batch [600/938], Loss D: 0.6570, Loss G: 0.7414\n",
      "Epoch [267/200], Batch [700/938], Loss D: 0.6250, Loss G: 0.8110\n",
      "Epoch [267/200], Batch [800/938], Loss D: 0.6716, Loss G: 0.7720\n",
      "Epoch [267/200], Batch [900/938], Loss D: 0.6605, Loss G: 0.7819\n",
      "Epoch [268/200], Batch [0/938], Loss D: 0.6350, Loss G: 0.8230\n",
      "Epoch [268/200], Batch [100/938], Loss D: 0.6715, Loss G: 0.7685\n",
      "Epoch [268/200], Batch [200/938], Loss D: 0.6827, Loss G: 0.7798\n",
      "Epoch [268/200], Batch [300/938], Loss D: 0.6462, Loss G: 0.7613\n",
      "Epoch [268/200], Batch [400/938], Loss D: 0.7139, Loss G: 0.7371\n",
      "Epoch [268/200], Batch [500/938], Loss D: 0.6108, Loss G: 0.8146\n",
      "Epoch [268/200], Batch [600/938], Loss D: 0.6487, Loss G: 0.7882\n",
      "Epoch [268/200], Batch [700/938], Loss D: 0.6650, Loss G: 0.7711\n",
      "Epoch [268/200], Batch [800/938], Loss D: 0.6514, Loss G: 0.7721\n",
      "Epoch [268/200], Batch [900/938], Loss D: 0.6226, Loss G: 0.8595\n",
      "Epoch [269/200], Batch [0/938], Loss D: 0.6828, Loss G: 0.8002\n",
      "Epoch [269/200], Batch [100/938], Loss D: 0.6728, Loss G: 0.8283\n",
      "Epoch [269/200], Batch [200/938], Loss D: 0.6665, Loss G: 0.8170\n",
      "Epoch [269/200], Batch [300/938], Loss D: 0.6395, Loss G: 0.8420\n",
      "Epoch [269/200], Batch [400/938], Loss D: 0.6171, Loss G: 0.8708\n",
      "Epoch [269/200], Batch [500/938], Loss D: 0.6939, Loss G: 0.7025\n",
      "Epoch [269/200], Batch [600/938], Loss D: 0.6575, Loss G: 0.8073\n",
      "Epoch [269/200], Batch [700/938], Loss D: 0.6708, Loss G: 0.8354\n",
      "Epoch [269/200], Batch [800/938], Loss D: 0.6026, Loss G: 0.7966\n",
      "Epoch [269/200], Batch [900/938], Loss D: 0.6598, Loss G: 0.7664\n",
      "Epoch [270/200], Batch [0/938], Loss D: 0.6627, Loss G: 0.7602\n",
      "Epoch [270/200], Batch [100/938], Loss D: 0.7154, Loss G: 0.7493\n",
      "Epoch [270/200], Batch [200/938], Loss D: 0.7233, Loss G: 0.7356\n",
      "Epoch [270/200], Batch [300/938], Loss D: 0.6610, Loss G: 0.8388\n",
      "Epoch [270/200], Batch [400/938], Loss D: 0.6849, Loss G: 0.8149\n",
      "Epoch [270/200], Batch [500/938], Loss D: 0.6806, Loss G: 0.8114\n",
      "Epoch [270/200], Batch [600/938], Loss D: 0.7397, Loss G: 0.7333\n",
      "Epoch [270/200], Batch [700/938], Loss D: 0.7159, Loss G: 0.7206\n",
      "Epoch [270/200], Batch [800/938], Loss D: 0.5847, Loss G: 0.8613\n",
      "Epoch [270/200], Batch [900/938], Loss D: 0.7664, Loss G: 0.6911\n",
      "Epoch [271/200], Batch [0/938], Loss D: 0.6365, Loss G: 0.8376\n",
      "Epoch [271/200], Batch [100/938], Loss D: 0.6741, Loss G: 0.7383\n",
      "Epoch [271/200], Batch [200/938], Loss D: 0.6507, Loss G: 0.8434\n",
      "Epoch [271/200], Batch [300/938], Loss D: 0.7009, Loss G: 0.7434\n",
      "Epoch [271/200], Batch [400/938], Loss D: 0.7107, Loss G: 0.7391\n",
      "Epoch [271/200], Batch [500/938], Loss D: 0.6635, Loss G: 0.7445\n",
      "Epoch [271/200], Batch [600/938], Loss D: 0.7235, Loss G: 0.7707\n",
      "Epoch [271/200], Batch [700/938], Loss D: 0.7264, Loss G: 0.7205\n",
      "Epoch [271/200], Batch [800/938], Loss D: 0.6455, Loss G: 0.7796\n",
      "Epoch [271/200], Batch [900/938], Loss D: 0.7022, Loss G: 0.7648\n",
      "Epoch [272/200], Batch [0/938], Loss D: 0.6621, Loss G: 0.8149\n",
      "Epoch [272/200], Batch [100/938], Loss D: 0.6675, Loss G: 0.7936\n",
      "Epoch [272/200], Batch [200/938], Loss D: 0.7082, Loss G: 0.7453\n",
      "Epoch [272/200], Batch [300/938], Loss D: 0.6691, Loss G: 0.7705\n",
      "Epoch [272/200], Batch [400/938], Loss D: 0.7002, Loss G: 0.8079\n",
      "Epoch [272/200], Batch [500/938], Loss D: 0.7473, Loss G: 0.7274\n",
      "Epoch [272/200], Batch [600/938], Loss D: 0.6899, Loss G: 0.7701\n",
      "Epoch [272/200], Batch [700/938], Loss D: 0.6590, Loss G: 0.8165\n",
      "Epoch [272/200], Batch [800/938], Loss D: 0.6211, Loss G: 0.8255\n",
      "Epoch [272/200], Batch [900/938], Loss D: 0.7044, Loss G: 0.6920\n",
      "Epoch [273/200], Batch [0/938], Loss D: 0.6669, Loss G: 0.7325\n",
      "Epoch [273/200], Batch [100/938], Loss D: 0.6289, Loss G: 0.7803\n",
      "Epoch [273/200], Batch [200/938], Loss D: 0.7337, Loss G: 0.7358\n",
      "Epoch [273/200], Batch [300/938], Loss D: 0.6940, Loss G: 0.7563\n",
      "Epoch [273/200], Batch [400/938], Loss D: 0.6388, Loss G: 0.7653\n",
      "Epoch [273/200], Batch [500/938], Loss D: 0.6811, Loss G: 0.7785\n",
      "Epoch [273/200], Batch [600/938], Loss D: 0.6434, Loss G: 0.8285\n",
      "Epoch [273/200], Batch [700/938], Loss D: 0.6778, Loss G: 0.8086\n",
      "Epoch [273/200], Batch [800/938], Loss D: 0.7426, Loss G: 0.7041\n",
      "Epoch [273/200], Batch [900/938], Loss D: 0.7549, Loss G: 0.6777\n",
      "Epoch [274/200], Batch [0/938], Loss D: 0.6390, Loss G: 0.8181\n",
      "Epoch [274/200], Batch [100/938], Loss D: 0.6282, Loss G: 0.8116\n",
      "Epoch [274/200], Batch [200/938], Loss D: 0.6509, Loss G: 0.8354\n",
      "Epoch [274/200], Batch [300/938], Loss D: 0.6332, Loss G: 0.8570\n",
      "Epoch [274/200], Batch [400/938], Loss D: 0.6356, Loss G: 0.8570\n",
      "Epoch [274/200], Batch [500/938], Loss D: 0.6953, Loss G: 0.7147\n",
      "Epoch [274/200], Batch [600/938], Loss D: 0.6365, Loss G: 0.8375\n",
      "Epoch [274/200], Batch [700/938], Loss D: 0.6572, Loss G: 0.7798\n",
      "Epoch [274/200], Batch [800/938], Loss D: 0.6738, Loss G: 0.7454\n",
      "Epoch [274/200], Batch [900/938], Loss D: 0.7328, Loss G: 0.6855\n",
      "Epoch [275/200], Batch [0/938], Loss D: 0.7197, Loss G: 0.7536\n",
      "Epoch [275/200], Batch [100/938], Loss D: 0.6652, Loss G: 0.7163\n",
      "Epoch [275/200], Batch [200/938], Loss D: 0.7585, Loss G: 0.6580\n",
      "Epoch [275/200], Batch [300/938], Loss D: 0.6679, Loss G: 0.7110\n",
      "Epoch [275/200], Batch [400/938], Loss D: 0.6814, Loss G: 0.7475\n",
      "Epoch [275/200], Batch [500/938], Loss D: 0.6879, Loss G: 0.7632\n",
      "Epoch [275/200], Batch [600/938], Loss D: 0.6682, Loss G: 0.7728\n",
      "Epoch [275/200], Batch [700/938], Loss D: 0.6857, Loss G: 0.7093\n",
      "Epoch [275/200], Batch [800/938], Loss D: 0.6569, Loss G: 0.8153\n",
      "Epoch [275/200], Batch [900/938], Loss D: 0.6396, Loss G: 0.7929\n",
      "Epoch [276/200], Batch [0/938], Loss D: 0.6599, Loss G: 0.7109\n",
      "Epoch [276/200], Batch [100/938], Loss D: 0.7317, Loss G: 0.7260\n",
      "Epoch [276/200], Batch [200/938], Loss D: 0.7043, Loss G: 0.7445\n",
      "Epoch [276/200], Batch [300/938], Loss D: 0.6722, Loss G: 0.7617\n",
      "Epoch [276/200], Batch [400/938], Loss D: 0.6485, Loss G: 0.8116\n",
      "Epoch [276/200], Batch [500/938], Loss D: 0.6970, Loss G: 0.7609\n",
      "Epoch [276/200], Batch [600/938], Loss D: 0.6500, Loss G: 0.7790\n",
      "Epoch [276/200], Batch [700/938], Loss D: 0.6255, Loss G: 0.8220\n",
      "Epoch [276/200], Batch [800/938], Loss D: 0.7359, Loss G: 0.7053\n",
      "Epoch [276/200], Batch [900/938], Loss D: 0.6692, Loss G: 0.7929\n",
      "Epoch [277/200], Batch [0/938], Loss D: 0.6777, Loss G: 0.7690\n",
      "Epoch [277/200], Batch [100/938], Loss D: 0.6227, Loss G: 0.8431\n",
      "Epoch [277/200], Batch [200/938], Loss D: 0.7087, Loss G: 0.7176\n",
      "Epoch [277/200], Batch [300/938], Loss D: 0.5812, Loss G: 0.8519\n",
      "Epoch [277/200], Batch [400/938], Loss D: 0.7168, Loss G: 0.7439\n",
      "Epoch [277/200], Batch [500/938], Loss D: 0.6644, Loss G: 0.7960\n",
      "Epoch [277/200], Batch [600/938], Loss D: 0.6474, Loss G: 0.8171\n",
      "Epoch [277/200], Batch [700/938], Loss D: 0.6686, Loss G: 0.7408\n",
      "Epoch [277/200], Batch [800/938], Loss D: 0.7104, Loss G: 0.7554\n",
      "Epoch [277/200], Batch [900/938], Loss D: 0.6110, Loss G: 0.8393\n",
      "Epoch [278/200], Batch [0/938], Loss D: 0.7067, Loss G: 0.7611\n",
      "Epoch [278/200], Batch [100/938], Loss D: 0.6887, Loss G: 0.7646\n",
      "Epoch [278/200], Batch [200/938], Loss D: 0.7105, Loss G: 0.7367\n",
      "Epoch [278/200], Batch [300/938], Loss D: 0.6640, Loss G: 0.7588\n",
      "Epoch [278/200], Batch [400/938], Loss D: 0.6713, Loss G: 0.7797\n",
      "Epoch [278/200], Batch [500/938], Loss D: 0.6768, Loss G: 0.7978\n",
      "Epoch [278/200], Batch [600/938], Loss D: 0.6809, Loss G: 0.7515\n",
      "Epoch [278/200], Batch [700/938], Loss D: 0.6462, Loss G: 0.8259\n",
      "Epoch [278/200], Batch [800/938], Loss D: 0.6303, Loss G: 0.8038\n",
      "Epoch [278/200], Batch [900/938], Loss D: 0.6529, Loss G: 0.8159\n",
      "Epoch [279/200], Batch [0/938], Loss D: 0.7277, Loss G: 0.6949\n",
      "Epoch [279/200], Batch [100/938], Loss D: 0.6563, Loss G: 0.7806\n",
      "Epoch [279/200], Batch [200/938], Loss D: 0.6448, Loss G: 0.8182\n",
      "Epoch [279/200], Batch [300/938], Loss D: 0.6741, Loss G: 0.7752\n",
      "Epoch [279/200], Batch [400/938], Loss D: 0.6877, Loss G: 0.7925\n",
      "Epoch [279/200], Batch [500/938], Loss D: 0.6376, Loss G: 0.8234\n",
      "Epoch [279/200], Batch [600/938], Loss D: 0.6987, Loss G: 0.7543\n",
      "Epoch [279/200], Batch [700/938], Loss D: 0.6839, Loss G: 0.8127\n",
      "Epoch [279/200], Batch [800/938], Loss D: 0.7606, Loss G: 0.7073\n",
      "Epoch [279/200], Batch [900/938], Loss D: 0.6375, Loss G: 0.8430\n",
      "Epoch [280/200], Batch [0/938], Loss D: 0.6830, Loss G: 0.7559\n",
      "Epoch [280/200], Batch [100/938], Loss D: 0.7042, Loss G: 0.7136\n",
      "Epoch [280/200], Batch [200/938], Loss D: 0.7048, Loss G: 0.7806\n",
      "Epoch [280/200], Batch [300/938], Loss D: 0.6604, Loss G: 0.8162\n",
      "Epoch [280/200], Batch [400/938], Loss D: 0.6213, Loss G: 0.8273\n",
      "Epoch [280/200], Batch [500/938], Loss D: 0.6711, Loss G: 0.8203\n",
      "Epoch [280/200], Batch [600/938], Loss D: 0.7061, Loss G: 0.7103\n",
      "Epoch [280/200], Batch [700/938], Loss D: 0.7043, Loss G: 0.7467\n",
      "Epoch [280/200], Batch [800/938], Loss D: 0.6370, Loss G: 0.7674\n",
      "Epoch [280/200], Batch [900/938], Loss D: 0.6667, Loss G: 0.8174\n",
      "Epoch [281/200], Batch [0/938], Loss D: 0.6460, Loss G: 0.7716\n",
      "Epoch [281/200], Batch [100/938], Loss D: 0.7059, Loss G: 0.7694\n",
      "Epoch [281/200], Batch [200/938], Loss D: 0.7234, Loss G: 0.7200\n",
      "Epoch [281/200], Batch [300/938], Loss D: 0.6779, Loss G: 0.7953\n",
      "Epoch [281/200], Batch [400/938], Loss D: 0.6550, Loss G: 0.7893\n",
      "Epoch [281/200], Batch [500/938], Loss D: 0.6304, Loss G: 0.8139\n",
      "Epoch [281/200], Batch [600/938], Loss D: 0.6353, Loss G: 0.8029\n",
      "Epoch [281/200], Batch [700/938], Loss D: 0.6264, Loss G: 0.8514\n",
      "Epoch [281/200], Batch [800/938], Loss D: 0.6652, Loss G: 0.7522\n",
      "Epoch [281/200], Batch [900/938], Loss D: 0.6764, Loss G: 0.7657\n",
      "Epoch [282/200], Batch [0/938], Loss D: 0.6092, Loss G: 0.9263\n",
      "Epoch [282/200], Batch [100/938], Loss D: 0.6641, Loss G: 0.7076\n",
      "Epoch [282/200], Batch [200/938], Loss D: 0.6846, Loss G: 0.7206\n",
      "Epoch [282/200], Batch [300/938], Loss D: 0.6707, Loss G: 0.7694\n",
      "Epoch [282/200], Batch [400/938], Loss D: 0.7144, Loss G: 0.6885\n",
      "Epoch [282/200], Batch [500/938], Loss D: 0.6677, Loss G: 0.8379\n",
      "Epoch [282/200], Batch [600/938], Loss D: 0.6477, Loss G: 0.8117\n",
      "Epoch [282/200], Batch [700/938], Loss D: 0.6414, Loss G: 0.8069\n",
      "Epoch [282/200], Batch [800/938], Loss D: 0.7082, Loss G: 0.7792\n",
      "Epoch [282/200], Batch [900/938], Loss D: 0.6610, Loss G: 0.8008\n",
      "Epoch [283/200], Batch [0/938], Loss D: 0.6920, Loss G: 0.7300\n",
      "Epoch [283/200], Batch [100/938], Loss D: 0.7009, Loss G: 0.7258\n",
      "Epoch [283/200], Batch [200/938], Loss D: 0.6762, Loss G: 0.7423\n",
      "Epoch [283/200], Batch [300/938], Loss D: 0.6920, Loss G: 0.7814\n",
      "Epoch [283/200], Batch [400/938], Loss D: 0.6988, Loss G: 0.7910\n",
      "Epoch [283/200], Batch [500/938], Loss D: 0.6713, Loss G: 0.7918\n",
      "Epoch [283/200], Batch [600/938], Loss D: 0.6803, Loss G: 0.7664\n",
      "Epoch [283/200], Batch [700/938], Loss D: 0.6647, Loss G: 0.8549\n",
      "Epoch [283/200], Batch [800/938], Loss D: 0.6471, Loss G: 0.8663\n",
      "Epoch [283/200], Batch [900/938], Loss D: 0.6796, Loss G: 0.7945\n",
      "Epoch [284/200], Batch [0/938], Loss D: 0.6330, Loss G: 0.8316\n",
      "Epoch [284/200], Batch [100/938], Loss D: 0.6363, Loss G: 0.8395\n",
      "Epoch [284/200], Batch [200/938], Loss D: 0.7046, Loss G: 0.7867\n",
      "Epoch [284/200], Batch [300/938], Loss D: 0.6586, Loss G: 0.8538\n",
      "Epoch [284/200], Batch [400/938], Loss D: 0.7306, Loss G: 0.7027\n",
      "Epoch [284/200], Batch [500/938], Loss D: 0.6969, Loss G: 0.7368\n",
      "Epoch [284/200], Batch [600/938], Loss D: 0.7109, Loss G: 0.7105\n",
      "Epoch [284/200], Batch [700/938], Loss D: 0.6905, Loss G: 0.7653\n",
      "Epoch [284/200], Batch [800/938], Loss D: 0.6048, Loss G: 0.8295\n",
      "Epoch [284/200], Batch [900/938], Loss D: 0.6478, Loss G: 0.8304\n",
      "Epoch [285/200], Batch [0/938], Loss D: 0.6789, Loss G: 0.8220\n",
      "Epoch [285/200], Batch [100/938], Loss D: 0.7083, Loss G: 0.7386\n",
      "Epoch [285/200], Batch [200/938], Loss D: 0.6475, Loss G: 0.7625\n",
      "Epoch [285/200], Batch [300/938], Loss D: 0.6872, Loss G: 0.7321\n",
      "Epoch [285/200], Batch [400/938], Loss D: 0.6391, Loss G: 0.8614\n",
      "Epoch [285/200], Batch [500/938], Loss D: 0.7149, Loss G: 0.7483\n",
      "Epoch [285/200], Batch [600/938], Loss D: 0.6558, Loss G: 0.7416\n",
      "Epoch [285/200], Batch [700/938], Loss D: 0.7285, Loss G: 0.6918\n",
      "Epoch [285/200], Batch [800/938], Loss D: 0.6454, Loss G: 0.8302\n",
      "Epoch [285/200], Batch [900/938], Loss D: 0.6928, Loss G: 0.6918\n",
      "Epoch [286/200], Batch [0/938], Loss D: 0.6417, Loss G: 0.7825\n",
      "Epoch [286/200], Batch [100/938], Loss D: 0.6602, Loss G: 0.8279\n",
      "Epoch [286/200], Batch [200/938], Loss D: 0.6896, Loss G: 0.7771\n",
      "Epoch [286/200], Batch [300/938], Loss D: 0.6186, Loss G: 0.8322\n",
      "Epoch [286/200], Batch [400/938], Loss D: 0.6965, Loss G: 0.7271\n",
      "Epoch [286/200], Batch [500/938], Loss D: 0.6953, Loss G: 0.7477\n",
      "Epoch [286/200], Batch [600/938], Loss D: 0.6861, Loss G: 0.7765\n",
      "Epoch [286/200], Batch [700/938], Loss D: 0.6905, Loss G: 0.7498\n",
      "Epoch [286/200], Batch [800/938], Loss D: 0.6884, Loss G: 0.7458\n",
      "Epoch [286/200], Batch [900/938], Loss D: 0.7042, Loss G: 0.7178\n",
      "Epoch [287/200], Batch [0/938], Loss D: 0.7256, Loss G: 0.7184\n",
      "Epoch [287/200], Batch [100/938], Loss D: 0.6481, Loss G: 0.7899\n",
      "Epoch [287/200], Batch [200/938], Loss D: 0.7111, Loss G: 0.7265\n",
      "Epoch [287/200], Batch [300/938], Loss D: 0.6987, Loss G: 0.7413\n",
      "Epoch [287/200], Batch [400/938], Loss D: 0.6889, Loss G: 0.7143\n",
      "Epoch [287/200], Batch [500/938], Loss D: 0.6501, Loss G: 0.7645\n",
      "Epoch [287/200], Batch [600/938], Loss D: 0.6451, Loss G: 0.7958\n",
      "Epoch [287/200], Batch [700/938], Loss D: 0.6583, Loss G: 0.8233\n",
      "Epoch [287/200], Batch [800/938], Loss D: 0.6691, Loss G: 0.7685\n",
      "Epoch [287/200], Batch [900/938], Loss D: 0.7399, Loss G: 0.7468\n",
      "Epoch [288/200], Batch [0/938], Loss D: 0.6893, Loss G: 0.7952\n",
      "Epoch [288/200], Batch [100/938], Loss D: 0.6844, Loss G: 0.7261\n",
      "Epoch [288/200], Batch [200/938], Loss D: 0.6966, Loss G: 0.7263\n",
      "Epoch [288/200], Batch [300/938], Loss D: 0.6600, Loss G: 0.8779\n",
      "Epoch [288/200], Batch [400/938], Loss D: 0.7143, Loss G: 0.7199\n",
      "Epoch [288/200], Batch [500/938], Loss D: 0.6703, Loss G: 0.7595\n",
      "Epoch [288/200], Batch [600/938], Loss D: 0.6824, Loss G: 0.7113\n",
      "Epoch [288/200], Batch [700/938], Loss D: 0.6805, Loss G: 0.7705\n",
      "Epoch [288/200], Batch [800/938], Loss D: 0.6370, Loss G: 0.8242\n",
      "Epoch [288/200], Batch [900/938], Loss D: 0.6883, Loss G: 0.6761\n",
      "Epoch [289/200], Batch [0/938], Loss D: 0.6879, Loss G: 0.7360\n",
      "Epoch [289/200], Batch [100/938], Loss D: 0.6570, Loss G: 0.8283\n",
      "Epoch [289/200], Batch [200/938], Loss D: 0.7390, Loss G: 0.7434\n",
      "Epoch [289/200], Batch [300/938], Loss D: 0.7089, Loss G: 0.7596\n",
      "Epoch [289/200], Batch [400/938], Loss D: 0.6977, Loss G: 0.7720\n",
      "Epoch [289/200], Batch [500/938], Loss D: 0.6971, Loss G: 0.7421\n",
      "Epoch [289/200], Batch [600/938], Loss D: 0.6627, Loss G: 0.7983\n",
      "Epoch [289/200], Batch [700/938], Loss D: 0.6519, Loss G: 0.8214\n",
      "Epoch [289/200], Batch [800/938], Loss D: 0.7082, Loss G: 0.7011\n",
      "Epoch [289/200], Batch [900/938], Loss D: 0.6715, Loss G: 0.8014\n",
      "Epoch [290/200], Batch [0/938], Loss D: 0.7602, Loss G: 0.6624\n",
      "Epoch [290/200], Batch [100/938], Loss D: 0.6611, Loss G: 0.8222\n",
      "Epoch [290/200], Batch [200/938], Loss D: 0.6847, Loss G: 0.7705\n",
      "Epoch [290/200], Batch [300/938], Loss D: 0.7008, Loss G: 0.7298\n",
      "Epoch [290/200], Batch [400/938], Loss D: 0.6743, Loss G: 0.7781\n",
      "Epoch [290/200], Batch [500/938], Loss D: 0.6515, Loss G: 0.7915\n",
      "Epoch [290/200], Batch [600/938], Loss D: 0.6511, Loss G: 0.7785\n",
      "Epoch [290/200], Batch [700/938], Loss D: 0.6322, Loss G: 0.7876\n",
      "Epoch [290/200], Batch [800/938], Loss D: 0.6383, Loss G: 0.8430\n",
      "Epoch [290/200], Batch [900/938], Loss D: 0.7027, Loss G: 0.7453\n",
      "Epoch [291/200], Batch [0/938], Loss D: 0.6506, Loss G: 0.7641\n",
      "Epoch [291/200], Batch [100/938], Loss D: 0.7216, Loss G: 0.7123\n",
      "Epoch [291/200], Batch [200/938], Loss D: 0.6871, Loss G: 0.7005\n",
      "Epoch [291/200], Batch [300/938], Loss D: 0.6578, Loss G: 0.7517\n",
      "Epoch [291/200], Batch [400/938], Loss D: 0.7020, Loss G: 0.7714\n",
      "Epoch [291/200], Batch [500/938], Loss D: 0.6899, Loss G: 0.7303\n",
      "Epoch [291/200], Batch [600/938], Loss D: 0.6415, Loss G: 0.8227\n",
      "Epoch [291/200], Batch [700/938], Loss D: 0.7066, Loss G: 0.7343\n",
      "Epoch [291/200], Batch [800/938], Loss D: 0.6955, Loss G: 0.7333\n",
      "Epoch [291/200], Batch [900/938], Loss D: 0.6817, Loss G: 0.7269\n",
      "Epoch [292/200], Batch [0/938], Loss D: 0.6736, Loss G: 0.7434\n",
      "Epoch [292/200], Batch [100/938], Loss D: 0.6968, Loss G: 0.7410\n",
      "Epoch [292/200], Batch [200/938], Loss D: 0.6955, Loss G: 0.6910\n",
      "Epoch [292/200], Batch [300/938], Loss D: 0.6726, Loss G: 0.7357\n",
      "Epoch [292/200], Batch [400/938], Loss D: 0.6784, Loss G: 0.7603\n",
      "Epoch [292/200], Batch [500/938], Loss D: 0.7070, Loss G: 0.7264\n",
      "Epoch [292/200], Batch [600/938], Loss D: 0.6629, Loss G: 0.7771\n",
      "Epoch [292/200], Batch [700/938], Loss D: 0.6250, Loss G: 0.8120\n",
      "Epoch [292/200], Batch [800/938], Loss D: 0.7249, Loss G: 0.7045\n",
      "Epoch [292/200], Batch [900/938], Loss D: 0.6994, Loss G: 0.7405\n",
      "Epoch [293/200], Batch [0/938], Loss D: 0.6917, Loss G: 0.7881\n",
      "Epoch [293/200], Batch [100/938], Loss D: 0.6876, Loss G: 0.7338\n",
      "Epoch [293/200], Batch [200/938], Loss D: 0.6848, Loss G: 0.7745\n",
      "Epoch [293/200], Batch [300/938], Loss D: 0.6769, Loss G: 0.7938\n",
      "Epoch [293/200], Batch [400/938], Loss D: 0.6616, Loss G: 0.7866\n",
      "Epoch [293/200], Batch [500/938], Loss D: 0.6514, Loss G: 0.7668\n",
      "Epoch [293/200], Batch [600/938], Loss D: 0.7092, Loss G: 0.6921\n",
      "Epoch [293/200], Batch [700/938], Loss D: 0.7027, Loss G: 0.7218\n",
      "Epoch [293/200], Batch [800/938], Loss D: 0.6975, Loss G: 0.7104\n",
      "Epoch [293/200], Batch [900/938], Loss D: 0.7013, Loss G: 0.7482\n",
      "Epoch [294/200], Batch [0/938], Loss D: 0.7071, Loss G: 0.7380\n",
      "Epoch [294/200], Batch [100/938], Loss D: 0.6736, Loss G: 0.7474\n",
      "Epoch [294/200], Batch [200/938], Loss D: 0.6758, Loss G: 0.7596\n",
      "Epoch [294/200], Batch [300/938], Loss D: 0.6911, Loss G: 0.7028\n",
      "Epoch [294/200], Batch [400/938], Loss D: 0.6800, Loss G: 0.7388\n",
      "Epoch [294/200], Batch [500/938], Loss D: 0.6765, Loss G: 0.7299\n",
      "Epoch [294/200], Batch [600/938], Loss D: 0.6757, Loss G: 0.7156\n",
      "Epoch [294/200], Batch [700/938], Loss D: 0.6839, Loss G: 0.7098\n",
      "Epoch [294/200], Batch [800/938], Loss D: 0.6411, Loss G: 0.7949\n",
      "Epoch [294/200], Batch [900/938], Loss D: 0.6464, Loss G: 0.8384\n",
      "Epoch [295/200], Batch [0/938], Loss D: 0.6892, Loss G: 0.7313\n",
      "Epoch [295/200], Batch [100/938], Loss D: 0.6714, Loss G: 0.7149\n",
      "Epoch [295/200], Batch [200/938], Loss D: 0.7024, Loss G: 0.6810\n",
      "Epoch [295/200], Batch [300/938], Loss D: 0.6947, Loss G: 0.6943\n",
      "Epoch [295/200], Batch [400/938], Loss D: 0.6429, Loss G: 0.8094\n",
      "Epoch [295/200], Batch [500/938], Loss D: 0.6553, Loss G: 0.7681\n",
      "Epoch [295/200], Batch [600/938], Loss D: 0.6555, Loss G: 0.8071\n",
      "Epoch [295/200], Batch [700/938], Loss D: 0.7266, Loss G: 0.6864\n",
      "Epoch [295/200], Batch [800/938], Loss D: 0.6785, Loss G: 0.7165\n",
      "Epoch [295/200], Batch [900/938], Loss D: 0.6560, Loss G: 0.7752\n",
      "Epoch [296/200], Batch [0/938], Loss D: 0.7074, Loss G: 0.7049\n",
      "Epoch [296/200], Batch [100/938], Loss D: 0.6727, Loss G: 0.7487\n",
      "Epoch [296/200], Batch [200/938], Loss D: 0.6485, Loss G: 0.8155\n",
      "Epoch [296/200], Batch [300/938], Loss D: 0.7005, Loss G: 0.7245\n",
      "Epoch [296/200], Batch [400/938], Loss D: 0.7248, Loss G: 0.6848\n",
      "Epoch [296/200], Batch [500/938], Loss D: 0.7114, Loss G: 0.7144\n",
      "Epoch [296/200], Batch [600/938], Loss D: 0.6830, Loss G: 0.7093\n",
      "Epoch [296/200], Batch [700/938], Loss D: 0.7114, Loss G: 0.7054\n",
      "Epoch [296/200], Batch [800/938], Loss D: 0.6828, Loss G: 0.7028\n",
      "Epoch [296/200], Batch [900/938], Loss D: 0.7042, Loss G: 0.7313\n",
      "Epoch [297/200], Batch [0/938], Loss D: 0.7145, Loss G: 0.7016\n",
      "Epoch [297/200], Batch [100/938], Loss D: 0.6857, Loss G: 0.7152\n",
      "Epoch [297/200], Batch [200/938], Loss D: 0.6510, Loss G: 0.7660\n",
      "Epoch [297/200], Batch [300/938], Loss D: 0.6906, Loss G: 0.7399\n",
      "Epoch [297/200], Batch [400/938], Loss D: 0.6266, Loss G: 0.7903\n",
      "Epoch [297/200], Batch [500/938], Loss D: 0.6485, Loss G: 0.7753\n",
      "Epoch [297/200], Batch [600/938], Loss D: 0.6542, Loss G: 0.7357\n",
      "Epoch [297/200], Batch [700/938], Loss D: 0.6957, Loss G: 0.7595\n",
      "Epoch [297/200], Batch [800/938], Loss D: 0.6410, Loss G: 0.7926\n",
      "Epoch [297/200], Batch [900/938], Loss D: 0.6617, Loss G: 0.7198\n",
      "Epoch [298/200], Batch [0/938], Loss D: 0.6637, Loss G: 0.7720\n",
      "Epoch [298/200], Batch [100/938], Loss D: 0.6924, Loss G: 0.7426\n",
      "Epoch [298/200], Batch [200/938], Loss D: 0.6882, Loss G: 0.7520\n",
      "Epoch [298/200], Batch [300/938], Loss D: 0.6931, Loss G: 0.7447\n",
      "Epoch [298/200], Batch [400/938], Loss D: 0.6456, Loss G: 0.7861\n",
      "Epoch [298/200], Batch [500/938], Loss D: 0.6599, Loss G: 0.7873\n",
      "Epoch [298/200], Batch [600/938], Loss D: 0.6204, Loss G: 0.8391\n",
      "Epoch [298/200], Batch [700/938], Loss D: 0.6857, Loss G: 0.7233\n",
      "Epoch [298/200], Batch [800/938], Loss D: 0.6592, Loss G: 0.8099\n",
      "Epoch [298/200], Batch [900/938], Loss D: 0.6668, Loss G: 0.7935\n",
      "Epoch [299/200], Batch [0/938], Loss D: 0.6724, Loss G: 0.6874\n",
      "Epoch [299/200], Batch [100/938], Loss D: 0.6771, Loss G: 0.7712\n",
      "Epoch [299/200], Batch [200/938], Loss D: 0.6663, Loss G: 0.7310\n",
      "Epoch [299/200], Batch [300/938], Loss D: 0.6549, Loss G: 0.7748\n",
      "Epoch [299/200], Batch [400/938], Loss D: 0.6567, Loss G: 0.7842\n",
      "Epoch [299/200], Batch [500/938], Loss D: 0.6830, Loss G: 0.7225\n",
      "Epoch [299/200], Batch [600/938], Loss D: 0.6533, Loss G: 0.8207\n",
      "Epoch [299/200], Batch [700/938], Loss D: 0.6808, Loss G: 0.7359\n",
      "Epoch [299/200], Batch [800/938], Loss D: 0.6480, Loss G: 0.7766\n",
      "Epoch [299/200], Batch [900/938], Loss D: 0.6837, Loss G: 0.7441\n",
      "Epoch [300/200], Batch [0/938], Loss D: 0.6842, Loss G: 0.7822\n",
      "Epoch [300/200], Batch [100/938], Loss D: 0.6824, Loss G: 0.7909\n",
      "Epoch [300/200], Batch [200/938], Loss D: 0.7138, Loss G: 0.7315\n",
      "Epoch [300/200], Batch [300/938], Loss D: 0.7061, Loss G: 0.6987\n",
      "Epoch [300/200], Batch [400/938], Loss D: 0.6968, Loss G: 0.7098\n",
      "Epoch [300/200], Batch [500/938], Loss D: 0.7139, Loss G: 0.7154\n",
      "Epoch [300/200], Batch [600/938], Loss D: 0.6846, Loss G: 0.7092\n",
      "Epoch [300/200], Batch [700/938], Loss D: 0.6506, Loss G: 0.7440\n",
      "Epoch [300/200], Batch [800/938], Loss D: 0.6811, Loss G: 0.7300\n",
      "Epoch [300/200], Batch [900/938], Loss D: 0.6936, Loss G: 0.7006\n",
      "Epoch [301/200], Batch [0/938], Loss D: 0.6941, Loss G: 0.7029\n",
      "Epoch [301/200], Batch [100/938], Loss D: 0.6979, Loss G: 0.7098\n",
      "Epoch [301/200], Batch [200/938], Loss D: 0.7203, Loss G: 0.7309\n",
      "Epoch [301/200], Batch [300/938], Loss D: 0.7209, Loss G: 0.7015\n",
      "Epoch [301/200], Batch [400/938], Loss D: 0.6967, Loss G: 0.7187\n",
      "Epoch [301/200], Batch [500/938], Loss D: 0.6689, Loss G: 0.7634\n",
      "Epoch [301/200], Batch [600/938], Loss D: 0.7192, Loss G: 0.7176\n",
      "Epoch [301/200], Batch [700/938], Loss D: 0.6580, Loss G: 0.7841\n",
      "Epoch [301/200], Batch [800/938], Loss D: 0.6966, Loss G: 0.7229\n",
      "Epoch [301/200], Batch [900/938], Loss D: 0.6847, Loss G: 0.7598\n",
      "Epoch [302/200], Batch [0/938], Loss D: 0.7095, Loss G: 0.7590\n",
      "Epoch [302/200], Batch [100/938], Loss D: 0.6534, Loss G: 0.7865\n",
      "Epoch [302/200], Batch [200/938], Loss D: 0.6749, Loss G: 0.7261\n",
      "Epoch [302/200], Batch [300/938], Loss D: 0.6966, Loss G: 0.7164\n",
      "Epoch [302/200], Batch [400/938], Loss D: 0.6890, Loss G: 0.7049\n",
      "Epoch [302/200], Batch [500/938], Loss D: 0.6745, Loss G: 0.7022\n",
      "Epoch [302/200], Batch [600/938], Loss D: 0.6823, Loss G: 0.7535\n",
      "Epoch [302/200], Batch [700/938], Loss D: 0.6971, Loss G: 0.7510\n",
      "Epoch [302/200], Batch [800/938], Loss D: 0.6783, Loss G: 0.7791\n",
      "Epoch [302/200], Batch [900/938], Loss D: 0.7088, Loss G: 0.7355\n",
      "Epoch [303/200], Batch [0/938], Loss D: 0.6986, Loss G: 0.7425\n",
      "Epoch [303/200], Batch [100/938], Loss D: 0.6721, Loss G: 0.7569\n",
      "Epoch [303/200], Batch [200/938], Loss D: 0.7021, Loss G: 0.7406\n",
      "Epoch [303/200], Batch [300/938], Loss D: 0.7235, Loss G: 0.6918\n",
      "Epoch [303/200], Batch [400/938], Loss D: 0.6651, Loss G: 0.7869\n",
      "Epoch [303/200], Batch [500/938], Loss D: 0.6385, Loss G: 0.8200\n",
      "Epoch [303/200], Batch [600/938], Loss D: 0.6747, Loss G: 0.7412\n",
      "Epoch [303/200], Batch [700/938], Loss D: 0.6933, Loss G: 0.6735\n",
      "Epoch [303/200], Batch [800/938], Loss D: 0.6813, Loss G: 0.7646\n",
      "Epoch [303/200], Batch [900/938], Loss D: 0.6641, Loss G: 0.7539\n",
      "Epoch [304/200], Batch [0/938], Loss D: 0.6816, Loss G: 0.7721\n",
      "Epoch [304/200], Batch [100/938], Loss D: 0.7162, Loss G: 0.6768\n",
      "Epoch [304/200], Batch [200/938], Loss D: 0.6625, Loss G: 0.7590\n",
      "Epoch [304/200], Batch [300/938], Loss D: 0.6851, Loss G: 0.7395\n",
      "Epoch [304/200], Batch [400/938], Loss D: 0.6615, Loss G: 0.7463\n",
      "Epoch [304/200], Batch [500/938], Loss D: 0.6782, Loss G: 0.7374\n",
      "Epoch [304/200], Batch [600/938], Loss D: 0.6638, Loss G: 0.7725\n",
      "Epoch [304/200], Batch [700/938], Loss D: 0.6577, Loss G: 0.7912\n",
      "Epoch [304/200], Batch [800/938], Loss D: 0.6916, Loss G: 0.7120\n",
      "Epoch [304/200], Batch [900/938], Loss D: 0.7038, Loss G: 0.7053\n",
      "Epoch [305/200], Batch [0/938], Loss D: 0.6694, Loss G: 0.7472\n",
      "Epoch [305/200], Batch [100/938], Loss D: 0.6861, Loss G: 0.7541\n",
      "Epoch [305/200], Batch [200/938], Loss D: 0.6520, Loss G: 0.7899\n",
      "Epoch [305/200], Batch [300/938], Loss D: 0.6922, Loss G: 0.7129\n",
      "Epoch [305/200], Batch [400/938], Loss D: 0.7104, Loss G: 0.6884\n",
      "Epoch [305/200], Batch [500/938], Loss D: 0.7165, Loss G: 0.7124\n",
      "Epoch [305/200], Batch [600/938], Loss D: 0.6907, Loss G: 0.7671\n",
      "Epoch [305/200], Batch [700/938], Loss D: 0.6612, Loss G: 0.8004\n",
      "Epoch [305/200], Batch [800/938], Loss D: 0.7488, Loss G: 0.6574\n",
      "Epoch [305/200], Batch [900/938], Loss D: 0.7082, Loss G: 0.7418\n",
      "Epoch [306/200], Batch [0/938], Loss D: 0.6982, Loss G: 0.7425\n",
      "Epoch [306/200], Batch [100/938], Loss D: 0.6931, Loss G: 0.7230\n",
      "Epoch [306/200], Batch [200/938], Loss D: 0.6803, Loss G: 0.7349\n",
      "Epoch [306/200], Batch [300/938], Loss D: 0.6648, Loss G: 0.8016\n",
      "Epoch [306/200], Batch [400/938], Loss D: 0.6665, Loss G: 0.7824\n",
      "Epoch [306/200], Batch [500/938], Loss D: 0.6345, Loss G: 0.7941\n",
      "Epoch [306/200], Batch [600/938], Loss D: 0.6583, Loss G: 0.7534\n",
      "Epoch [306/200], Batch [700/938], Loss D: 0.6558, Loss G: 0.7787\n",
      "Epoch [306/200], Batch [800/938], Loss D: 0.6777, Loss G: 0.7405\n",
      "Epoch [306/200], Batch [900/938], Loss D: 0.7110, Loss G: 0.7096\n",
      "Epoch [307/200], Batch [0/938], Loss D: 0.7161, Loss G: 0.6916\n",
      "Epoch [307/200], Batch [100/938], Loss D: 0.7078, Loss G: 0.7100\n",
      "Epoch [307/200], Batch [200/938], Loss D: 0.6946, Loss G: 0.7040\n",
      "Epoch [307/200], Batch [300/938], Loss D: 0.7082, Loss G: 0.6755\n",
      "Epoch [307/200], Batch [400/938], Loss D: 0.7124, Loss G: 0.7177\n",
      "Epoch [307/200], Batch [500/938], Loss D: 0.6769, Loss G: 0.7568\n",
      "Epoch [307/200], Batch [600/938], Loss D: 0.7123, Loss G: 0.7492\n",
      "Epoch [307/200], Batch [700/938], Loss D: 0.7057, Loss G: 0.7443\n",
      "Epoch [307/200], Batch [800/938], Loss D: 0.6443, Loss G: 0.7874\n",
      "Epoch [307/200], Batch [900/938], Loss D: 0.6662, Loss G: 0.7057\n",
      "Epoch [308/200], Batch [0/938], Loss D: 0.6930, Loss G: 0.7128\n",
      "Epoch [308/200], Batch [100/938], Loss D: 0.6428, Loss G: 0.7638\n",
      "Epoch [308/200], Batch [200/938], Loss D: 0.6650, Loss G: 0.7626\n",
      "Epoch [308/200], Batch [300/938], Loss D: 0.6858, Loss G: 0.7525\n",
      "Epoch [308/200], Batch [400/938], Loss D: 0.6752, Loss G: 0.7353\n",
      "Epoch [308/200], Batch [500/938], Loss D: 0.6936, Loss G: 0.7195\n",
      "Epoch [308/200], Batch [600/938], Loss D: 0.7257, Loss G: 0.7243\n",
      "Epoch [308/200], Batch [700/938], Loss D: 0.7331, Loss G: 0.7456\n",
      "Epoch [308/200], Batch [800/938], Loss D: 0.7155, Loss G: 0.7169\n",
      "Epoch [308/200], Batch [900/938], Loss D: 0.6717, Loss G: 0.7501\n",
      "Epoch [309/200], Batch [0/938], Loss D: 0.7013, Loss G: 0.7244\n",
      "Epoch [309/200], Batch [100/938], Loss D: 0.6992, Loss G: 0.7610\n",
      "Epoch [309/200], Batch [200/938], Loss D: 0.6832, Loss G: 0.7441\n",
      "Epoch [309/200], Batch [300/938], Loss D: 0.6833, Loss G: 0.8005\n",
      "Epoch [309/200], Batch [400/938], Loss D: 0.7205, Loss G: 0.6712\n",
      "Epoch [309/200], Batch [500/938], Loss D: 0.6626, Loss G: 0.7791\n",
      "Epoch [309/200], Batch [600/938], Loss D: 0.6922, Loss G: 0.7083\n",
      "Epoch [309/200], Batch [700/938], Loss D: 0.6831, Loss G: 0.7335\n",
      "Epoch [309/200], Batch [800/938], Loss D: 0.6778, Loss G: 0.6913\n",
      "Epoch [309/200], Batch [900/938], Loss D: 0.6917, Loss G: 0.7715\n",
      "Epoch [310/200], Batch [0/938], Loss D: 0.6830, Loss G: 0.7602\n",
      "Epoch [310/200], Batch [100/938], Loss D: 0.6882, Loss G: 0.6998\n",
      "Epoch [310/200], Batch [200/938], Loss D: 0.6965, Loss G: 0.7397\n",
      "Epoch [310/200], Batch [300/938], Loss D: 0.6611, Loss G: 0.7769\n",
      "Epoch [310/200], Batch [400/938], Loss D: 0.7024, Loss G: 0.7420\n",
      "Epoch [310/200], Batch [500/938], Loss D: 0.6600, Loss G: 0.7451\n",
      "Epoch [310/200], Batch [600/938], Loss D: 0.6621, Loss G: 0.8585\n",
      "Epoch [310/200], Batch [700/938], Loss D: 0.7286, Loss G: 0.6658\n",
      "Epoch [310/200], Batch [800/938], Loss D: 0.6749, Loss G: 0.7178\n",
      "Epoch [310/200], Batch [900/938], Loss D: 0.6153, Loss G: 0.8448\n",
      "Epoch [311/200], Batch [0/938], Loss D: 0.6840, Loss G: 0.7151\n",
      "Epoch [311/200], Batch [100/938], Loss D: 0.6696, Loss G: 0.7456\n",
      "Epoch [311/200], Batch [200/938], Loss D: 0.7119, Loss G: 0.6898\n",
      "Epoch [311/200], Batch [300/938], Loss D: 0.6745, Loss G: 0.7629\n",
      "Epoch [311/200], Batch [400/938], Loss D: 0.6871, Loss G: 0.7633\n",
      "Epoch [311/200], Batch [500/938], Loss D: 0.6827, Loss G: 0.7653\n",
      "Epoch [311/200], Batch [600/938], Loss D: 0.6851, Loss G: 0.6734\n",
      "Epoch [311/200], Batch [700/938], Loss D: 0.7244, Loss G: 0.6672\n",
      "Epoch [311/200], Batch [800/938], Loss D: 0.7069, Loss G: 0.7160\n",
      "Epoch [311/200], Batch [900/938], Loss D: 0.6731, Loss G: 0.7549\n",
      "Epoch [312/200], Batch [0/938], Loss D: 0.6924, Loss G: 0.7094\n",
      "Epoch [312/200], Batch [100/938], Loss D: 0.6952, Loss G: 0.7191\n",
      "Epoch [312/200], Batch [200/938], Loss D: 0.7197, Loss G: 0.6842\n",
      "Epoch [312/200], Batch [300/938], Loss D: 0.6637, Loss G: 0.7664\n",
      "Epoch [312/200], Batch [400/938], Loss D: 0.7003, Loss G: 0.7452\n",
      "Epoch [312/200], Batch [500/938], Loss D: 0.7023, Loss G: 0.7497\n",
      "Epoch [312/200], Batch [600/938], Loss D: 0.6608, Loss G: 0.7168\n",
      "Epoch [312/200], Batch [700/938], Loss D: 0.7456, Loss G: 0.6794\n",
      "Epoch [312/200], Batch [800/938], Loss D: 0.6644, Loss G: 0.7077\n",
      "Epoch [312/200], Batch [900/938], Loss D: 0.7129, Loss G: 0.6940\n",
      "Epoch [313/200], Batch [0/938], Loss D: 0.7177, Loss G: 0.6835\n",
      "Epoch [313/200], Batch [100/938], Loss D: 0.6627, Loss G: 0.7604\n",
      "Epoch [313/200], Batch [200/938], Loss D: 0.6887, Loss G: 0.7676\n",
      "Epoch [313/200], Batch [300/938], Loss D: 0.6771, Loss G: 0.7353\n",
      "Epoch [313/200], Batch [400/938], Loss D: 0.7454, Loss G: 0.6714\n",
      "Epoch [313/200], Batch [500/938], Loss D: 0.6727, Loss G: 0.7269\n",
      "Epoch [313/200], Batch [600/938], Loss D: 0.6885, Loss G: 0.7259\n",
      "Epoch [313/200], Batch [700/938], Loss D: 0.6935, Loss G: 0.7599\n",
      "Epoch [313/200], Batch [800/938], Loss D: 0.7187, Loss G: 0.6956\n",
      "Epoch [313/200], Batch [900/938], Loss D: 0.6771, Loss G: 0.7658\n",
      "Epoch [314/200], Batch [0/938], Loss D: 0.7235, Loss G: 0.7067\n",
      "Epoch [314/200], Batch [100/938], Loss D: 0.6452, Loss G: 0.7810\n",
      "Epoch [314/200], Batch [200/938], Loss D: 0.6682, Loss G: 0.7680\n",
      "Epoch [314/200], Batch [300/938], Loss D: 0.6744, Loss G: 0.7603\n",
      "Epoch [314/200], Batch [400/938], Loss D: 0.7053, Loss G: 0.7200\n",
      "Epoch [314/200], Batch [500/938], Loss D: 0.6940, Loss G: 0.6989\n",
      "Epoch [314/200], Batch [600/938], Loss D: 0.7193, Loss G: 0.6847\n",
      "Epoch [314/200], Batch [700/938], Loss D: 0.7288, Loss G: 0.6654\n",
      "Epoch [314/200], Batch [800/938], Loss D: 0.6718, Loss G: 0.7421\n",
      "Epoch [314/200], Batch [900/938], Loss D: 0.6290, Loss G: 0.8568\n",
      "Epoch [315/200], Batch [0/938], Loss D: 0.6379, Loss G: 0.7676\n",
      "Epoch [315/200], Batch [100/938], Loss D: 0.7107, Loss G: 0.6775\n",
      "Epoch [315/200], Batch [200/938], Loss D: 0.6784, Loss G: 0.7313\n",
      "Epoch [315/200], Batch [300/938], Loss D: 0.6739, Loss G: 0.7410\n",
      "Epoch [315/200], Batch [400/938], Loss D: 0.6716, Loss G: 0.7452\n",
      "Epoch [315/200], Batch [500/938], Loss D: 0.6783, Loss G: 0.7227\n",
      "Epoch [315/200], Batch [600/938], Loss D: 0.6987, Loss G: 0.6805\n",
      "Epoch [315/200], Batch [700/938], Loss D: 0.6814, Loss G: 0.7210\n",
      "Epoch [315/200], Batch [800/938], Loss D: 0.6624, Loss G: 0.7520\n",
      "Epoch [315/200], Batch [900/938], Loss D: 0.7072, Loss G: 0.7054\n",
      "Epoch [316/200], Batch [0/938], Loss D: 0.6866, Loss G: 0.7741\n",
      "Epoch [316/200], Batch [100/938], Loss D: 0.7431, Loss G: 0.7030\n",
      "Epoch [316/200], Batch [200/938], Loss D: 0.6988, Loss G: 0.7206\n",
      "Epoch [316/200], Batch [300/938], Loss D: 0.6973, Loss G: 0.7416\n",
      "Epoch [316/200], Batch [400/938], Loss D: 0.6439, Loss G: 0.7999\n",
      "Epoch [316/200], Batch [500/938], Loss D: 0.6632, Loss G: 0.7994\n",
      "Epoch [316/200], Batch [600/938], Loss D: 0.6954, Loss G: 0.7136\n",
      "Epoch [316/200], Batch [700/938], Loss D: 0.7175, Loss G: 0.6647\n",
      "Epoch [316/200], Batch [800/938], Loss D: 0.6399, Loss G: 0.8293\n",
      "Epoch [316/200], Batch [900/938], Loss D: 0.6585, Loss G: 0.8181\n",
      "Epoch [317/200], Batch [0/938], Loss D: 0.7058, Loss G: 0.6638\n",
      "Epoch [317/200], Batch [100/938], Loss D: 0.7212, Loss G: 0.6996\n",
      "Epoch [317/200], Batch [200/938], Loss D: 0.6541, Loss G: 0.7100\n",
      "Epoch [317/200], Batch [300/938], Loss D: 0.7213, Loss G: 0.6457\n",
      "Epoch [317/200], Batch [400/938], Loss D: 0.6742, Loss G: 0.7447\n",
      "Epoch [317/200], Batch [500/938], Loss D: 0.6547, Loss G: 0.7813\n",
      "Epoch [317/200], Batch [600/938], Loss D: 0.6859, Loss G: 0.7505\n",
      "Epoch [317/200], Batch [700/938], Loss D: 0.6745, Loss G: 0.7419\n",
      "Epoch [317/200], Batch [800/938], Loss D: 0.7066, Loss G: 0.7175\n",
      "Epoch [317/200], Batch [900/938], Loss D: 0.6540, Loss G: 0.8098\n",
      "Epoch [318/200], Batch [0/938], Loss D: 0.6609, Loss G: 0.7861\n",
      "Epoch [318/200], Batch [100/938], Loss D: 0.6464, Loss G: 0.8010\n",
      "Epoch [318/200], Batch [200/938], Loss D: 0.6585, Loss G: 0.7418\n",
      "Epoch [318/200], Batch [300/938], Loss D: 0.6789, Loss G: 0.7658\n",
      "Epoch [318/200], Batch [400/938], Loss D: 0.7201, Loss G: 0.7395\n",
      "Epoch [318/200], Batch [500/938], Loss D: 0.6723, Loss G: 0.7665\n",
      "Epoch [318/200], Batch [600/938], Loss D: 0.7000, Loss G: 0.6949\n",
      "Epoch [318/200], Batch [700/938], Loss D: 0.6888, Loss G: 0.7472\n",
      "Epoch [318/200], Batch [800/938], Loss D: 0.6837, Loss G: 0.8074\n",
      "Epoch [318/200], Batch [900/938], Loss D: 0.6469, Loss G: 0.8287\n",
      "Epoch [319/200], Batch [0/938], Loss D: 0.6678, Loss G: 0.7685\n",
      "Epoch [319/200], Batch [100/938], Loss D: 0.7007, Loss G: 0.7078\n",
      "Epoch [319/200], Batch [200/938], Loss D: 0.6578, Loss G: 0.7673\n",
      "Epoch [319/200], Batch [300/938], Loss D: 0.6706, Loss G: 0.7472\n",
      "Epoch [319/200], Batch [400/938], Loss D: 0.6886, Loss G: 0.7357\n",
      "Epoch [319/200], Batch [500/938], Loss D: 0.6535, Loss G: 0.8263\n",
      "Epoch [319/200], Batch [600/938], Loss D: 0.6677, Loss G: 0.7578\n",
      "Epoch [319/200], Batch [700/938], Loss D: 0.6455, Loss G: 0.7648\n",
      "Epoch [319/200], Batch [800/938], Loss D: 0.7159, Loss G: 0.7707\n",
      "Epoch [319/200], Batch [900/938], Loss D: 0.6884, Loss G: 0.7528\n",
      "Epoch [320/200], Batch [0/938], Loss D: 0.6853, Loss G: 0.7755\n",
      "Epoch [320/200], Batch [100/938], Loss D: 0.6841, Loss G: 0.7541\n",
      "Epoch [320/200], Batch [200/938], Loss D: 0.6536, Loss G: 0.8030\n",
      "Epoch [320/200], Batch [300/938], Loss D: 0.7051, Loss G: 0.7371\n",
      "Epoch [320/200], Batch [400/938], Loss D: 0.6835, Loss G: 0.7841\n",
      "Epoch [320/200], Batch [500/938], Loss D: 0.6803, Loss G: 0.7307\n",
      "Epoch [320/200], Batch [600/938], Loss D: 0.6865, Loss G: 0.7032\n",
      "Epoch [320/200], Batch [700/938], Loss D: 0.6341, Loss G: 0.8042\n",
      "Epoch [320/200], Batch [800/938], Loss D: 0.7115, Loss G: 0.7337\n",
      "Epoch [320/200], Batch [900/938], Loss D: 0.6972, Loss G: 0.7166\n",
      "Epoch [321/200], Batch [0/938], Loss D: 0.7222, Loss G: 0.7239\n",
      "Epoch [321/200], Batch [100/938], Loss D: 0.6906, Loss G: 0.7286\n",
      "Epoch [321/200], Batch [200/938], Loss D: 0.7364, Loss G: 0.6343\n",
      "Epoch [321/200], Batch [300/938], Loss D: 0.7077, Loss G: 0.7129\n",
      "Epoch [321/200], Batch [400/938], Loss D: 0.7123, Loss G: 0.7615\n",
      "Epoch [321/200], Batch [500/938], Loss D: 0.7180, Loss G: 0.7007\n",
      "Epoch [321/200], Batch [600/938], Loss D: 0.6881, Loss G: 0.7062\n",
      "Epoch [321/200], Batch [700/938], Loss D: 0.6909, Loss G: 0.7266\n",
      "Epoch [321/200], Batch [800/938], Loss D: 0.7017, Loss G: 0.7173\n",
      "Epoch [321/200], Batch [900/938], Loss D: 0.6809, Loss G: 0.7556\n",
      "Epoch [322/200], Batch [0/938], Loss D: 0.7160, Loss G: 0.7083\n",
      "Epoch [322/200], Batch [100/938], Loss D: 0.6841, Loss G: 0.7560\n",
      "Epoch [322/200], Batch [200/938], Loss D: 0.6830, Loss G: 0.6897\n",
      "Epoch [322/200], Batch [300/938], Loss D: 0.7090, Loss G: 0.7367\n",
      "Epoch [322/200], Batch [400/938], Loss D: 0.6700, Loss G: 0.7808\n",
      "Epoch [322/200], Batch [500/938], Loss D: 0.6812, Loss G: 0.7322\n",
      "Epoch [322/200], Batch [600/938], Loss D: 0.6572, Loss G: 0.7534\n",
      "Epoch [322/200], Batch [700/938], Loss D: 0.6743, Loss G: 0.7364\n",
      "Epoch [322/200], Batch [800/938], Loss D: 0.6402, Loss G: 0.8142\n",
      "Epoch [322/200], Batch [900/938], Loss D: 0.6770, Loss G: 0.7603\n",
      "Epoch [323/200], Batch [0/938], Loss D: 0.6388, Loss G: 0.7471\n",
      "Epoch [323/200], Batch [100/938], Loss D: 0.7250, Loss G: 0.7222\n",
      "Epoch [323/200], Batch [200/938], Loss D: 0.6754, Loss G: 0.7486\n",
      "Epoch [323/200], Batch [300/938], Loss D: 0.6801, Loss G: 0.7344\n",
      "Epoch [323/200], Batch [400/938], Loss D: 0.6735, Loss G: 0.7792\n",
      "Epoch [323/200], Batch [500/938], Loss D: 0.7084, Loss G: 0.6932\n",
      "Epoch [323/200], Batch [600/938], Loss D: 0.6708, Loss G: 0.7685\n",
      "Epoch [323/200], Batch [700/938], Loss D: 0.6610, Loss G: 0.7889\n",
      "Epoch [323/200], Batch [800/938], Loss D: 0.6515, Loss G: 0.7680\n",
      "Epoch [323/200], Batch [900/938], Loss D: 0.6689, Loss G: 0.7264\n",
      "Epoch [324/200], Batch [0/938], Loss D: 0.6690, Loss G: 0.7214\n",
      "Epoch [324/200], Batch [100/938], Loss D: 0.6827, Loss G: 0.7321\n",
      "Epoch [324/200], Batch [200/938], Loss D: 0.6632, Loss G: 0.7743\n",
      "Epoch [324/200], Batch [300/938], Loss D: 0.7605, Loss G: 0.6611\n",
      "Epoch [324/200], Batch [400/938], Loss D: 0.6943, Loss G: 0.7263\n",
      "Epoch [324/200], Batch [500/938], Loss D: 0.7031, Loss G: 0.7318\n",
      "Epoch [324/200], Batch [600/938], Loss D: 0.6821, Loss G: 0.7321\n",
      "Epoch [324/200], Batch [700/938], Loss D: 0.6991, Loss G: 0.7474\n",
      "Epoch [324/200], Batch [800/938], Loss D: 0.6301, Loss G: 0.7915\n",
      "Epoch [324/200], Batch [900/938], Loss D: 0.6763, Loss G: 0.7499\n",
      "Epoch [325/200], Batch [0/938], Loss D: 0.7359, Loss G: 0.6897\n",
      "Epoch [325/200], Batch [100/938], Loss D: 0.6523, Loss G: 0.7915\n",
      "Epoch [325/200], Batch [200/938], Loss D: 0.6678, Loss G: 0.7297\n",
      "Epoch [325/200], Batch [300/938], Loss D: 0.6666, Loss G: 0.7875\n",
      "Epoch [325/200], Batch [400/938], Loss D: 0.6893, Loss G: 0.7373\n",
      "Epoch [325/200], Batch [500/938], Loss D: 0.7043, Loss G: 0.7255\n",
      "Epoch [325/200], Batch [600/938], Loss D: 0.6337, Loss G: 0.7548\n",
      "Epoch [325/200], Batch [700/938], Loss D: 0.6852, Loss G: 0.7768\n",
      "Epoch [325/200], Batch [800/938], Loss D: 0.6908, Loss G: 0.7535\n",
      "Epoch [325/200], Batch [900/938], Loss D: 0.7074, Loss G: 0.7603\n",
      "Epoch [326/200], Batch [0/938], Loss D: 0.6386, Loss G: 0.7680\n",
      "Epoch [326/200], Batch [100/938], Loss D: 0.6861, Loss G: 0.6830\n",
      "Epoch [326/200], Batch [200/938], Loss D: 0.7161, Loss G: 0.7174\n",
      "Epoch [326/200], Batch [300/938], Loss D: 0.7119, Loss G: 0.6939\n",
      "Epoch [326/200], Batch [400/938], Loss D: 0.6550, Loss G: 0.7507\n",
      "Epoch [326/200], Batch [500/938], Loss D: 0.6990, Loss G: 0.7701\n",
      "Epoch [326/200], Batch [600/938], Loss D: 0.6733, Loss G: 0.7931\n",
      "Epoch [326/200], Batch [700/938], Loss D: 0.6977, Loss G: 0.7343\n",
      "Epoch [326/200], Batch [800/938], Loss D: 0.6629, Loss G: 0.7644\n",
      "Epoch [326/200], Batch [900/938], Loss D: 0.7061, Loss G: 0.7034\n",
      "Epoch [327/200], Batch [0/938], Loss D: 0.6813, Loss G: 0.7380\n",
      "Epoch [327/200], Batch [100/938], Loss D: 0.7169, Loss G: 0.6953\n",
      "Epoch [327/200], Batch [200/938], Loss D: 0.6921, Loss G: 0.7291\n",
      "Epoch [327/200], Batch [300/938], Loss D: 0.7226, Loss G: 0.6963\n",
      "Epoch [327/200], Batch [400/938], Loss D: 0.6930, Loss G: 0.7139\n",
      "Epoch [327/200], Batch [500/938], Loss D: 0.6857, Loss G: 0.7443\n",
      "Epoch [327/200], Batch [600/938], Loss D: 0.6954, Loss G: 0.7211\n",
      "Epoch [327/200], Batch [700/938], Loss D: 0.6680, Loss G: 0.7496\n",
      "Epoch [327/200], Batch [800/938], Loss D: 0.7133, Loss G: 0.7295\n",
      "Epoch [327/200], Batch [900/938], Loss D: 0.6566, Loss G: 0.7702\n",
      "Epoch [328/200], Batch [0/938], Loss D: 0.6982, Loss G: 0.7216\n",
      "Epoch [328/200], Batch [100/938], Loss D: 0.6526, Loss G: 0.7746\n",
      "Epoch [328/200], Batch [200/938], Loss D: 0.6728, Loss G: 0.7192\n",
      "Epoch [328/200], Batch [300/938], Loss D: 0.6819, Loss G: 0.7773\n",
      "Epoch [328/200], Batch [400/938], Loss D: 0.6894, Loss G: 0.7508\n",
      "Epoch [328/200], Batch [500/938], Loss D: 0.6725, Loss G: 0.7690\n",
      "Epoch [328/200], Batch [600/938], Loss D: 0.6909, Loss G: 0.7247\n",
      "Epoch [328/200], Batch [700/938], Loss D: 0.7140, Loss G: 0.7445\n",
      "Epoch [328/200], Batch [800/938], Loss D: 0.6724, Loss G: 0.7303\n",
      "Epoch [328/200], Batch [900/938], Loss D: 0.7081, Loss G: 0.7024\n",
      "Epoch [329/200], Batch [0/938], Loss D: 0.6498, Loss G: 0.8161\n",
      "Epoch [329/200], Batch [100/938], Loss D: 0.6586, Loss G: 0.7845\n",
      "Epoch [329/200], Batch [200/938], Loss D: 0.6806, Loss G: 0.7659\n",
      "Epoch [329/200], Batch [300/938], Loss D: 0.6665, Loss G: 0.7248\n",
      "Epoch [329/200], Batch [400/938], Loss D: 0.6476, Loss G: 0.7983\n",
      "Epoch [329/200], Batch [500/938], Loss D: 0.6453, Loss G: 0.7631\n",
      "Epoch [329/200], Batch [600/938], Loss D: 0.6552, Loss G: 0.7199\n",
      "Epoch [329/200], Batch [700/938], Loss D: 0.7089, Loss G: 0.6951\n",
      "Epoch [329/200], Batch [800/938], Loss D: 0.6708, Loss G: 0.7777\n",
      "Epoch [329/200], Batch [900/938], Loss D: 0.7076, Loss G: 0.7176\n",
      "Epoch [330/200], Batch [0/938], Loss D: 0.6759, Loss G: 0.7600\n",
      "Epoch [330/200], Batch [100/938], Loss D: 0.6833, Loss G: 0.7357\n",
      "Epoch [330/200], Batch [200/938], Loss D: 0.6479, Loss G: 0.8168\n",
      "Epoch [330/200], Batch [300/938], Loss D: 0.6772, Loss G: 0.7568\n",
      "Epoch [330/200], Batch [400/938], Loss D: 0.6922, Loss G: 0.7288\n",
      "Epoch [330/200], Batch [500/938], Loss D: 0.6743, Loss G: 0.7741\n",
      "Epoch [330/200], Batch [600/938], Loss D: 0.7097, Loss G: 0.6805\n",
      "Epoch [330/200], Batch [700/938], Loss D: 0.7038, Loss G: 0.6678\n",
      "Epoch [330/200], Batch [800/938], Loss D: 0.6401, Loss G: 0.8053\n",
      "Epoch [330/200], Batch [900/938], Loss D: 0.6849, Loss G: 0.7289\n",
      "Epoch [331/200], Batch [0/938], Loss D: 0.6600, Loss G: 0.7894\n",
      "Epoch [331/200], Batch [100/938], Loss D: 0.6951, Loss G: 0.7738\n",
      "Epoch [331/200], Batch [200/938], Loss D: 0.6937, Loss G: 0.7177\n",
      "Epoch [331/200], Batch [300/938], Loss D: 0.6993, Loss G: 0.7066\n",
      "Epoch [331/200], Batch [400/938], Loss D: 0.6731, Loss G: 0.7664\n",
      "Epoch [331/200], Batch [500/938], Loss D: 0.6697, Loss G: 0.7502\n",
      "Epoch [331/200], Batch [600/938], Loss D: 0.7150, Loss G: 0.7365\n",
      "Epoch [331/200], Batch [700/938], Loss D: 0.6220, Loss G: 0.7943\n",
      "Epoch [331/200], Batch [800/938], Loss D: 0.7007, Loss G: 0.7171\n",
      "Epoch [331/200], Batch [900/938], Loss D: 0.6997, Loss G: 0.7681\n",
      "Epoch [332/200], Batch [0/938], Loss D: 0.6674, Loss G: 0.7332\n",
      "Epoch [332/200], Batch [100/938], Loss D: 0.6849, Loss G: 0.7433\n",
      "Epoch [332/200], Batch [200/938], Loss D: 0.6390, Loss G: 0.7900\n",
      "Epoch [332/200], Batch [300/938], Loss D: 0.6726, Loss G: 0.7372\n",
      "Epoch [332/200], Batch [400/938], Loss D: 0.7055, Loss G: 0.6754\n",
      "Epoch [332/200], Batch [500/938], Loss D: 0.7155, Loss G: 0.6798\n",
      "Epoch [332/200], Batch [600/938], Loss D: 0.6874, Loss G: 0.7452\n",
      "Epoch [332/200], Batch [700/938], Loss D: 0.6622, Loss G: 0.7518\n",
      "Epoch [332/200], Batch [800/938], Loss D: 0.6720, Loss G: 0.7555\n",
      "Epoch [332/200], Batch [900/938], Loss D: 0.7169, Loss G: 0.6865\n",
      "Epoch [333/200], Batch [0/938], Loss D: 0.6907, Loss G: 0.7381\n",
      "Epoch [333/200], Batch [100/938], Loss D: 0.6672, Loss G: 0.7206\n",
      "Epoch [333/200], Batch [200/938], Loss D: 0.7421, Loss G: 0.6931\n",
      "Epoch [333/200], Batch [300/938], Loss D: 0.6558, Loss G: 0.7650\n",
      "Epoch [333/200], Batch [400/938], Loss D: 0.6970, Loss G: 0.7459\n",
      "Epoch [333/200], Batch [500/938], Loss D: 0.6848, Loss G: 0.7815\n",
      "Epoch [333/200], Batch [600/938], Loss D: 0.7077, Loss G: 0.7209\n",
      "Epoch [333/200], Batch [700/938], Loss D: 0.6521, Loss G: 0.7988\n",
      "Epoch [333/200], Batch [800/938], Loss D: 0.6857, Loss G: 0.7019\n",
      "Epoch [333/200], Batch [900/938], Loss D: 0.6600, Loss G: 0.7893\n",
      "Epoch [334/200], Batch [0/938], Loss D: 0.6843, Loss G: 0.7092\n",
      "Epoch [334/200], Batch [100/938], Loss D: 0.6660, Loss G: 0.7788\n",
      "Epoch [334/200], Batch [200/938], Loss D: 0.6954, Loss G: 0.7262\n",
      "Epoch [334/200], Batch [300/938], Loss D: 0.6728, Loss G: 0.7764\n",
      "Epoch [334/200], Batch [400/938], Loss D: 0.7052, Loss G: 0.7181\n",
      "Epoch [334/200], Batch [500/938], Loss D: 0.6964, Loss G: 0.7598\n",
      "Epoch [334/200], Batch [600/938], Loss D: 0.6576, Loss G: 0.7734\n",
      "Epoch [334/200], Batch [700/938], Loss D: 0.6671, Loss G: 0.7218\n",
      "Epoch [334/200], Batch [800/938], Loss D: 0.6775, Loss G: 0.7519\n",
      "Epoch [334/200], Batch [900/938], Loss D: 0.6517, Loss G: 0.7691\n",
      "Epoch [335/200], Batch [0/938], Loss D: 0.7146, Loss G: 0.7062\n",
      "Epoch [335/200], Batch [100/938], Loss D: 0.6771, Loss G: 0.7403\n",
      "Epoch [335/200], Batch [200/938], Loss D: 0.6908, Loss G: 0.7597\n",
      "Epoch [335/200], Batch [300/938], Loss D: 0.6948, Loss G: 0.7467\n",
      "Epoch [335/200], Batch [400/938], Loss D: 0.6679, Loss G: 0.7469\n",
      "Epoch [335/200], Batch [500/938], Loss D: 0.6766, Loss G: 0.7545\n",
      "Epoch [335/200], Batch [600/938], Loss D: 0.6911, Loss G: 0.6877\n",
      "Epoch [335/200], Batch [700/938], Loss D: 0.6800, Loss G: 0.7597\n",
      "Epoch [335/200], Batch [800/938], Loss D: 0.6701, Loss G: 0.7319\n",
      "Epoch [335/200], Batch [900/938], Loss D: 0.7207, Loss G: 0.6855\n",
      "Epoch [336/200], Batch [0/938], Loss D: 0.6788, Loss G: 0.7606\n",
      "Epoch [336/200], Batch [100/938], Loss D: 0.6703, Loss G: 0.7240\n",
      "Epoch [336/200], Batch [200/938], Loss D: 0.6637, Loss G: 0.7496\n",
      "Epoch [336/200], Batch [300/938], Loss D: 0.6794, Loss G: 0.7232\n",
      "Epoch [336/200], Batch [400/938], Loss D: 0.6722, Loss G: 0.7678\n",
      "Epoch [336/200], Batch [500/938], Loss D: 0.6908, Loss G: 0.6729\n",
      "Epoch [336/200], Batch [600/938], Loss D: 0.6974, Loss G: 0.7013\n",
      "Epoch [336/200], Batch [700/938], Loss D: 0.7096, Loss G: 0.6752\n",
      "Epoch [336/200], Batch [800/938], Loss D: 0.6916, Loss G: 0.7484\n",
      "Epoch [336/200], Batch [900/938], Loss D: 0.6916, Loss G: 0.7210\n",
      "Epoch [337/200], Batch [0/938], Loss D: 0.6542, Loss G: 0.8034\n",
      "Epoch [337/200], Batch [100/938], Loss D: 0.6975, Loss G: 0.7208\n",
      "Epoch [337/200], Batch [200/938], Loss D: 0.6890, Loss G: 0.6910\n",
      "Epoch [337/200], Batch [300/938], Loss D: 0.6435, Loss G: 0.7439\n",
      "Epoch [337/200], Batch [400/938], Loss D: 0.6697, Loss G: 0.7552\n",
      "Epoch [337/200], Batch [500/938], Loss D: 0.7036, Loss G: 0.7151\n",
      "Epoch [337/200], Batch [600/938], Loss D: 0.6899, Loss G: 0.7142\n",
      "Epoch [337/200], Batch [700/938], Loss D: 0.6862, Loss G: 0.7414\n",
      "Epoch [337/200], Batch [800/938], Loss D: 0.6435, Loss G: 0.8088\n",
      "Epoch [337/200], Batch [900/938], Loss D: 0.6887, Loss G: 0.7342\n",
      "Epoch [338/200], Batch [0/938], Loss D: 0.6662, Loss G: 0.7488\n",
      "Epoch [338/200], Batch [100/938], Loss D: 0.7069, Loss G: 0.7352\n",
      "Epoch [338/200], Batch [200/938], Loss D: 0.6594, Loss G: 0.7603\n",
      "Epoch [338/200], Batch [300/938], Loss D: 0.7102, Loss G: 0.6753\n",
      "Epoch [338/200], Batch [400/938], Loss D: 0.7063, Loss G: 0.7373\n",
      "Epoch [338/200], Batch [500/938], Loss D: 0.7255, Loss G: 0.6896\n",
      "Epoch [338/200], Batch [600/938], Loss D: 0.6968, Loss G: 0.7224\n",
      "Epoch [338/200], Batch [700/938], Loss D: 0.6935, Loss G: 0.7474\n",
      "Epoch [338/200], Batch [800/938], Loss D: 0.6921, Loss G: 0.7512\n",
      "Epoch [338/200], Batch [900/938], Loss D: 0.6877, Loss G: 0.7074\n",
      "Epoch [339/200], Batch [0/938], Loss D: 0.6878, Loss G: 0.7237\n",
      "Epoch [339/200], Batch [100/938], Loss D: 0.6799, Loss G: 0.7213\n",
      "Epoch [339/200], Batch [200/938], Loss D: 0.6705, Loss G: 0.7290\n",
      "Epoch [339/200], Batch [300/938], Loss D: 0.7292, Loss G: 0.6914\n",
      "Epoch [339/200], Batch [400/938], Loss D: 0.6537, Loss G: 0.7595\n",
      "Epoch [339/200], Batch [500/938], Loss D: 0.6511, Loss G: 0.7601\n",
      "Epoch [339/200], Batch [600/938], Loss D: 0.7057, Loss G: 0.7074\n",
      "Epoch [339/200], Batch [700/938], Loss D: 0.7200, Loss G: 0.6848\n",
      "Epoch [339/200], Batch [800/938], Loss D: 0.6776, Loss G: 0.7307\n",
      "Epoch [339/200], Batch [900/938], Loss D: 0.6788, Loss G: 0.7340\n",
      "Epoch [340/200], Batch [0/938], Loss D: 0.6792, Loss G: 0.7094\n",
      "Epoch [340/200], Batch [100/938], Loss D: 0.6587, Loss G: 0.7746\n",
      "Epoch [340/200], Batch [200/938], Loss D: 0.7036, Loss G: 0.7597\n",
      "Epoch [340/200], Batch [300/938], Loss D: 0.6816, Loss G: 0.7223\n",
      "Epoch [340/200], Batch [400/938], Loss D: 0.6664, Loss G: 0.7639\n",
      "Epoch [340/200], Batch [500/938], Loss D: 0.6824, Loss G: 0.7048\n",
      "Epoch [340/200], Batch [600/938], Loss D: 0.6964, Loss G: 0.7654\n",
      "Epoch [340/200], Batch [700/938], Loss D: 0.6754, Loss G: 0.7405\n",
      "Epoch [340/200], Batch [800/938], Loss D: 0.7084, Loss G: 0.7071\n",
      "Epoch [340/200], Batch [900/938], Loss D: 0.6764, Loss G: 0.7571\n",
      "Epoch [341/200], Batch [0/938], Loss D: 0.7193, Loss G: 0.6912\n",
      "Epoch [341/200], Batch [100/938], Loss D: 0.6902, Loss G: 0.7403\n",
      "Epoch [341/200], Batch [200/938], Loss D: 0.7206, Loss G: 0.7124\n",
      "Epoch [341/200], Batch [300/938], Loss D: 0.6465, Loss G: 0.7904\n",
      "Epoch [341/200], Batch [400/938], Loss D: 0.6874, Loss G: 0.7156\n",
      "Epoch [341/200], Batch [500/938], Loss D: 0.7041, Loss G: 0.7164\n",
      "Epoch [341/200], Batch [600/938], Loss D: 0.6696, Loss G: 0.7487\n",
      "Epoch [341/200], Batch [700/938], Loss D: 0.6754, Loss G: 0.7414\n",
      "Epoch [341/200], Batch [800/938], Loss D: 0.6984, Loss G: 0.6960\n",
      "Epoch [341/200], Batch [900/938], Loss D: 0.6874, Loss G: 0.7031\n",
      "Epoch [342/200], Batch [0/938], Loss D: 0.7216, Loss G: 0.7116\n",
      "Epoch [342/200], Batch [100/938], Loss D: 0.6702, Loss G: 0.7394\n",
      "Epoch [342/200], Batch [200/938], Loss D: 0.7012, Loss G: 0.7344\n",
      "Epoch [342/200], Batch [300/938], Loss D: 0.6863, Loss G: 0.6747\n",
      "Epoch [342/200], Batch [400/938], Loss D: 0.6796, Loss G: 0.7364\n",
      "Epoch [342/200], Batch [500/938], Loss D: 0.7102, Loss G: 0.7051\n",
      "Epoch [342/200], Batch [600/938], Loss D: 0.7200, Loss G: 0.7533\n",
      "Epoch [342/200], Batch [700/938], Loss D: 0.6832, Loss G: 0.7731\n",
      "Epoch [342/200], Batch [800/938], Loss D: 0.7237, Loss G: 0.6992\n",
      "Epoch [342/200], Batch [900/938], Loss D: 0.6666, Loss G: 0.7543\n",
      "Epoch [343/200], Batch [0/938], Loss D: 0.6778, Loss G: 0.7549\n",
      "Epoch [343/200], Batch [100/938], Loss D: 0.6886, Loss G: 0.7308\n",
      "Epoch [343/200], Batch [200/938], Loss D: 0.6941, Loss G: 0.7024\n",
      "Epoch [343/200], Batch [300/938], Loss D: 0.6685, Loss G: 0.7660\n",
      "Epoch [343/200], Batch [400/938], Loss D: 0.6858, Loss G: 0.7190\n",
      "Epoch [343/200], Batch [500/938], Loss D: 0.6604, Loss G: 0.7784\n",
      "Epoch [343/200], Batch [600/938], Loss D: 0.6431, Loss G: 0.7545\n",
      "Epoch [343/200], Batch [700/938], Loss D: 0.7138, Loss G: 0.7216\n",
      "Epoch [343/200], Batch [800/938], Loss D: 0.7072, Loss G: 0.6881\n",
      "Epoch [343/200], Batch [900/938], Loss D: 0.6745, Loss G: 0.7440\n",
      "Epoch [344/200], Batch [0/938], Loss D: 0.6598, Loss G: 0.7504\n",
      "Epoch [344/200], Batch [100/938], Loss D: 0.6872, Loss G: 0.7340\n",
      "Epoch [344/200], Batch [200/938], Loss D: 0.6719, Loss G: 0.7631\n",
      "Epoch [344/200], Batch [300/938], Loss D: 0.6885, Loss G: 0.7493\n",
      "Epoch [344/200], Batch [400/938], Loss D: 0.6689, Loss G: 0.7163\n",
      "Epoch [344/200], Batch [500/938], Loss D: 0.6757, Loss G: 0.7398\n",
      "Epoch [344/200], Batch [600/938], Loss D: 0.6457, Loss G: 0.7613\n",
      "Epoch [344/200], Batch [700/938], Loss D: 0.6876, Loss G: 0.7259\n",
      "Epoch [344/200], Batch [800/938], Loss D: 0.6929, Loss G: 0.7266\n",
      "Epoch [344/200], Batch [900/938], Loss D: 0.6789, Loss G: 0.7312\n",
      "Epoch [345/200], Batch [0/938], Loss D: 0.7127, Loss G: 0.6990\n",
      "Epoch [345/200], Batch [100/938], Loss D: 0.6707, Loss G: 0.7323\n",
      "Epoch [345/200], Batch [200/938], Loss D: 0.6879, Loss G: 0.7379\n",
      "Epoch [345/200], Batch [300/938], Loss D: 0.7002, Loss G: 0.7165\n",
      "Epoch [345/200], Batch [400/938], Loss D: 0.7237, Loss G: 0.7217\n",
      "Epoch [345/200], Batch [500/938], Loss D: 0.6672, Loss G: 0.7496\n",
      "Epoch [345/200], Batch [600/938], Loss D: 0.6723, Loss G: 0.7402\n",
      "Epoch [345/200], Batch [700/938], Loss D: 0.6980, Loss G: 0.7164\n",
      "Epoch [345/200], Batch [800/938], Loss D: 0.6911, Loss G: 0.6960\n",
      "Epoch [345/200], Batch [900/938], Loss D: 0.6732, Loss G: 0.7280\n",
      "Epoch [346/200], Batch [0/938], Loss D: 0.6991, Loss G: 0.6833\n",
      "Epoch [346/200], Batch [100/938], Loss D: 0.6903, Loss G: 0.7568\n",
      "Epoch [346/200], Batch [200/938], Loss D: 0.6793, Loss G: 0.7367\n",
      "Epoch [346/200], Batch [300/938], Loss D: 0.6698, Loss G: 0.7251\n",
      "Epoch [346/200], Batch [400/938], Loss D: 0.7051, Loss G: 0.6874\n",
      "Epoch [346/200], Batch [500/938], Loss D: 0.6651, Loss G: 0.7395\n",
      "Epoch [346/200], Batch [600/938], Loss D: 0.6939, Loss G: 0.7455\n",
      "Epoch [346/200], Batch [700/938], Loss D: 0.6647, Loss G: 0.7222\n",
      "Epoch [346/200], Batch [800/938], Loss D: 0.6825, Loss G: 0.7098\n",
      "Epoch [346/200], Batch [900/938], Loss D: 0.7003, Loss G: 0.6857\n",
      "Epoch [347/200], Batch [0/938], Loss D: 0.6878, Loss G: 0.7249\n",
      "Epoch [347/200], Batch [100/938], Loss D: 0.6831, Loss G: 0.7469\n",
      "Epoch [347/200], Batch [200/938], Loss D: 0.7137, Loss G: 0.6990\n",
      "Epoch [347/200], Batch [300/938], Loss D: 0.6787, Loss G: 0.7145\n",
      "Epoch [347/200], Batch [400/938], Loss D: 0.6924, Loss G: 0.7464\n",
      "Epoch [347/200], Batch [500/938], Loss D: 0.6869, Loss G: 0.7118\n",
      "Epoch [347/200], Batch [600/938], Loss D: 0.6997, Loss G: 0.7166\n",
      "Epoch [347/200], Batch [700/938], Loss D: 0.6938, Loss G: 0.7268\n",
      "Epoch [347/200], Batch [800/938], Loss D: 0.6726, Loss G: 0.7692\n",
      "Epoch [347/200], Batch [900/938], Loss D: 0.6863, Loss G: 0.7196\n",
      "Epoch [348/200], Batch [0/938], Loss D: 0.6780, Loss G: 0.7291\n",
      "Epoch [348/200], Batch [100/938], Loss D: 0.6968, Loss G: 0.7234\n",
      "Epoch [348/200], Batch [200/938], Loss D: 0.6909, Loss G: 0.7048\n",
      "Epoch [348/200], Batch [300/938], Loss D: 0.6899, Loss G: 0.7285\n",
      "Epoch [348/200], Batch [400/938], Loss D: 0.7041, Loss G: 0.7227\n",
      "Epoch [348/200], Batch [500/938], Loss D: 0.6725, Loss G: 0.7356\n",
      "Epoch [348/200], Batch [600/938], Loss D: 0.6934, Loss G: 0.7044\n",
      "Epoch [348/200], Batch [700/938], Loss D: 0.6817, Loss G: 0.7094\n",
      "Epoch [348/200], Batch [800/938], Loss D: 0.6761, Loss G: 0.7589\n",
      "Epoch [348/200], Batch [900/938], Loss D: 0.6859, Loss G: 0.7135\n",
      "Epoch [349/200], Batch [0/938], Loss D: 0.6523, Loss G: 0.7741\n",
      "Epoch [349/200], Batch [100/938], Loss D: 0.7005, Loss G: 0.7204\n",
      "Epoch [349/200], Batch [200/938], Loss D: 0.6525, Loss G: 0.7288\n",
      "Epoch [349/200], Batch [300/938], Loss D: 0.7095, Loss G: 0.7091\n",
      "Epoch [349/200], Batch [400/938], Loss D: 0.6822, Loss G: 0.7351\n",
      "Epoch [349/200], Batch [500/938], Loss D: 0.6640, Loss G: 0.7746\n",
      "Epoch [349/200], Batch [600/938], Loss D: 0.6740, Loss G: 0.7441\n",
      "Epoch [349/200], Batch [700/938], Loss D: 0.6392, Loss G: 0.7594\n",
      "Epoch [349/200], Batch [800/938], Loss D: 0.6717, Loss G: 0.7722\n",
      "Epoch [349/200], Batch [900/938], Loss D: 0.7271, Loss G: 0.6863\n",
      "Epoch [350/200], Batch [0/938], Loss D: 0.7034, Loss G: 0.7502\n",
      "Epoch [350/200], Batch [100/938], Loss D: 0.6805, Loss G: 0.7521\n",
      "Epoch [350/200], Batch [200/938], Loss D: 0.6973, Loss G: 0.7156\n",
      "Epoch [350/200], Batch [300/938], Loss D: 0.7027, Loss G: 0.6964\n",
      "Epoch [350/200], Batch [400/938], Loss D: 0.6625, Loss G: 0.7695\n",
      "Epoch [350/200], Batch [500/938], Loss D: 0.6803, Loss G: 0.7321\n",
      "Epoch [350/200], Batch [600/938], Loss D: 0.6352, Loss G: 0.8108\n",
      "Epoch [350/200], Batch [700/938], Loss D: 0.6892, Loss G: 0.6958\n",
      "Epoch [350/200], Batch [800/938], Loss D: 0.6876, Loss G: 0.7038\n",
      "Epoch [350/200], Batch [900/938], Loss D: 0.6819, Loss G: 0.7050\n",
      "Epoch [351/200], Batch [0/938], Loss D: 0.6661, Loss G: 0.7473\n",
      "Epoch [351/200], Batch [100/938], Loss D: 0.7082, Loss G: 0.7016\n",
      "Epoch [351/200], Batch [200/938], Loss D: 0.6834, Loss G: 0.6967\n",
      "Epoch [351/200], Batch [300/938], Loss D: 0.6667, Loss G: 0.8038\n",
      "Epoch [351/200], Batch [400/938], Loss D: 0.6784, Loss G: 0.7219\n",
      "Epoch [351/200], Batch [500/938], Loss D: 0.7129, Loss G: 0.7416\n",
      "Epoch [351/200], Batch [600/938], Loss D: 0.7086, Loss G: 0.7366\n",
      "Epoch [351/200], Batch [700/938], Loss D: 0.6384, Loss G: 0.7889\n",
      "Epoch [351/200], Batch [800/938], Loss D: 0.7177, Loss G: 0.7143\n",
      "Epoch [351/200], Batch [900/938], Loss D: 0.6853, Loss G: 0.6893\n",
      "Epoch [352/200], Batch [0/938], Loss D: 0.7026, Loss G: 0.6924\n",
      "Epoch [352/200], Batch [100/938], Loss D: 0.6908, Loss G: 0.7243\n",
      "Epoch [352/200], Batch [200/938], Loss D: 0.6718, Loss G: 0.7475\n",
      "Epoch [352/200], Batch [300/938], Loss D: 0.6913, Loss G: 0.7305\n",
      "Epoch [352/200], Batch [400/938], Loss D: 0.6840, Loss G: 0.7149\n",
      "Epoch [352/200], Batch [500/938], Loss D: 0.7186, Loss G: 0.7335\n",
      "Epoch [352/200], Batch [600/938], Loss D: 0.7052, Loss G: 0.6953\n",
      "Epoch [352/200], Batch [700/938], Loss D: 0.6561, Loss G: 0.7421\n",
      "Epoch [352/200], Batch [800/938], Loss D: 0.6865, Loss G: 0.7538\n",
      "Epoch [352/200], Batch [900/938], Loss D: 0.6843, Loss G: 0.7061\n",
      "Epoch [353/200], Batch [0/938], Loss D: 0.6784, Loss G: 0.7260\n",
      "Epoch [353/200], Batch [100/938], Loss D: 0.6733, Loss G: 0.7221\n",
      "Epoch [353/200], Batch [200/938], Loss D: 0.6760, Loss G: 0.7453\n",
      "Epoch [353/200], Batch [300/938], Loss D: 0.6588, Loss G: 0.7350\n",
      "Epoch [353/200], Batch [400/938], Loss D: 0.7027, Loss G: 0.7085\n",
      "Epoch [353/200], Batch [500/938], Loss D: 0.7111, Loss G: 0.6965\n",
      "Epoch [353/200], Batch [600/938], Loss D: 0.7142, Loss G: 0.6972\n",
      "Epoch [353/200], Batch [700/938], Loss D: 0.6576, Loss G: 0.7867\n",
      "Epoch [353/200], Batch [800/938], Loss D: 0.6819, Loss G: 0.7366\n",
      "Epoch [353/200], Batch [900/938], Loss D: 0.6367, Loss G: 0.7974\n",
      "Epoch [354/200], Batch [0/938], Loss D: 0.7148, Loss G: 0.6877\n",
      "Epoch [354/200], Batch [100/938], Loss D: 0.6553, Loss G: 0.7387\n",
      "Epoch [354/200], Batch [200/938], Loss D: 0.6906, Loss G: 0.7469\n",
      "Epoch [354/200], Batch [300/938], Loss D: 0.6910, Loss G: 0.7029\n",
      "Epoch [354/200], Batch [400/938], Loss D: 0.6871, Loss G: 0.7138\n",
      "Epoch [354/200], Batch [500/938], Loss D: 0.6986, Loss G: 0.6955\n",
      "Epoch [354/200], Batch [600/938], Loss D: 0.6768, Loss G: 0.7358\n",
      "Epoch [354/200], Batch [700/938], Loss D: 0.7020, Loss G: 0.7215\n",
      "Epoch [354/200], Batch [800/938], Loss D: 0.7210, Loss G: 0.7241\n",
      "Epoch [354/200], Batch [900/938], Loss D: 0.6869, Loss G: 0.7061\n",
      "Epoch [355/200], Batch [0/938], Loss D: 0.6891, Loss G: 0.7510\n",
      "Epoch [355/200], Batch [100/938], Loss D: 0.7138, Loss G: 0.6737\n",
      "Epoch [355/200], Batch [200/938], Loss D: 0.6900, Loss G: 0.7272\n",
      "Epoch [355/200], Batch [300/938], Loss D: 0.6684, Loss G: 0.7546\n",
      "Epoch [355/200], Batch [400/938], Loss D: 0.6733, Loss G: 0.7445\n",
      "Epoch [355/200], Batch [500/938], Loss D: 0.6727, Loss G: 0.7504\n",
      "Epoch [355/200], Batch [600/938], Loss D: 0.7086, Loss G: 0.7101\n",
      "Epoch [355/200], Batch [700/938], Loss D: 0.6875, Loss G: 0.7008\n",
      "Epoch [355/200], Batch [800/938], Loss D: 0.6934, Loss G: 0.7127\n",
      "Epoch [355/200], Batch [900/938], Loss D: 0.7026, Loss G: 0.7032\n",
      "Epoch [356/200], Batch [0/938], Loss D: 0.6694, Loss G: 0.7752\n",
      "Epoch [356/200], Batch [100/938], Loss D: 0.7005, Loss G: 0.7197\n",
      "Epoch [356/200], Batch [200/938], Loss D: 0.6836, Loss G: 0.7249\n",
      "Epoch [356/200], Batch [300/938], Loss D: 0.6603, Loss G: 0.7747\n",
      "Epoch [356/200], Batch [400/938], Loss D: 0.6351, Loss G: 0.7794\n",
      "Epoch [356/200], Batch [500/938], Loss D: 0.6656, Loss G: 0.7300\n",
      "Epoch [356/200], Batch [600/938], Loss D: 0.6895, Loss G: 0.7241\n",
      "Epoch [356/200], Batch [700/938], Loss D: 0.6983, Loss G: 0.7066\n",
      "Epoch [356/200], Batch [800/938], Loss D: 0.6461, Loss G: 0.7512\n",
      "Epoch [356/200], Batch [900/938], Loss D: 0.6592, Loss G: 0.7996\n",
      "Epoch [357/200], Batch [0/938], Loss D: 0.7006, Loss G: 0.7072\n",
      "Epoch [357/200], Batch [100/938], Loss D: 0.6673, Loss G: 0.7255\n",
      "Epoch [357/200], Batch [200/938], Loss D: 0.6436, Loss G: 0.7934\n",
      "Epoch [357/200], Batch [300/938], Loss D: 0.7027, Loss G: 0.6978\n",
      "Epoch [357/200], Batch [400/938], Loss D: 0.6799, Loss G: 0.7259\n",
      "Epoch [357/200], Batch [500/938], Loss D: 0.6675, Loss G: 0.7275\n",
      "Epoch [357/200], Batch [600/938], Loss D: 0.6923, Loss G: 0.7017\n",
      "Epoch [357/200], Batch [700/938], Loss D: 0.7007, Loss G: 0.7192\n",
      "Epoch [357/200], Batch [800/938], Loss D: 0.6800, Loss G: 0.7489\n",
      "Epoch [357/200], Batch [900/938], Loss D: 0.7052, Loss G: 0.7284\n",
      "Epoch [358/200], Batch [0/938], Loss D: 0.7000, Loss G: 0.7112\n",
      "Epoch [358/200], Batch [100/938], Loss D: 0.6755, Loss G: 0.7348\n",
      "Epoch [358/200], Batch [200/938], Loss D: 0.6946, Loss G: 0.7351\n",
      "Epoch [358/200], Batch [300/938], Loss D: 0.6900, Loss G: 0.7131\n",
      "Epoch [358/200], Batch [400/938], Loss D: 0.6795, Loss G: 0.7541\n",
      "Epoch [358/200], Batch [500/938], Loss D: 0.6872, Loss G: 0.6734\n",
      "Epoch [358/200], Batch [600/938], Loss D: 0.6835, Loss G: 0.7123\n",
      "Epoch [358/200], Batch [700/938], Loss D: 0.6781, Loss G: 0.7268\n",
      "Epoch [358/200], Batch [800/938], Loss D: 0.6661, Loss G: 0.7788\n",
      "Epoch [358/200], Batch [900/938], Loss D: 0.6844, Loss G: 0.7296\n",
      "Epoch [359/200], Batch [0/938], Loss D: 0.6442, Loss G: 0.7857\n",
      "Epoch [359/200], Batch [100/938], Loss D: 0.7116, Loss G: 0.7312\n",
      "Epoch [359/200], Batch [200/938], Loss D: 0.6809, Loss G: 0.7491\n",
      "Epoch [359/200], Batch [300/938], Loss D: 0.6783, Loss G: 0.7586\n",
      "Epoch [359/200], Batch [400/938], Loss D: 0.6816, Loss G: 0.7555\n",
      "Epoch [359/200], Batch [500/938], Loss D: 0.6658, Loss G: 0.7181\n",
      "Epoch [359/200], Batch [600/938], Loss D: 0.6782, Loss G: 0.7457\n",
      "Epoch [359/200], Batch [700/938], Loss D: 0.6767, Loss G: 0.7448\n",
      "Epoch [359/200], Batch [800/938], Loss D: 0.7324, Loss G: 0.7221\n",
      "Epoch [359/200], Batch [900/938], Loss D: 0.6913, Loss G: 0.7412\n",
      "Epoch [360/200], Batch [0/938], Loss D: 0.6682, Loss G: 0.7285\n",
      "Epoch [360/200], Batch [100/938], Loss D: 0.7068, Loss G: 0.7083\n",
      "Epoch [360/200], Batch [200/938], Loss D: 0.6817, Loss G: 0.7566\n",
      "Epoch [360/200], Batch [300/938], Loss D: 0.6396, Loss G: 0.7926\n",
      "Epoch [360/200], Batch [400/938], Loss D: 0.7006, Loss G: 0.6911\n",
      "Epoch [360/200], Batch [500/938], Loss D: 0.6623, Loss G: 0.7509\n",
      "Epoch [360/200], Batch [600/938], Loss D: 0.6901, Loss G: 0.7362\n",
      "Epoch [360/200], Batch [700/938], Loss D: 0.6609, Loss G: 0.7379\n",
      "Epoch [360/200], Batch [800/938], Loss D: 0.6816, Loss G: 0.7386\n",
      "Epoch [360/200], Batch [900/938], Loss D: 0.6838, Loss G: 0.7452\n",
      "Epoch [361/200], Batch [0/938], Loss D: 0.6966, Loss G: 0.6802\n",
      "Epoch [361/200], Batch [100/938], Loss D: 0.6593, Loss G: 0.7764\n",
      "Epoch [361/200], Batch [200/938], Loss D: 0.6874, Loss G: 0.7019\n",
      "Epoch [361/200], Batch [300/938], Loss D: 0.6855, Loss G: 0.7478\n",
      "Epoch [361/200], Batch [400/938], Loss D: 0.6549, Loss G: 0.7790\n",
      "Epoch [361/200], Batch [500/938], Loss D: 0.6792, Loss G: 0.7713\n",
      "Epoch [361/200], Batch [600/938], Loss D: 0.6764, Loss G: 0.7497\n",
      "Epoch [361/200], Batch [700/938], Loss D: 0.6852, Loss G: 0.7025\n",
      "Epoch [361/200], Batch [800/938], Loss D: 0.6662, Loss G: 0.7521\n",
      "Epoch [361/200], Batch [900/938], Loss D: 0.6724, Loss G: 0.7564\n",
      "Epoch [362/200], Batch [0/938], Loss D: 0.6773, Loss G: 0.7132\n",
      "Epoch [362/200], Batch [100/938], Loss D: 0.6523, Loss G: 0.8013\n",
      "Epoch [362/200], Batch [200/938], Loss D: 0.6662, Loss G: 0.7822\n",
      "Epoch [362/200], Batch [300/938], Loss D: 0.6505, Loss G: 0.8135\n",
      "Epoch [362/200], Batch [400/938], Loss D: 0.7122, Loss G: 0.7229\n",
      "Epoch [362/200], Batch [500/938], Loss D: 0.6701, Loss G: 0.7512\n",
      "Epoch [362/200], Batch [600/938], Loss D: 0.7012, Loss G: 0.7615\n",
      "Epoch [362/200], Batch [700/938], Loss D: 0.6971, Loss G: 0.7205\n",
      "Epoch [362/200], Batch [800/938], Loss D: 0.6589, Loss G: 0.8034\n",
      "Epoch [362/200], Batch [900/938], Loss D: 0.6832, Loss G: 0.6947\n",
      "Epoch [363/200], Batch [0/938], Loss D: 0.6927, Loss G: 0.7308\n",
      "Epoch [363/200], Batch [100/938], Loss D: 0.7238, Loss G: 0.7341\n",
      "Epoch [363/200], Batch [200/938], Loss D: 0.6689, Loss G: 0.8046\n",
      "Epoch [363/200], Batch [300/938], Loss D: 0.6892, Loss G: 0.7110\n",
      "Epoch [363/200], Batch [400/938], Loss D: 0.6691, Loss G: 0.7304\n",
      "Epoch [363/200], Batch [500/938], Loss D: 0.6839, Loss G: 0.7388\n",
      "Epoch [363/200], Batch [600/938], Loss D: 0.6514, Loss G: 0.7325\n",
      "Epoch [363/200], Batch [700/938], Loss D: 0.6855, Loss G: 0.7415\n",
      "Epoch [363/200], Batch [800/938], Loss D: 0.6571, Loss G: 0.7758\n",
      "Epoch [363/200], Batch [900/938], Loss D: 0.6769, Loss G: 0.7423\n",
      "Epoch [364/200], Batch [0/938], Loss D: 0.6823, Loss G: 0.7054\n",
      "Epoch [364/200], Batch [100/938], Loss D: 0.6718, Loss G: 0.7298\n",
      "Epoch [364/200], Batch [200/938], Loss D: 0.6788, Loss G: 0.7476\n",
      "Epoch [364/200], Batch [300/938], Loss D: 0.6596, Loss G: 0.7585\n",
      "Epoch [364/200], Batch [400/938], Loss D: 0.7105, Loss G: 0.7180\n",
      "Epoch [364/200], Batch [500/938], Loss D: 0.6821, Loss G: 0.7146\n",
      "Epoch [364/200], Batch [600/938], Loss D: 0.6597, Loss G: 0.7514\n",
      "Epoch [364/200], Batch [700/938], Loss D: 0.6922, Loss G: 0.7114\n",
      "Epoch [364/200], Batch [800/938], Loss D: 0.6814, Loss G: 0.7350\n",
      "Epoch [364/200], Batch [900/938], Loss D: 0.7081, Loss G: 0.7427\n",
      "Epoch [365/200], Batch [0/938], Loss D: 0.6393, Loss G: 0.7960\n",
      "Epoch [365/200], Batch [100/938], Loss D: 0.6643, Loss G: 0.7375\n",
      "Epoch [365/200], Batch [200/938], Loss D: 0.6657, Loss G: 0.7199\n",
      "Epoch [365/200], Batch [300/938], Loss D: 0.6672, Loss G: 0.7405\n",
      "Epoch [365/200], Batch [400/938], Loss D: 0.6737, Loss G: 0.7644\n",
      "Epoch [365/200], Batch [500/938], Loss D: 0.6765, Loss G: 0.7265\n",
      "Epoch [365/200], Batch [600/938], Loss D: 0.6833, Loss G: 0.7166\n",
      "Epoch [365/200], Batch [700/938], Loss D: 0.7195, Loss G: 0.6898\n",
      "Epoch [365/200], Batch [800/938], Loss D: 0.6738, Loss G: 0.7443\n",
      "Epoch [365/200], Batch [900/938], Loss D: 0.6873, Loss G: 0.7077\n",
      "Epoch [366/200], Batch [0/938], Loss D: 0.7073, Loss G: 0.7010\n",
      "Epoch [366/200], Batch [100/938], Loss D: 0.6695, Loss G: 0.7271\n",
      "Epoch [366/200], Batch [200/938], Loss D: 0.6982, Loss G: 0.7425\n",
      "Epoch [366/200], Batch [300/938], Loss D: 0.7000, Loss G: 0.7068\n",
      "Epoch [366/200], Batch [400/938], Loss D: 0.7237, Loss G: 0.6932\n",
      "Epoch [366/200], Batch [500/938], Loss D: 0.6737, Loss G: 0.7533\n",
      "Epoch [366/200], Batch [600/938], Loss D: 0.6952, Loss G: 0.7578\n",
      "Epoch [366/200], Batch [700/938], Loss D: 0.6990, Loss G: 0.7402\n",
      "Epoch [366/200], Batch [800/938], Loss D: 0.6917, Loss G: 0.7085\n",
      "Epoch [366/200], Batch [900/938], Loss D: 0.6598, Loss G: 0.7581\n",
      "Epoch [367/200], Batch [0/938], Loss D: 0.6981, Loss G: 0.7076\n",
      "Epoch [367/200], Batch [100/938], Loss D: 0.7067, Loss G: 0.7665\n",
      "Epoch [367/200], Batch [200/938], Loss D: 0.6472, Loss G: 0.7862\n",
      "Epoch [367/200], Batch [300/938], Loss D: 0.6899, Loss G: 0.7458\n",
      "Epoch [367/200], Batch [400/938], Loss D: 0.6731, Loss G: 0.7066\n",
      "Epoch [367/200], Batch [500/938], Loss D: 0.6938, Loss G: 0.7468\n",
      "Epoch [367/200], Batch [600/938], Loss D: 0.6978, Loss G: 0.7215\n",
      "Epoch [367/200], Batch [700/938], Loss D: 0.6759, Loss G: 0.7199\n",
      "Epoch [367/200], Batch [800/938], Loss D: 0.6828, Loss G: 0.7838\n",
      "Epoch [367/200], Batch [900/938], Loss D: 0.7108, Loss G: 0.6914\n",
      "Epoch [368/200], Batch [0/938], Loss D: 0.6885, Loss G: 0.7575\n",
      "Epoch [368/200], Batch [100/938], Loss D: 0.6880, Loss G: 0.7602\n",
      "Epoch [368/200], Batch [200/938], Loss D: 0.6654, Loss G: 0.7540\n",
      "Epoch [368/200], Batch [300/938], Loss D: 0.6835, Loss G: 0.7665\n",
      "Epoch [368/200], Batch [400/938], Loss D: 0.6633, Loss G: 0.7332\n",
      "Epoch [368/200], Batch [500/938], Loss D: 0.6699, Loss G: 0.7234\n",
      "Epoch [368/200], Batch [600/938], Loss D: 0.6706, Loss G: 0.7897\n",
      "Epoch [368/200], Batch [700/938], Loss D: 0.6830, Loss G: 0.7565\n",
      "Epoch [368/200], Batch [800/938], Loss D: 0.6716, Loss G: 0.7148\n",
      "Epoch [368/200], Batch [900/938], Loss D: 0.6377, Loss G: 0.7755\n",
      "Epoch [369/200], Batch [0/938], Loss D: 0.7088, Loss G: 0.7117\n",
      "Epoch [369/200], Batch [100/938], Loss D: 0.6734, Loss G: 0.7436\n",
      "Epoch [369/200], Batch [200/938], Loss D: 0.6716, Loss G: 0.7589\n",
      "Epoch [369/200], Batch [300/938], Loss D: 0.6607, Loss G: 0.7581\n",
      "Epoch [369/200], Batch [400/938], Loss D: 0.7171, Loss G: 0.7416\n",
      "Epoch [369/200], Batch [500/938], Loss D: 0.6612, Loss G: 0.7008\n",
      "Epoch [369/200], Batch [600/938], Loss D: 0.6379, Loss G: 0.7706\n",
      "Epoch [369/200], Batch [700/938], Loss D: 0.7004, Loss G: 0.7106\n",
      "Epoch [369/200], Batch [800/938], Loss D: 0.7321, Loss G: 0.6605\n",
      "Epoch [369/200], Batch [900/938], Loss D: 0.7138, Loss G: 0.6872\n",
      "Epoch [370/200], Batch [0/938], Loss D: 0.6903, Loss G: 0.7534\n",
      "Epoch [370/200], Batch [100/938], Loss D: 0.7163, Loss G: 0.6717\n",
      "Epoch [370/200], Batch [200/938], Loss D: 0.7002, Loss G: 0.7021\n",
      "Epoch [370/200], Batch [300/938], Loss D: 0.6813, Loss G: 0.7556\n",
      "Epoch [370/200], Batch [400/938], Loss D: 0.6788, Loss G: 0.7676\n",
      "Epoch [370/200], Batch [500/938], Loss D: 0.6495, Loss G: 0.7797\n",
      "Epoch [370/200], Batch [600/938], Loss D: 0.6364, Loss G: 0.7354\n",
      "Epoch [370/200], Batch [700/938], Loss D: 0.6859, Loss G: 0.7108\n",
      "Epoch [370/200], Batch [800/938], Loss D: 0.7022, Loss G: 0.7199\n",
      "Epoch [370/200], Batch [900/938], Loss D: 0.6913, Loss G: 0.7316\n",
      "Epoch [371/200], Batch [0/938], Loss D: 0.6860, Loss G: 0.7442\n",
      "Epoch [371/200], Batch [100/938], Loss D: 0.6603, Loss G: 0.7407\n",
      "Epoch [371/200], Batch [200/938], Loss D: 0.6939, Loss G: 0.7346\n",
      "Epoch [371/200], Batch [300/938], Loss D: 0.6920, Loss G: 0.7311\n",
      "Epoch [371/200], Batch [400/938], Loss D: 0.6646, Loss G: 0.7507\n",
      "Epoch [371/200], Batch [500/938], Loss D: 0.6496, Loss G: 0.7753\n",
      "Epoch [371/200], Batch [600/938], Loss D: 0.6895, Loss G: 0.6744\n",
      "Epoch [371/200], Batch [700/938], Loss D: 0.7210, Loss G: 0.7012\n",
      "Epoch [371/200], Batch [800/938], Loss D: 0.6610, Loss G: 0.7570\n",
      "Epoch [371/200], Batch [900/938], Loss D: 0.6843, Loss G: 0.7787\n",
      "Epoch [372/200], Batch [0/938], Loss D: 0.6551, Loss G: 0.7515\n",
      "Epoch [372/200], Batch [100/938], Loss D: 0.6446, Loss G: 0.8152\n",
      "Epoch [372/200], Batch [200/938], Loss D: 0.6869, Loss G: 0.6933\n",
      "Epoch [372/200], Batch [300/938], Loss D: 0.6760, Loss G: 0.7572\n",
      "Epoch [372/200], Batch [400/938], Loss D: 0.6683, Loss G: 0.7187\n",
      "Epoch [372/200], Batch [500/938], Loss D: 0.7070, Loss G: 0.7147\n",
      "Epoch [372/200], Batch [600/938], Loss D: 0.6768, Loss G: 0.7424\n",
      "Epoch [372/200], Batch [700/938], Loss D: 0.6981, Loss G: 0.7208\n",
      "Epoch [372/200], Batch [800/938], Loss D: 0.7311, Loss G: 0.7249\n",
      "Epoch [372/200], Batch [900/938], Loss D: 0.6858, Loss G: 0.7522\n",
      "Epoch [373/200], Batch [0/938], Loss D: 0.6440, Loss G: 0.7456\n",
      "Epoch [373/200], Batch [100/938], Loss D: 0.6951, Loss G: 0.6818\n",
      "Epoch [373/200], Batch [200/938], Loss D: 0.6910, Loss G: 0.7186\n",
      "Epoch [373/200], Batch [300/938], Loss D: 0.6865, Loss G: 0.7311\n",
      "Epoch [373/200], Batch [400/938], Loss D: 0.6971, Loss G: 0.7168\n",
      "Epoch [373/200], Batch [500/938], Loss D: 0.6872, Loss G: 0.7209\n",
      "Epoch [373/200], Batch [600/938], Loss D: 0.6999, Loss G: 0.7261\n",
      "Epoch [373/200], Batch [700/938], Loss D: 0.7012, Loss G: 0.7244\n",
      "Epoch [373/200], Batch [800/938], Loss D: 0.6913, Loss G: 0.6825\n",
      "Epoch [373/200], Batch [900/938], Loss D: 0.6893, Loss G: 0.7131\n",
      "Epoch [374/200], Batch [0/938], Loss D: 0.6974, Loss G: 0.7208\n",
      "Epoch [374/200], Batch [100/938], Loss D: 0.6768, Loss G: 0.7443\n",
      "Epoch [374/200], Batch [200/938], Loss D: 0.6881, Loss G: 0.7487\n",
      "Epoch [374/200], Batch [300/938], Loss D: 0.6890, Loss G: 0.7499\n",
      "Epoch [374/200], Batch [400/938], Loss D: 0.6793, Loss G: 0.7474\n",
      "Epoch [374/200], Batch [500/938], Loss D: 0.6987, Loss G: 0.7217\n",
      "Epoch [374/200], Batch [600/938], Loss D: 0.6707, Loss G: 0.7582\n",
      "Epoch [374/200], Batch [700/938], Loss D: 0.6880, Loss G: 0.7323\n",
      "Epoch [374/200], Batch [800/938], Loss D: 0.6616, Loss G: 0.7561\n",
      "Epoch [374/200], Batch [900/938], Loss D: 0.6939, Loss G: 0.7391\n",
      "Epoch [375/200], Batch [0/938], Loss D: 0.7068, Loss G: 0.6719\n",
      "Epoch [375/200], Batch [100/938], Loss D: 0.7166, Loss G: 0.6958\n",
      "Epoch [375/200], Batch [200/938], Loss D: 0.6798, Loss G: 0.7244\n",
      "Epoch [375/200], Batch [300/938], Loss D: 0.6862, Loss G: 0.7976\n",
      "Epoch [375/200], Batch [400/938], Loss D: 0.6826, Loss G: 0.7109\n",
      "Epoch [375/200], Batch [500/938], Loss D: 0.6597, Loss G: 0.7476\n",
      "Epoch [375/200], Batch [600/938], Loss D: 0.6703, Loss G: 0.6991\n",
      "Epoch [375/200], Batch [700/938], Loss D: 0.6670, Loss G: 0.7615\n",
      "Epoch [375/200], Batch [800/938], Loss D: 0.6766, Loss G: 0.7448\n",
      "Epoch [375/200], Batch [900/938], Loss D: 0.7079, Loss G: 0.7007\n",
      "Epoch [376/200], Batch [0/938], Loss D: 0.6945, Loss G: 0.6790\n",
      "Epoch [376/200], Batch [100/938], Loss D: 0.6705, Loss G: 0.7559\n",
      "Epoch [376/200], Batch [200/938], Loss D: 0.6967, Loss G: 0.7579\n",
      "Epoch [376/200], Batch [300/938], Loss D: 0.6916, Loss G: 0.7186\n",
      "Epoch [376/200], Batch [400/938], Loss D: 0.7143, Loss G: 0.7320\n",
      "Epoch [376/200], Batch [500/938], Loss D: 0.6738, Loss G: 0.7299\n",
      "Epoch [376/200], Batch [600/938], Loss D: 0.6649, Loss G: 0.7548\n",
      "Epoch [376/200], Batch [700/938], Loss D: 0.7046, Loss G: 0.7025\n",
      "Epoch [376/200], Batch [800/938], Loss D: 0.6954, Loss G: 0.7404\n",
      "Epoch [376/200], Batch [900/938], Loss D: 0.7048, Loss G: 0.7044\n",
      "Epoch [377/200], Batch [0/938], Loss D: 0.6707, Loss G: 0.7579\n",
      "Epoch [377/200], Batch [100/938], Loss D: 0.7016, Loss G: 0.7243\n",
      "Epoch [377/200], Batch [200/938], Loss D: 0.6504, Loss G: 0.7634\n",
      "Epoch [377/200], Batch [300/938], Loss D: 0.7017, Loss G: 0.7056\n",
      "Epoch [377/200], Batch [400/938], Loss D: 0.6804, Loss G: 0.7148\n",
      "Epoch [377/200], Batch [500/938], Loss D: 0.6606, Loss G: 0.7455\n",
      "Epoch [377/200], Batch [600/938], Loss D: 0.6873, Loss G: 0.7306\n",
      "Epoch [377/200], Batch [700/938], Loss D: 0.6610, Loss G: 0.7370\n",
      "Epoch [377/200], Batch [800/938], Loss D: 0.6919, Loss G: 0.7180\n",
      "Epoch [377/200], Batch [900/938], Loss D: 0.6944, Loss G: 0.7136\n",
      "Epoch [378/200], Batch [0/938], Loss D: 0.7180, Loss G: 0.6702\n",
      "Epoch [378/200], Batch [100/938], Loss D: 0.6864, Loss G: 0.7201\n",
      "Epoch [378/200], Batch [200/938], Loss D: 0.6794, Loss G: 0.7377\n",
      "Epoch [378/200], Batch [300/938], Loss D: 0.6852, Loss G: 0.7774\n",
      "Epoch [378/200], Batch [400/938], Loss D: 0.6679, Loss G: 0.7715\n",
      "Epoch [378/200], Batch [500/938], Loss D: 0.6514, Loss G: 0.7966\n",
      "Epoch [378/200], Batch [600/938], Loss D: 0.6621, Loss G: 0.7816\n",
      "Epoch [378/200], Batch [700/938], Loss D: 0.6845, Loss G: 0.7236\n",
      "Epoch [378/200], Batch [800/938], Loss D: 0.6718, Loss G: 0.7249\n",
      "Epoch [378/200], Batch [900/938], Loss D: 0.6756, Loss G: 0.7336\n",
      "Epoch [379/200], Batch [0/938], Loss D: 0.6712, Loss G: 0.7270\n",
      "Epoch [379/200], Batch [100/938], Loss D: 0.6632, Loss G: 0.7494\n",
      "Epoch [379/200], Batch [200/938], Loss D: 0.7006, Loss G: 0.6970\n",
      "Epoch [379/200], Batch [300/938], Loss D: 0.6653, Loss G: 0.7257\n",
      "Epoch [379/200], Batch [400/938], Loss D: 0.6635, Loss G: 0.7603\n",
      "Epoch [379/200], Batch [500/938], Loss D: 0.6858, Loss G: 0.7267\n",
      "Epoch [379/200], Batch [600/938], Loss D: 0.6865, Loss G: 0.7123\n",
      "Epoch [379/200], Batch [700/938], Loss D: 0.6823, Loss G: 0.7245\n",
      "Epoch [379/200], Batch [800/938], Loss D: 0.6936, Loss G: 0.6838\n",
      "Epoch [379/200], Batch [900/938], Loss D: 0.6801, Loss G: 0.7370\n",
      "Epoch [380/200], Batch [0/938], Loss D: 0.6545, Loss G: 0.7722\n",
      "Epoch [380/200], Batch [100/938], Loss D: 0.7189, Loss G: 0.6729\n",
      "Epoch [380/200], Batch [200/938], Loss D: 0.7029, Loss G: 0.7203\n",
      "Epoch [380/200], Batch [300/938], Loss D: 0.6853, Loss G: 0.7255\n",
      "Epoch [380/200], Batch [400/938], Loss D: 0.6651, Loss G: 0.8062\n",
      "Epoch [380/200], Batch [500/938], Loss D: 0.6920, Loss G: 0.7634\n",
      "Epoch [380/200], Batch [600/938], Loss D: 0.6787, Loss G: 0.7507\n",
      "Epoch [380/200], Batch [700/938], Loss D: 0.7011, Loss G: 0.7314\n",
      "Epoch [380/200], Batch [800/938], Loss D: 0.6522, Loss G: 0.7703\n",
      "Epoch [380/200], Batch [900/938], Loss D: 0.6878, Loss G: 0.7185\n",
      "Epoch [381/200], Batch [0/938], Loss D: 0.6540, Loss G: 0.7374\n",
      "Epoch [381/200], Batch [100/938], Loss D: 0.6997, Loss G: 0.7575\n",
      "Epoch [381/200], Batch [200/938], Loss D: 0.6873, Loss G: 0.7232\n",
      "Epoch [381/200], Batch [300/938], Loss D: 0.6466, Loss G: 0.7377\n",
      "Epoch [381/200], Batch [400/938], Loss D: 0.6632, Loss G: 0.7326\n",
      "Epoch [381/200], Batch [500/938], Loss D: 0.6965, Loss G: 0.6986\n",
      "Epoch [381/200], Batch [600/938], Loss D: 0.7088, Loss G: 0.6933\n",
      "Epoch [381/200], Batch [700/938], Loss D: 0.6938, Loss G: 0.7069\n",
      "Epoch [381/200], Batch [800/938], Loss D: 0.6947, Loss G: 0.7148\n",
      "Epoch [381/200], Batch [900/938], Loss D: 0.7002, Loss G: 0.6907\n",
      "Epoch [382/200], Batch [0/938], Loss D: 0.6525, Loss G: 0.7399\n",
      "Epoch [382/200], Batch [100/938], Loss D: 0.6852, Loss G: 0.7192\n",
      "Epoch [382/200], Batch [200/938], Loss D: 0.6794, Loss G: 0.7427\n",
      "Epoch [382/200], Batch [300/938], Loss D: 0.6685, Loss G: 0.7021\n",
      "Epoch [382/200], Batch [400/938], Loss D: 0.6845, Loss G: 0.7185\n",
      "Epoch [382/200], Batch [500/938], Loss D: 0.6901, Loss G: 0.7096\n",
      "Epoch [382/200], Batch [600/938], Loss D: 0.6811, Loss G: 0.7523\n",
      "Epoch [382/200], Batch [700/938], Loss D: 0.6824, Loss G: 0.7331\n",
      "Epoch [382/200], Batch [800/938], Loss D: 0.6906, Loss G: 0.7225\n",
      "Epoch [382/200], Batch [900/938], Loss D: 0.6598, Loss G: 0.7699\n",
      "Epoch [383/200], Batch [0/938], Loss D: 0.6912, Loss G: 0.7095\n",
      "Epoch [383/200], Batch [100/938], Loss D: 0.7127, Loss G: 0.7048\n",
      "Epoch [383/200], Batch [200/938], Loss D: 0.6866, Loss G: 0.7298\n",
      "Epoch [383/200], Batch [300/938], Loss D: 0.6621, Loss G: 0.7763\n",
      "Epoch [383/200], Batch [400/938], Loss D: 0.6645, Loss G: 0.7600\n",
      "Epoch [383/200], Batch [500/938], Loss D: 0.7036, Loss G: 0.7157\n",
      "Epoch [383/200], Batch [600/938], Loss D: 0.6693, Loss G: 0.7684\n",
      "Epoch [383/200], Batch [700/938], Loss D: 0.6360, Loss G: 0.7866\n",
      "Epoch [383/200], Batch [800/938], Loss D: 0.6946, Loss G: 0.7047\n",
      "Epoch [383/200], Batch [900/938], Loss D: 0.6543, Loss G: 0.7353\n",
      "Epoch [384/200], Batch [0/938], Loss D: 0.6874, Loss G: 0.7093\n",
      "Epoch [384/200], Batch [100/938], Loss D: 0.6861, Loss G: 0.7622\n",
      "Epoch [384/200], Batch [200/938], Loss D: 0.6799, Loss G: 0.7648\n",
      "Epoch [384/200], Batch [300/938], Loss D: 0.6987, Loss G: 0.7035\n",
      "Epoch [384/200], Batch [400/938], Loss D: 0.6767, Loss G: 0.7351\n",
      "Epoch [384/200], Batch [500/938], Loss D: 0.7076, Loss G: 0.6806\n",
      "Epoch [384/200], Batch [600/938], Loss D: 0.6850, Loss G: 0.7230\n",
      "Epoch [384/200], Batch [700/938], Loss D: 0.6597, Loss G: 0.7554\n",
      "Epoch [384/200], Batch [800/938], Loss D: 0.6806, Loss G: 0.7550\n",
      "Epoch [384/200], Batch [900/938], Loss D: 0.6607, Loss G: 0.7401\n",
      "Epoch [385/200], Batch [0/938], Loss D: 0.7086, Loss G: 0.6773\n",
      "Epoch [385/200], Batch [100/938], Loss D: 0.6689, Loss G: 0.7301\n",
      "Epoch [385/200], Batch [200/938], Loss D: 0.6845, Loss G: 0.7418\n",
      "Epoch [385/200], Batch [300/938], Loss D: 0.6740, Loss G: 0.7150\n",
      "Epoch [385/200], Batch [400/938], Loss D: 0.6564, Loss G: 0.7817\n",
      "Epoch [385/200], Batch [500/938], Loss D: 0.6786, Loss G: 0.7090\n",
      "Epoch [385/200], Batch [600/938], Loss D: 0.6433, Loss G: 0.7886\n",
      "Epoch [385/200], Batch [700/938], Loss D: 0.6592, Loss G: 0.7509\n",
      "Epoch [385/200], Batch [800/938], Loss D: 0.6922, Loss G: 0.7438\n",
      "Epoch [385/200], Batch [900/938], Loss D: 0.7300, Loss G: 0.6848\n",
      "Epoch [386/200], Batch [0/938], Loss D: 0.6721, Loss G: 0.7854\n",
      "Epoch [386/200], Batch [100/938], Loss D: 0.6826, Loss G: 0.7360\n",
      "Epoch [386/200], Batch [200/938], Loss D: 0.6965, Loss G: 0.7115\n",
      "Epoch [386/200], Batch [300/938], Loss D: 0.7121, Loss G: 0.6986\n",
      "Epoch [386/200], Batch [400/938], Loss D: 0.6536, Loss G: 0.7837\n",
      "Epoch [386/200], Batch [500/938], Loss D: 0.7082, Loss G: 0.6674\n",
      "Epoch [386/200], Batch [600/938], Loss D: 0.6924, Loss G: 0.7244\n",
      "Epoch [386/200], Batch [700/938], Loss D: 0.6647, Loss G: 0.7212\n",
      "Epoch [386/200], Batch [800/938], Loss D: 0.6808, Loss G: 0.7444\n",
      "Epoch [386/200], Batch [900/938], Loss D: 0.6592, Loss G: 0.7741\n",
      "Epoch [387/200], Batch [0/938], Loss D: 0.6559, Loss G: 0.7590\n",
      "Epoch [387/200], Batch [100/938], Loss D: 0.6908, Loss G: 0.7119\n",
      "Epoch [387/200], Batch [200/938], Loss D: 0.6841, Loss G: 0.7360\n",
      "Epoch [387/200], Batch [300/938], Loss D: 0.7150, Loss G: 0.6982\n",
      "Epoch [387/200], Batch [400/938], Loss D: 0.7178, Loss G: 0.6863\n",
      "Epoch [387/200], Batch [500/938], Loss D: 0.7094, Loss G: 0.6560\n",
      "Epoch [387/200], Batch [600/938], Loss D: 0.6718, Loss G: 0.7254\n",
      "Epoch [387/200], Batch [700/938], Loss D: 0.6615, Loss G: 0.8052\n",
      "Epoch [387/200], Batch [800/938], Loss D: 0.7157, Loss G: 0.7086\n",
      "Epoch [387/200], Batch [900/938], Loss D: 0.6678, Loss G: 0.7725\n",
      "Epoch [388/200], Batch [0/938], Loss D: 0.6910, Loss G: 0.7569\n",
      "Epoch [388/200], Batch [100/938], Loss D: 0.6806, Loss G: 0.7672\n",
      "Epoch [388/200], Batch [200/938], Loss D: 0.6684, Loss G: 0.7887\n",
      "Epoch [388/200], Batch [300/938], Loss D: 0.6831, Loss G: 0.7246\n",
      "Epoch [388/200], Batch [400/938], Loss D: 0.7177, Loss G: 0.7020\n",
      "Epoch [388/200], Batch [500/938], Loss D: 0.7219, Loss G: 0.6906\n",
      "Epoch [388/200], Batch [600/938], Loss D: 0.6565, Loss G: 0.7813\n",
      "Epoch [388/200], Batch [700/938], Loss D: 0.7181, Loss G: 0.7603\n",
      "Epoch [388/200], Batch [800/938], Loss D: 0.6737, Loss G: 0.7455\n",
      "Epoch [388/200], Batch [900/938], Loss D: 0.6561, Loss G: 0.7928\n",
      "Epoch [389/200], Batch [0/938], Loss D: 0.7098, Loss G: 0.6698\n",
      "Epoch [389/200], Batch [100/938], Loss D: 0.6711, Loss G: 0.7260\n",
      "Epoch [389/200], Batch [200/938], Loss D: 0.6831, Loss G: 0.7628\n",
      "Epoch [389/200], Batch [300/938], Loss D: 0.6842, Loss G: 0.7430\n",
      "Epoch [389/200], Batch [400/938], Loss D: 0.6697, Loss G: 0.7250\n",
      "Epoch [389/200], Batch [500/938], Loss D: 0.6907, Loss G: 0.7407\n",
      "Epoch [389/200], Batch [600/938], Loss D: 0.6752, Loss G: 0.7322\n",
      "Epoch [389/200], Batch [700/938], Loss D: 0.6775, Loss G: 0.7212\n",
      "Epoch [389/200], Batch [800/938], Loss D: 0.6803, Loss G: 0.7329\n",
      "Epoch [389/200], Batch [900/938], Loss D: 0.6723, Loss G: 0.7452\n",
      "Epoch [390/200], Batch [0/938], Loss D: 0.6988, Loss G: 0.7201\n",
      "Epoch [390/200], Batch [100/938], Loss D: 0.7240, Loss G: 0.7399\n",
      "Epoch [390/200], Batch [200/938], Loss D: 0.6329, Loss G: 0.7605\n",
      "Epoch [390/200], Batch [300/938], Loss D: 0.6986, Loss G: 0.6840\n",
      "Epoch [390/200], Batch [400/938], Loss D: 0.7048, Loss G: 0.7502\n",
      "Epoch [390/200], Batch [500/938], Loss D: 0.6846, Loss G: 0.7012\n",
      "Epoch [390/200], Batch [600/938], Loss D: 0.6989, Loss G: 0.7133\n",
      "Epoch [390/200], Batch [700/938], Loss D: 0.6683, Loss G: 0.7861\n",
      "Epoch [390/200], Batch [800/938], Loss D: 0.6846, Loss G: 0.7254\n",
      "Epoch [390/200], Batch [900/938], Loss D: 0.7021, Loss G: 0.7535\n",
      "Epoch [391/200], Batch [0/938], Loss D: 0.7035, Loss G: 0.7642\n",
      "Epoch [391/200], Batch [100/938], Loss D: 0.6929, Loss G: 0.7771\n",
      "Epoch [391/200], Batch [200/938], Loss D: 0.6723, Loss G: 0.7835\n",
      "Epoch [391/200], Batch [300/938], Loss D: 0.6889, Loss G: 0.6738\n",
      "Epoch [391/200], Batch [400/938], Loss D: 0.7084, Loss G: 0.7139\n",
      "Epoch [391/200], Batch [500/938], Loss D: 0.6968, Loss G: 0.7098\n",
      "Epoch [391/200], Batch [600/938], Loss D: 0.7031, Loss G: 0.7170\n",
      "Epoch [391/200], Batch [700/938], Loss D: 0.6926, Loss G: 0.7573\n",
      "Epoch [391/200], Batch [800/938], Loss D: 0.6735, Loss G: 0.7442\n",
      "Epoch [391/200], Batch [900/938], Loss D: 0.7063, Loss G: 0.7288\n",
      "Epoch [392/200], Batch [0/938], Loss D: 0.6681, Loss G: 0.7797\n",
      "Epoch [392/200], Batch [100/938], Loss D: 0.6727, Loss G: 0.7329\n",
      "Epoch [392/200], Batch [200/938], Loss D: 0.6688, Loss G: 0.7267\n",
      "Epoch [392/200], Batch [300/938], Loss D: 0.6829, Loss G: 0.7445\n",
      "Epoch [392/200], Batch [400/938], Loss D: 0.6686, Loss G: 0.7704\n",
      "Epoch [392/200], Batch [500/938], Loss D: 0.6976, Loss G: 0.6957\n",
      "Epoch [392/200], Batch [600/938], Loss D: 0.6866, Loss G: 0.6909\n",
      "Epoch [392/200], Batch [700/938], Loss D: 0.6961, Loss G: 0.6881\n",
      "Epoch [392/200], Batch [800/938], Loss D: 0.7050, Loss G: 0.7244\n",
      "Epoch [392/200], Batch [900/938], Loss D: 0.6637, Loss G: 0.7522\n",
      "Epoch [393/200], Batch [0/938], Loss D: 0.6750, Loss G: 0.7288\n",
      "Epoch [393/200], Batch [100/938], Loss D: 0.6901, Loss G: 0.7282\n",
      "Epoch [393/200], Batch [200/938], Loss D: 0.6486, Loss G: 0.7727\n",
      "Epoch [393/200], Batch [300/938], Loss D: 0.6898, Loss G: 0.6959\n",
      "Epoch [393/200], Batch [400/938], Loss D: 0.6693, Loss G: 0.7246\n",
      "Epoch [393/200], Batch [500/938], Loss D: 0.6760, Loss G: 0.7401\n",
      "Epoch [393/200], Batch [600/938], Loss D: 0.6741, Loss G: 0.7423\n",
      "Epoch [393/200], Batch [700/938], Loss D: 0.6928, Loss G: 0.7067\n",
      "Epoch [393/200], Batch [800/938], Loss D: 0.6778, Loss G: 0.7206\n",
      "Epoch [393/200], Batch [900/938], Loss D: 0.7097, Loss G: 0.7106\n",
      "Epoch [394/200], Batch [0/938], Loss D: 0.6839, Loss G: 0.7420\n",
      "Epoch [394/200], Batch [100/938], Loss D: 0.6684, Loss G: 0.7444\n",
      "Epoch [394/200], Batch [200/938], Loss D: 0.6737, Loss G: 0.7682\n",
      "Epoch [394/200], Batch [300/938], Loss D: 0.6830, Loss G: 0.7479\n",
      "Epoch [394/200], Batch [400/938], Loss D: 0.6854, Loss G: 0.7159\n",
      "Epoch [394/200], Batch [500/938], Loss D: 0.6926, Loss G: 0.7060\n",
      "Epoch [394/200], Batch [600/938], Loss D: 0.6664, Loss G: 0.7653\n",
      "Epoch [394/200], Batch [700/938], Loss D: 0.6749, Loss G: 0.7357\n",
      "Epoch [394/200], Batch [800/938], Loss D: 0.6969, Loss G: 0.7272\n",
      "Epoch [394/200], Batch [900/938], Loss D: 0.6846, Loss G: 0.7287\n",
      "Epoch [395/200], Batch [0/938], Loss D: 0.6972, Loss G: 0.7416\n",
      "Epoch [395/200], Batch [100/938], Loss D: 0.6902, Loss G: 0.7089\n",
      "Epoch [395/200], Batch [200/938], Loss D: 0.6902, Loss G: 0.7355\n",
      "Epoch [395/200], Batch [300/938], Loss D: 0.6811, Loss G: 0.7651\n",
      "Epoch [395/200], Batch [400/938], Loss D: 0.6817, Loss G: 0.7532\n",
      "Epoch [395/200], Batch [500/938], Loss D: 0.6957, Loss G: 0.6901\n",
      "Epoch [395/200], Batch [600/938], Loss D: 0.6684, Loss G: 0.7737\n",
      "Epoch [395/200], Batch [700/938], Loss D: 0.6984, Loss G: 0.7116\n",
      "Epoch [395/200], Batch [800/938], Loss D: 0.6639, Loss G: 0.8025\n",
      "Epoch [395/200], Batch [900/938], Loss D: 0.6737, Loss G: 0.7146\n",
      "Epoch [396/200], Batch [0/938], Loss D: 0.6910, Loss G: 0.7259\n",
      "Epoch [396/200], Batch [100/938], Loss D: 0.7324, Loss G: 0.6949\n",
      "Epoch [396/200], Batch [200/938], Loss D: 0.6519, Loss G: 0.8026\n",
      "Epoch [396/200], Batch [300/938], Loss D: 0.6957, Loss G: 0.7130\n",
      "Epoch [396/200], Batch [400/938], Loss D: 0.6985, Loss G: 0.7225\n",
      "Epoch [396/200], Batch [500/938], Loss D: 0.6504, Loss G: 0.7928\n",
      "Epoch [396/200], Batch [600/938], Loss D: 0.6920, Loss G: 0.7222\n",
      "Epoch [396/200], Batch [700/938], Loss D: 0.6825, Loss G: 0.7563\n",
      "Epoch [396/200], Batch [800/938], Loss D: 0.6978, Loss G: 0.6878\n",
      "Epoch [396/200], Batch [900/938], Loss D: 0.6959, Loss G: 0.6757\n",
      "Epoch [397/200], Batch [0/938], Loss D: 0.6723, Loss G: 0.8103\n",
      "Epoch [397/200], Batch [100/938], Loss D: 0.6688, Loss G: 0.7513\n",
      "Epoch [397/200], Batch [200/938], Loss D: 0.6743, Loss G: 0.7978\n",
      "Epoch [397/200], Batch [300/938], Loss D: 0.6689, Loss G: 0.7173\n",
      "Epoch [397/200], Batch [400/938], Loss D: 0.6651, Loss G: 0.7599\n",
      "Epoch [397/200], Batch [500/938], Loss D: 0.6882, Loss G: 0.6996\n",
      "Epoch [397/200], Batch [600/938], Loss D: 0.6899, Loss G: 0.7395\n",
      "Epoch [397/200], Batch [700/938], Loss D: 0.6907, Loss G: 0.7170\n",
      "Epoch [397/200], Batch [800/938], Loss D: 0.6754, Loss G: 0.7218\n",
      "Epoch [397/200], Batch [900/938], Loss D: 0.6741, Loss G: 0.7387\n",
      "Epoch [398/200], Batch [0/938], Loss D: 0.6727, Loss G: 0.7103\n",
      "Epoch [398/200], Batch [100/938], Loss D: 0.6898, Loss G: 0.7304\n",
      "Epoch [398/200], Batch [200/938], Loss D: 0.6956, Loss G: 0.7061\n",
      "Epoch [398/200], Batch [300/938], Loss D: 0.6965, Loss G: 0.7160\n",
      "Epoch [398/200], Batch [400/938], Loss D: 0.6806, Loss G: 0.7607\n",
      "Epoch [398/200], Batch [500/938], Loss D: 0.6854, Loss G: 0.7481\n",
      "Epoch [398/200], Batch [600/938], Loss D: 0.6300, Loss G: 0.7497\n",
      "Epoch [398/200], Batch [700/938], Loss D: 0.6772, Loss G: 0.7111\n",
      "Epoch [398/200], Batch [800/938], Loss D: 0.6608, Loss G: 0.7714\n",
      "Epoch [398/200], Batch [900/938], Loss D: 0.6793, Loss G: 0.7003\n",
      "Epoch [399/200], Batch [0/938], Loss D: 0.6581, Loss G: 0.7143\n",
      "Epoch [399/200], Batch [100/938], Loss D: 0.7053, Loss G: 0.7161\n",
      "Epoch [399/200], Batch [200/938], Loss D: 0.6857, Loss G: 0.7158\n",
      "Epoch [399/200], Batch [300/938], Loss D: 0.6954, Loss G: 0.7281\n",
      "Epoch [399/200], Batch [400/938], Loss D: 0.6819, Loss G: 0.7356\n",
      "Epoch [399/200], Batch [500/938], Loss D: 0.7165, Loss G: 0.6905\n",
      "Epoch [399/200], Batch [600/938], Loss D: 0.6963, Loss G: 0.7261\n",
      "Epoch [399/200], Batch [700/938], Loss D: 0.6846, Loss G: 0.7268\n",
      "Epoch [399/200], Batch [800/938], Loss D: 0.6867, Loss G: 0.7513\n",
      "Epoch [399/200], Batch [900/938], Loss D: 0.6671, Loss G: 0.7822\n"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "for epoch in range(num_epochs, 2*num_epochs):\n",
    "    for batch_idx, (real_images, labels) in enumerate(train_loader):\n",
    "        # 生成随机噪声和条件信息\n",
    "        z = torch.randn(real_images.size(0), input_size)\n",
    "        y = torch.eye(num_classes)[labels]  # 将类别转换为one-hot编码\n",
    "        \n",
    "        # 训练判别器\n",
    "        optimizer_d.zero_grad()\n",
    "        fake_images, fake_outputs = cgan(z, y)\n",
    "        real_outputs = discriminator(real_images.view(-1, output_size), y)\n",
    "        loss_d_real = criterion(real_outputs, torch.ones_like(real_outputs))  # 真实图像的判别器损失\n",
    "        loss_d_fake = criterion(fake_outputs, torch.zeros_like(fake_outputs))  # 生成图像的判别器损失\n",
    "        loss_d = (loss_d_real + loss_d_fake) / 2\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # 训练生成器\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_images, fake_outputs = cgan(z, y)\n",
    "        loss_g = criterion(fake_outputs, torch.ones_like(fake_outputs))  # 生成器损失\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # 打印训练信息\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{2*num_epochs}], Batch [{batch_idx}/{len(train_loader)}], \"\n",
    "                  f\"Loss D: {loss_d:.4f}, Loss G: {loss_g:.4f}\")\n",
    "        if epoch == 0 and batch_idx==len(train_loader)-1:\n",
    "            real_images = to_img(real_images.cuda().data)\n",
    "            save_image(real_images, str(epoch)+'_real_images.png')\n",
    "        if batch_idx==len(train_loader)-1:\n",
    "            fake_images = to_img(fake_images.cuda().data)\n",
    "            save_image(fake_images, str(epoch)+'_fake_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def images_to_video(image_folder, video_name, fps):\n",
    "    images = [img for img in sorted(os.listdir(image_folder)) if img.endswith(\".png\") and \"fake_images\" in img]\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, fps, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "# 调用函数\n",
    "images_to_video('.', 'output_video.mp4', 30)  # 假设每秒30帧\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
