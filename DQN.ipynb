{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何使用pytorch框架实现一个mnist手写数字识别的简单的强化学习demo？\n",
    "\n",
    "实现一个简单的基于PyTorch的强化学习（RL）Demo来解决MNIST手写数字识别问题是不太常见的，因为通常RL更适用于连续动作和状态空间的问题，而不是像MNIST这样的离散分类问题。然而，你可以将这个问题转换成一个RL问题，例如，你可以尝试使用深度强化学习算法，如DQN，来训练一个代理（agent）来处理MNIST数据。\n",
    "\n",
    "下面是一个简单的示例代码，演示如何使用PyTorch和DQN来解决MNIST手写数字识别问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.012562339194118977\n",
      "loss: 0.008099504746496677\n",
      "loss: 0.005411564838141203\n",
      "loss: 0.004174706991761923\n",
      "loss: 0.0034522463101893663\n",
      "loss: 0.002754356013610959\n",
      "loss: 0.002635128563269973\n",
      "loss: 0.0029291007667779922\n",
      "loss: 0.005342904012650251\n",
      "loss: 0.005789635237306356\n",
      "loss: 0.004471152555197477\n",
      "loss: 0.006033007055521011\n",
      "loss: 0.004363666754215956\n",
      "loss: 0.004042037297040224\n",
      "loss: 0.006107087712734938\n",
      "loss: 0.005817280150949955\n",
      "loss: 0.004700637888163328\n",
      "loss: 0.0019112963927909732\n",
      "loss: 0.0033415004145354033\n",
      "loss: 0.002527732402086258\n",
      "loss: 0.0015616053715348244\n",
      "loss: 0.004957437515258789\n",
      "loss: 0.0041802204214036465\n",
      "loss: 0.003533989191055298\n",
      "loss: 0.00202165893279016\n",
      "loss: 0.0035462421365082264\n",
      "loss: 0.0010870522819459438\n",
      "loss: 0.001648181350901723\n",
      "loss: 0.002967839129269123\n",
      "loss: 0.0006663869135081768\n",
      "loss: 0.0022631410975009203\n",
      "loss: 0.00031358475098386407\n",
      "loss: 0.0013479801127687097\n",
      "loss: 0.0032708204817026854\n",
      "loss: 0.0029314581770449877\n",
      "loss: 0.0011303186183795333\n",
      "loss: 0.0025356963742524385\n",
      "loss: 0.003118448192253709\n",
      "loss: 0.0017165429890155792\n",
      "loss: 0.00375930592417717\n",
      "loss: 0.002820338122546673\n",
      "loss: 0.0008604589966125786\n",
      "loss: 0.0033885010052472353\n",
      "loss: 0.0014107618480920792\n",
      "loss: 0.0025661156978458166\n",
      "loss: 0.0016799032455310225\n",
      "loss: 0.0014528532046824694\n",
      "loss: 0.0023916473146528006\n",
      "loss: 0.0005401356029324234\n",
      "loss: 0.0011763358488678932\n",
      "loss: 0.0022090994752943516\n",
      "loss: 0.0011416553752496839\n",
      "loss: 0.0008286235970444977\n",
      "loss: 0.0010184819111600518\n",
      "loss: 0.0009480615262873471\n",
      "loss: 0.0034031409304589033\n",
      "loss: 0.0009069369989447296\n",
      "loss: 0.00030490368953906\n",
      "loss: 0.0024134849663823843\n",
      "loss: 0.0018385384464636445\n",
      "loss: 0.0003115593863185495\n",
      "loss: 0.0011043440317735076\n",
      "loss: 0.0015052742091938853\n",
      "loss: 0.0007222624844871461\n",
      "loss: 0.0003296022186987102\n",
      "loss: 0.001648670993745327\n",
      "loss: 0.004711949732154608\n",
      "loss: 0.0015770121244713664\n",
      "loss: 0.0015117990551516414\n",
      "loss: 0.00033856616937555373\n",
      "loss: 0.0016432658303529024\n",
      "loss: 0.0038269024807959795\n",
      "loss: 0.00043430892401374876\n",
      "loss: 0.0037302381824702024\n",
      "loss: 0.003528932575136423\n",
      "loss: 0.0014599153073504567\n",
      "loss: 0.00048162959865294397\n",
      "loss: 0.0005929889157414436\n",
      "loss: 0.0006785590085200965\n",
      "loss: 0.0008468645974062383\n",
      "loss: 0.0029912181198596954\n",
      "loss: 0.00031691850745119154\n",
      "loss: 0.0007024265360087156\n",
      "loss: 0.00047875032760202885\n",
      "loss: 0.0015358240343630314\n",
      "loss: 0.007573753595352173\n",
      "loss: 0.0011377999326214194\n",
      "loss: 0.0062746829353272915\n",
      "loss: 0.003887797240167856\n",
      "loss: 0.0037376489490270615\n",
      "loss: 0.0018494073301553726\n",
      "loss: 0.003238316625356674\n",
      "loss: 0.007461113389581442\n",
      "loss: 0.0032358875032514334\n",
      "loss: 0.0024708067066967487\n",
      "loss: 0.0012647219700738788\n",
      "loss: 0.004252330400049686\n",
      "loss: 0.0006084296619519591\n",
      "loss: 0.0014152040239423513\n",
      "loss: 0.0026206281036138535\n",
      "loss: 0.0004956735647283494\n",
      "loss: 0.0016671185148879886\n",
      "loss: 0.0012141868937760592\n",
      "loss: 0.0004983441322110593\n",
      "loss: 0.0011120367562398314\n",
      "loss: 0.0005162072484381497\n",
      "loss: 0.0019335513934493065\n",
      "loss: 0.0010757171548902988\n",
      "loss: 0.0022000877652317286\n",
      "loss: 0.001042816205881536\n",
      "loss: 0.001163059496320784\n",
      "loss: 0.0009876894764602184\n",
      "loss: 0.0008236758294515312\n",
      "loss: 0.0009309444576501846\n",
      "loss: 0.00043998361798003316\n",
      "loss: 0.0001960261579370126\n",
      "loss: 0.0011186515912413597\n",
      "loss: 0.001291866647079587\n",
      "loss: 0.0006152267451398075\n",
      "loss: 0.0011074269423261285\n",
      "loss: 0.00021048453345429152\n",
      "loss: 0.0003630340506788343\n",
      "loss: 0.0007588276639580727\n",
      "loss: 0.00034804342431016266\n",
      "loss: 0.0004423523787409067\n",
      "loss: 0.0004705965693574399\n",
      "loss: 0.0006355368532240391\n",
      "loss: 0.0007640778785571456\n",
      "loss: 0.0003190301649738103\n",
      "loss: 0.00014439875667449087\n",
      "loss: 0.003461219137534499\n",
      "loss: 0.004326823633164167\n",
      "loss: 0.0004414177674334496\n",
      "loss: 0.00018335827917326242\n",
      "loss: 0.00032542593544349074\n",
      "loss: 0.00015922868624329567\n",
      "loss: 0.0024941812735050917\n",
      "loss: 0.0005553046357817948\n",
      "loss: 0.00048352297744713724\n",
      "loss: 0.0003006925981026143\n",
      "loss: 0.003382966620847583\n",
      "loss: 0.005092869978398085\n",
      "loss: 0.00019527167023625225\n",
      "loss: 0.004478229209780693\n",
      "loss: 0.00045273135765455663\n",
      "loss: 0.00027998595032840967\n",
      "loss: 0.0003114350838586688\n",
      "loss: 0.0013003175845369697\n",
      "loss: 0.0003458700084593147\n",
      "loss: 0.00032091609318740666\n",
      "loss: 0.00042275787563994527\n",
      "loss: 0.00017450680024921894\n",
      "loss: 0.002485282253473997\n",
      "loss: 0.0005813510506413877\n",
      "loss: 0.0006153883296065032\n",
      "loss: 0.0004297216946724802\n",
      "loss: 0.0021061219740659\n",
      "loss: 0.003193038748577237\n",
      "loss: 0.0004977117641828954\n",
      "loss: 0.0005606903578154743\n",
      "loss: 0.0015018987469375134\n",
      "loss: 0.0009417342371307313\n",
      "loss: 0.0020009272266179323\n",
      "loss: 0.0011827613925561309\n",
      "loss: 0.0005494991200976074\n",
      "loss: 0.0004041909705847502\n",
      "loss: 0.002263021655380726\n",
      "loss: 0.00203399034217\n",
      "loss: 0.0005902012344449759\n",
      "loss: 0.005584431812167168\n",
      "loss: 0.0014615602558478713\n",
      "loss: 0.0009174475562758744\n",
      "loss: 0.003165480447933078\n",
      "loss: 0.000921299506444484\n",
      "loss: 0.00046813508379273117\n",
      "loss: 0.0027387612499296665\n",
      "loss: 0.0004032870347145945\n",
      "loss: 0.0016273176297545433\n",
      "loss: 0.0019027431262657046\n",
      "loss: 0.0019988378044217825\n",
      "loss: 0.0003144699730910361\n",
      "loss: 0.0019469511462375522\n",
      "loss: 0.0016946196556091309\n",
      "loss: 0.0010895113227888942\n",
      "loss: 0.0015764603158459067\n",
      "loss: 0.0012453723466023803\n",
      "loss: 0.0014216355048120022\n",
      "loss: 0.0008625093032605946\n",
      "loss: 0.0007342112949118018\n",
      "loss: 0.0012164259096607566\n",
      "loss: 0.0005252563860267401\n",
      "loss: 0.0018415696686133742\n",
      "loss: 0.00046578794717788696\n",
      "loss: 0.0003654354950413108\n",
      "loss: 0.0008912553312256932\n",
      "loss: 0.0011723487405106425\n",
      "loss: 0.0011094581568613648\n",
      "loss: 0.00026708602672442794\n",
      "loss: 0.0009937185095623136\n",
      "loss: 0.0008075462537817657\n",
      "loss: 0.0024328806903213263\n",
      "loss: 0.0006261375383473933\n",
      "loss: 0.0023651360534131527\n",
      "loss: 0.0007745250477455556\n",
      "loss: 0.0005769384442828596\n",
      "loss: 0.00039832937181927264\n",
      "loss: 0.0008495214278809726\n",
      "loss: 0.0023874423932284117\n",
      "loss: 0.000663018727209419\n",
      "loss: 0.001324775512330234\n",
      "loss: 0.0006758141680620611\n",
      "loss: 0.0009496938437223434\n",
      "loss: 0.0031366252806037664\n",
      "loss: 0.0008688644738867879\n",
      "loss: 0.0008659933810122311\n",
      "loss: 0.0015341469552367926\n",
      "loss: 0.000980678480118513\n",
      "loss: 0.0013775049010291696\n",
      "loss: 0.000991986715234816\n",
      "loss: 0.0006646838155575097\n",
      "loss: 0.00020173890516161919\n",
      "loss: 0.001602310105226934\n",
      "loss: 0.00045850203605368733\n",
      "loss: 0.00028137105982750654\n",
      "loss: 0.0005875431816093624\n",
      "loss: 0.0010315763065591455\n",
      "loss: 0.0027261932846158743\n",
      "loss: 0.0002998019626829773\n",
      "loss: 0.0006859550485387444\n",
      "loss: 0.002015970181673765\n",
      "loss: 0.004102669190615416\n",
      "loss: 0.0026141605339944363\n",
      "loss: 0.0026138133835047483\n",
      "loss: 0.0007177951629273593\n",
      "loss: 0.0006706471904180944\n",
      "loss: 0.0023768669925630093\n",
      "loss: 0.0008517528185620904\n",
      "loss: 0.00326210493221879\n",
      "loss: 0.001226540538482368\n",
      "loss: 0.00045201738248579204\n",
      "loss: 0.0005700200563296676\n",
      "loss: 0.003173458157107234\n",
      "loss: 0.0007825010688975453\n",
      "loss: 0.001919377245940268\n",
      "loss: 0.0009916573762893677\n",
      "loss: 0.0031563632655888796\n",
      "loss: 0.00045001861872151494\n",
      "loss: 0.001569657470099628\n",
      "loss: 0.0004130510787945241\n",
      "loss: 0.0026236854027956724\n",
      "loss: 0.00030758464708924294\n",
      "loss: 0.0016028694808483124\n",
      "loss: 0.0005837101489305496\n",
      "loss: 0.0009654233581386507\n",
      "loss: 0.0006630552234128118\n",
      "loss: 0.0004356824792921543\n",
      "loss: 0.0007867802050895989\n",
      "loss: 0.0029604327864944935\n",
      "loss: 0.0007935565081425011\n",
      "loss: 0.0048145391047000885\n",
      "loss: 0.0015835625817999244\n",
      "loss: 0.006146397441625595\n",
      "loss: 0.0010296193649992347\n",
      "loss: 0.0020527211017906666\n",
      "loss: 0.0012198422336950898\n",
      "loss: 0.0007966976845636964\n",
      "loss: 0.002138335956260562\n",
      "loss: 0.00042153598042204976\n",
      "loss: 0.0017919879173859954\n",
      "loss: 0.003120396053418517\n",
      "loss: 0.003132896963506937\n",
      "loss: 0.0012217145413160324\n",
      "loss: 0.0023254358675330877\n",
      "loss: 0.0028489225078374147\n",
      "loss: 0.0003502779291011393\n",
      "loss: 0.0017491815378889441\n",
      "loss: 0.0021444361191242933\n",
      "loss: 0.0006569179240614176\n",
      "loss: 0.0008221642347052693\n",
      "loss: 0.0018941097659990191\n",
      "loss: 0.0008316081366501749\n",
      "loss: 0.0005228963564150035\n",
      "loss: 0.0005456435028463602\n",
      "loss: 0.0023232540115714073\n",
      "loss: 0.0009216161561198533\n",
      "loss: 0.00039480935083702207\n",
      "loss: 0.0011444835690781474\n",
      "loss: 0.0017570689087733626\n",
      "loss: 0.0007998846704140306\n",
      "loss: 0.0016700158594176173\n",
      "loss: 0.0011302169878035784\n",
      "loss: 0.0018226243555545807\n",
      "loss: 0.002133038593456149\n",
      "loss: 0.0015193712897598743\n",
      "loss: 0.0005368593265302479\n",
      "loss: 0.001715100253932178\n",
      "loss: 0.0005943160504102707\n",
      "loss: 0.0016339157009497285\n",
      "loss: 0.0007196599617600441\n",
      "loss: 0.0008474077912978828\n",
      "loss: 0.002828640164807439\n",
      "loss: 0.005367595236748457\n",
      "loss: 0.0007806185749359429\n",
      "loss: 0.0008929599425755441\n",
      "loss: 0.0013208402087911963\n",
      "loss: 0.0011546139139682055\n",
      "loss: 0.0057162633165717125\n",
      "loss: 0.0009080246090888977\n",
      "loss: 0.0008172689122147858\n",
      "loss: 0.0008325762464664876\n",
      "loss: 0.0014267958467826247\n",
      "loss: 0.001708664232864976\n",
      "loss: 0.0012073060497641563\n",
      "loss: 0.0009080478921532631\n",
      "loss: 0.0008280322072096169\n",
      "loss: 0.0011223836336284876\n",
      "loss: 0.0009002626757137477\n",
      "loss: 0.0018717985367402434\n",
      "loss: 0.0009629908017814159\n",
      "loss: 0.0032976919319480658\n",
      "loss: 0.0030571322422474623\n",
      "loss: 0.000888296402990818\n",
      "loss: 0.001110527548007667\n",
      "loss: 0.00045955527457408607\n",
      "loss: 0.0011214505648240447\n",
      "loss: 0.000730892876163125\n",
      "loss: 0.0025218434166163206\n",
      "loss: 0.00030425103614106774\n",
      "loss: 0.0015439342241734266\n",
      "loss: 0.0022421248722821474\n",
      "loss: 0.002591171767562628\n",
      "loss: 0.0011054622009396553\n",
      "loss: 0.002125559141859412\n",
      "loss: 0.00065346818882972\n",
      "loss: 0.0009950341191142797\n",
      "loss: 0.0031539485789835453\n",
      "loss: 0.0004457230679690838\n",
      "loss: 0.0006877534324303269\n",
      "loss: 0.0012099905870854855\n",
      "loss: 0.0020899698138237\n",
      "loss: 0.0025083504151552916\n",
      "loss: 0.0016827507643029094\n",
      "loss: 0.0003400423738639802\n",
      "loss: 0.0014966236194595695\n",
      "loss: 0.0015588366659358144\n",
      "loss: 0.0005051837651990354\n",
      "loss: 0.003542958525940776\n",
      "loss: 0.00030284229433164\n",
      "loss: 0.0008464821730740368\n",
      "loss: 0.003045295365154743\n",
      "loss: 0.0003440387372393161\n",
      "loss: 0.0005737234023399651\n",
      "loss: 0.0007551279268227518\n",
      "loss: 0.0013965100515633821\n",
      "loss: 0.0010326671181246638\n",
      "loss: 0.0015116446884348989\n",
      "loss: 0.0017690298845991492\n",
      "loss: 0.003524204483255744\n",
      "loss: 0.0024764433037489653\n",
      "loss: 0.0005315520684234798\n",
      "loss: 0.0015930816298350692\n",
      "loss: 0.0018504838226363063\n",
      "loss: 0.0020893968176096678\n",
      "loss: 0.000942194543313235\n",
      "loss: 0.0010743197053670883\n",
      "loss: 0.005172437056899071\n",
      "loss: 0.0011947095626965165\n",
      "loss: 0.0008380789658986032\n",
      "loss: 0.0024234310258179903\n",
      "loss: 0.0021688833367079496\n",
      "loss: 0.0023025318514555693\n",
      "loss: 0.001523554907180369\n",
      "loss: 0.0015182208735495806\n",
      "loss: 0.0009509906521998346\n",
      "loss: 0.001250385190360248\n",
      "loss: 0.000980685348622501\n",
      "loss: 0.0016504405066370964\n",
      "loss: 0.0009461860172450542\n",
      "loss: 0.00406259298324585\n",
      "loss: 0.0009351694025099277\n",
      "loss: 0.0013897748431190848\n",
      "loss: 0.0006196931353770196\n",
      "loss: 0.0005079383845441043\n",
      "loss: 0.0003819089033640921\n",
      "loss: 0.0005896735819987953\n",
      "loss: 0.0008909126627258956\n",
      "loss: 0.0007516597397625446\n",
      "loss: 0.0005250764661468565\n",
      "loss: 0.002901286818087101\n",
      "loss: 0.0005564968450926244\n",
      "loss: 0.001396271400153637\n",
      "loss: 0.0011180862784385681\n",
      "loss: 0.0013293892843648791\n",
      "loss: 0.0004576187639031559\n",
      "loss: 0.0012503809994086623\n",
      "loss: 0.0003660671063698828\n",
      "loss: 0.0034669784363359213\n",
      "loss: 0.0015103182522580028\n",
      "loss: 0.001103667775169015\n",
      "loss: 0.0009215534664690495\n",
      "loss: 0.001033884473145008\n",
      "loss: 0.0010799525771290064\n",
      "loss: 0.0011892573675140738\n",
      "loss: 0.0006849858909845352\n",
      "loss: 0.0010078303748741746\n",
      "loss: 0.0010974728502333164\n",
      "loss: 0.0004503785166889429\n",
      "loss: 0.0008523695287294686\n",
      "loss: 0.0006015896797180176\n",
      "loss: 0.003178147366270423\n",
      "loss: 0.0009076492860913277\n",
      "loss: 0.0006984215578995645\n",
      "loss: 0.0012073175748810172\n",
      "loss: 0.001048984588123858\n",
      "loss: 0.0005273191491141915\n",
      "loss: 0.004187862854450941\n",
      "loss: 0.0024259397760033607\n",
      "loss: 0.0006183207733556628\n",
      "loss: 0.00027239538030698895\n",
      "loss: 0.0024149983655661345\n",
      "loss: 0.0005724295042455196\n",
      "loss: 0.004649966489523649\n",
      "loss: 0.0019655160140246153\n",
      "loss: 0.0003072167746722698\n",
      "loss: 0.002246298361569643\n",
      "loss: 0.0007056845352053642\n",
      "loss: 0.0006844747113063931\n",
      "loss: 0.00622622761875391\n",
      "loss: 0.00034205717383883893\n",
      "loss: 0.002961531048640609\n",
      "loss: 0.002482806099578738\n",
      "loss: 0.0003820566926151514\n",
      "loss: 0.0003789593174587935\n",
      "loss: 0.000999856973066926\n",
      "loss: 0.0007958635105751455\n",
      "loss: 0.0021952209062874317\n",
      "loss: 0.000716189038939774\n",
      "loss: 0.0022320423740893602\n",
      "loss: 0.003579448675736785\n",
      "loss: 0.0006676562479697168\n",
      "loss: 0.0014926219591870904\n",
      "loss: 0.0016422637272626162\n",
      "loss: 0.0013596458593383431\n",
      "loss: 0.000582390814088285\n",
      "loss: 0.0005918056122027338\n",
      "loss: 0.0023660436272621155\n",
      "loss: 0.001507140463218093\n",
      "loss: 0.0022372640669345856\n",
      "loss: 0.002010492142289877\n",
      "loss: 0.0004763845936395228\n",
      "loss: 0.0005632531829178333\n",
      "loss: 0.0007453752914443612\n",
      "loss: 0.00027846379089169204\n",
      "loss: 0.0010397230507805943\n",
      "loss: 0.0016949698328971863\n",
      "loss: 0.0012885787291452289\n",
      "loss: 0.0006885679322294891\n",
      "loss: 0.0008110484923236072\n",
      "loss: 0.0018426489550620317\n",
      "loss: 0.0007008115062490106\n",
      "loss: 0.0007198050734587014\n",
      "loss: 0.0007993541657924652\n",
      "loss: 0.00042628170922398567\n",
      "loss: 0.0034150925930589437\n",
      "loss: 0.0006118844612501562\n",
      "loss: 0.0013685791054740548\n",
      "loss: 0.0011069182073697448\n",
      "loss: 0.001652655191719532\n",
      "loss: 0.0024128819350153208\n",
      "loss: 0.00028165275580249727\n",
      "loss: 0.002012164331972599\n",
      "loss: 0.006991947535425425\n",
      "loss: 0.002935988362878561\n",
      "loss: 0.0023108371533453465\n",
      "loss: 0.0011419240618124604\n",
      "loss: 0.0013889274559915066\n",
      "loss: 0.0006334861973300576\n",
      "loss: 0.0008896882063709199\n",
      "loss: 0.0008236334542743862\n",
      "loss: 0.0022863061167299747\n",
      "loss: 0.0002751785214059055\n",
      "loss: 0.0007486493559554219\n",
      "loss: 0.0013988447608426213\n",
      "loss: 0.0016014123102650046\n",
      "loss: 0.0009733724291436374\n",
      "loss: 0.00251327664591372\n",
      "loss: 0.0010308721102774143\n",
      "loss: 0.004977624863386154\n",
      "loss: 0.0005503254942595959\n",
      "loss: 0.0011503066634759307\n",
      "loss: 0.0007479485357180238\n",
      "loss: 0.003791517810896039\n",
      "loss: 0.00024183967616409063\n",
      "loss: 0.0021257493644952774\n",
      "loss: 0.002427157247439027\n",
      "loss: 0.00028448764351196587\n",
      "loss: 0.0008869130979292095\n",
      "loss: 0.0026594705414026976\n",
      "loss: 0.0017507171723991632\n",
      "loss: 0.0005100988200865686\n",
      "loss: 0.00047780759632587433\n",
      "loss: 0.0012828608741983771\n",
      "loss: 0.0019907415844500065\n",
      "loss: 0.0003685019910335541\n",
      "loss: 0.0005886149010621011\n",
      "loss: 0.0006538489833474159\n",
      "loss: 0.002286217175424099\n",
      "loss: 0.0005709572578780353\n",
      "loss: 0.0006777635426260531\n",
      "loss: 0.0006199064082466066\n",
      "loss: 0.0013664817670360208\n",
      "loss: 0.0032673224341124296\n",
      "loss: 0.0012981684412807226\n",
      "loss: 0.001955952262505889\n",
      "loss: 0.003083998803049326\n",
      "loss: 0.005786614492535591\n",
      "loss: 0.0024284450337290764\n",
      "loss: 0.0012986299116164446\n",
      "loss: 0.0012958705192431808\n",
      "loss: 0.002175785368308425\n",
      "loss: 0.002111456822603941\n",
      "loss: 0.0024152633268386126\n",
      "loss: 0.0009519489831291139\n",
      "loss: 0.0022775502875447273\n",
      "loss: 0.002278375206515193\n",
      "loss: 0.0007495162426494062\n",
      "loss: 0.00059352075913921\n",
      "loss: 0.0014520275872200727\n",
      "loss: 0.001057482440955937\n",
      "loss: 0.0006240512593649328\n",
      "loss: 0.001793845440261066\n",
      "loss: 0.002645249245688319\n",
      "loss: 0.0014920418616384268\n",
      "loss: 0.0005891616456210613\n",
      "loss: 0.0021973003167659044\n",
      "loss: 0.004302304703742266\n",
      "loss: 0.0033579454757273197\n",
      "loss: 0.0007377119618467987\n",
      "loss: 0.002043060725554824\n",
      "loss: 0.0016349161742255092\n",
      "loss: 0.0011172350496053696\n",
      "loss: 0.0012247232953086495\n",
      "loss: 0.0010119060752913356\n",
      "loss: 0.0018470622599124908\n",
      "loss: 0.001854797126725316\n",
      "loss: 0.00155580451246351\n",
      "loss: 0.0005831901216879487\n",
      "loss: 0.0008923152345232666\n",
      "loss: 0.0003912185784429312\n",
      "loss: 0.0016991982702165842\n",
      "loss: 0.0013770671794191003\n",
      "loss: 0.001185440574772656\n",
      "loss: 0.0010494323214516044\n",
      "loss: 0.0009311007452197373\n",
      "loss: 0.0007254122756421566\n",
      "loss: 0.0013433044077828526\n",
      "loss: 0.000820406072307378\n",
      "loss: 0.004552842117846012\n",
      "loss: 0.0008089912007562816\n",
      "loss: 0.0005200858577154577\n",
      "loss: 0.001697084284387529\n",
      "loss: 0.000620783946942538\n",
      "loss: 0.0012664834503084421\n",
      "loss: 0.0005287355161271989\n",
      "loss: 0.001637703157030046\n",
      "loss: 0.0006323577254079282\n",
      "loss: 0.0006370978662744164\n",
      "loss: 0.0006278689834289253\n",
      "loss: 0.001620684051886201\n",
      "loss: 0.0010236829984933138\n",
      "loss: 0.001055693137459457\n",
      "loss: 0.0044519174844026566\n",
      "loss: 0.0007576625212095678\n",
      "loss: 0.0006057044374756515\n",
      "loss: 0.0030420266557484865\n",
      "loss: 0.0007141403038986027\n",
      "loss: 0.0011360502103343606\n",
      "loss: 0.0007498206687159836\n",
      "loss: 0.0006685456610284746\n",
      "loss: 0.0005396076594479382\n",
      "loss: 0.0007032303255982697\n",
      "loss: 0.0005133456434123218\n",
      "loss: 0.0004482436052057892\n",
      "loss: 0.001371891819871962\n",
      "loss: 0.0005472408956848085\n",
      "loss: 0.0015128458617255092\n",
      "loss: 0.0005610913503915071\n",
      "loss: 0.0012207812396809459\n",
      "loss: 0.000651263166218996\n",
      "loss: 0.002901561325415969\n",
      "loss: 0.0010921868961304426\n",
      "loss: 0.0046136942692101\n",
      "loss: 0.003980168607085943\n",
      "loss: 0.0006774484063498676\n",
      "loss: 0.0007169762975536287\n",
      "loss: 0.0023188565392047167\n",
      "loss: 0.0024454554077237844\n",
      "loss: 0.0005271774134598672\n",
      "loss: 0.0005154369864612818\n",
      "loss: 0.0018757496727630496\n",
      "loss: 0.0027700024656951427\n",
      "loss: 0.0019649043679237366\n",
      "loss: 0.004062851425260305\n",
      "loss: 0.001207159017212689\n",
      "loss: 0.00430053798481822\n",
      "loss: 0.004087566863745451\n",
      "loss: 0.0007782707689329982\n",
      "loss: 0.001100145629607141\n",
      "loss: 0.0017698154551908374\n",
      "loss: 0.0052443696185946465\n",
      "loss: 0.0018507337663322687\n",
      "loss: 0.00227349903434515\n",
      "loss: 0.001409303629770875\n",
      "loss: 0.0008088952163234353\n",
      "loss: 0.0016947109252214432\n",
      "loss: 0.0005458408850245178\n",
      "loss: 0.002158858347684145\n",
      "loss: 0.002140665426850319\n",
      "loss: 0.0005886202561669052\n",
      "loss: 0.0003168850962538272\n",
      "loss: 0.0007649025646969676\n",
      "loss: 0.0010794573463499546\n",
      "loss: 0.0009054666734300554\n",
      "loss: 0.0015979055315256119\n",
      "loss: 0.0017358288168907166\n",
      "loss: 0.00141378294210881\n",
      "loss: 0.0012641176581382751\n",
      "loss: 0.0005371560109779239\n",
      "loss: 0.0008612648816779256\n",
      "loss: 0.0019956924952566624\n",
      "loss: 0.0007610013126395643\n",
      "loss: 0.0005784534150734544\n",
      "loss: 0.0018047683406621218\n",
      "loss: 0.00119267706759274\n",
      "loss: 0.0010023497743532062\n",
      "loss: 0.00043007131898775697\n",
      "loss: 0.0021046821493655443\n",
      "loss: 0.0007261623977683485\n",
      "loss: 0.0017837536288425326\n",
      "loss: 0.001291693071834743\n",
      "loss: 0.0017644033068791032\n",
      "loss: 0.0017054278869181871\n",
      "loss: 0.0008865557610988617\n",
      "loss: 0.0003623579686973244\n",
      "loss: 0.0008291276171803474\n",
      "loss: 0.0006392464274540544\n",
      "loss: 0.000422188313677907\n",
      "loss: 0.0005950411432422698\n",
      "loss: 0.00018015709065366536\n",
      "loss: 0.00034698177478276193\n",
      "loss: 0.0032468547578901052\n",
      "loss: 0.0004103217215742916\n",
      "loss: 0.0009328253800049424\n",
      "loss: 0.00023992665228433907\n",
      "loss: 0.0011830661678686738\n",
      "loss: 0.0026899557560682297\n",
      "loss: 0.0003435592807363719\n",
      "loss: 0.0013712503714486957\n",
      "loss: 0.0019202996045351028\n",
      "loss: 0.0008213885012082756\n",
      "loss: 0.00031740040867589414\n",
      "loss: 0.0028101950883865356\n",
      "loss: 0.0006697450880892575\n",
      "loss: 0.000329402246279642\n",
      "loss: 0.001527660759165883\n",
      "loss: 0.0036926071625202894\n",
      "loss: 0.0009798334212973714\n",
      "loss: 0.002674382645636797\n",
      "loss: 0.0019897515885531902\n",
      "loss: 0.0008822502568364143\n",
      "loss: 0.000618588353972882\n",
      "loss: 0.000423240679083392\n",
      "loss: 0.00024351946194656193\n",
      "loss: 0.0015537969302386045\n",
      "loss: 0.0010196494404226542\n",
      "loss: 0.0009559140889905393\n",
      "loss: 0.0006783013814128935\n",
      "loss: 0.0028468328528106213\n",
      "loss: 0.0012340311659500003\n",
      "loss: 0.0003662503440864384\n",
      "loss: 0.000511069200001657\n",
      "loss: 0.0003600740747060627\n",
      "loss: 0.0010714437812566757\n",
      "loss: 0.0007653071079403162\n",
      "loss: 0.0018543187761679292\n",
      "loss: 0.0003096537839155644\n",
      "loss: 0.0006281581008806825\n",
      "loss: 0.001212993054650724\n",
      "loss: 0.00029393608565442264\n",
      "loss: 0.0008564821328036487\n",
      "loss: 0.0009728159639053047\n",
      "loss: 0.003359562251716852\n",
      "loss: 0.0009292701142840087\n",
      "loss: 0.0003030736406799406\n",
      "loss: 0.0005613117828033864\n",
      "loss: 0.0005692701088264585\n",
      "loss: 0.0006970805115997791\n",
      "loss: 0.0019486561650410295\n",
      "loss: 0.0028191518504172564\n",
      "loss: 0.0011174491373822093\n",
      "loss: 0.0005888398154638708\n",
      "loss: 0.0004847165255341679\n",
      "loss: 0.0022559158969670534\n",
      "loss: 0.0028813323006033897\n",
      "loss: 0.0009768700692802668\n",
      "loss: 0.0022806019987910986\n",
      "loss: 0.00150375219527632\n",
      "loss: 0.0004645702429115772\n",
      "loss: 0.0016887456877157092\n",
      "loss: 0.0039328704588115215\n",
      "loss: 0.004811003804206848\n",
      "loss: 0.0022029646206647158\n",
      "loss: 0.0004723312449641526\n",
      "loss: 0.002467904007062316\n",
      "loss: 0.0004721127625089139\n",
      "loss: 0.0006243036477826536\n",
      "loss: 0.0011757140746340156\n",
      "loss: 0.0016874263528734446\n",
      "loss: 0.0010188505984842777\n",
      "loss: 0.0008139529381878674\n",
      "loss: 0.001373019185848534\n",
      "loss: 0.0016116105252876878\n",
      "loss: 0.0021306134294718504\n",
      "loss: 0.0005658005247823894\n",
      "loss: 0.0025059690233319998\n",
      "loss: 0.0013043269282206893\n",
      "loss: 0.0028315531089901924\n",
      "loss: 0.0005847883294336498\n",
      "loss: 0.0022903564386069775\n",
      "loss: 0.0005087208119221032\n",
      "loss: 0.0004603535926435143\n",
      "loss: 0.0008967792964540422\n",
      "loss: 0.001319354516454041\n",
      "loss: 0.0018368728924542665\n",
      "loss: 0.00133901194203645\n",
      "loss: 0.0016543027013540268\n",
      "loss: 0.001055621774867177\n",
      "loss: 0.0039451164193451405\n",
      "loss: 0.002902660286054015\n",
      "loss: 0.0005901465192437172\n",
      "loss: 0.0047716619446873665\n",
      "loss: 0.0011146290926262736\n",
      "loss: 0.0008501458214595914\n",
      "loss: 0.0008396667544730008\n",
      "loss: 0.0009193735313601792\n",
      "loss: 0.0005400952650234103\n",
      "loss: 0.0008792333537712693\n",
      "loss: 0.0002626564528327435\n",
      "loss: 0.0020224701147526503\n",
      "loss: 0.00030259130289778113\n",
      "loss: 0.0013566429261118174\n",
      "loss: 0.0024717594496905804\n",
      "loss: 0.0016617143992334604\n",
      "loss: 0.002490898361429572\n",
      "loss: 0.0012444236781448126\n",
      "loss: 0.002171205123886466\n",
      "loss: 0.0010579084046185017\n",
      "loss: 0.0005346891121007502\n",
      "loss: 0.0005923002609051764\n",
      "loss: 0.0004997739451937377\n",
      "loss: 0.0011352923465892673\n",
      "loss: 0.0012903859606012702\n",
      "loss: 0.0009580582263879478\n",
      "loss: 0.00136366521473974\n",
      "loss: 0.0007985709235072136\n",
      "loss: 0.0010948452400043607\n",
      "loss: 0.00225517968647182\n",
      "loss: 0.0007436504238285124\n",
      "loss: 0.004336139652878046\n",
      "loss: 0.0008889225427992642\n",
      "loss: 0.0022014304995536804\n",
      "loss: 0.0012334393104538321\n",
      "loss: 0.0019540125504136086\n",
      "loss: 0.0024423834402114153\n",
      "loss: 0.0008352729491889477\n",
      "loss: 0.0014051705366000533\n",
      "loss: 0.0030780930537730455\n",
      "loss: 0.003920292016118765\n",
      "loss: 0.0004699132405221462\n",
      "loss: 0.0008897241204977036\n",
      "loss: 0.0005115823005326092\n",
      "loss: 0.0020798449404537678\n",
      "loss: 0.0005556353717111051\n",
      "loss: 0.00047599346726201475\n",
      "loss: 0.001527252490632236\n",
      "loss: 0.0017156839603558183\n",
      "loss: 0.000907176174223423\n",
      "loss: 0.0004231988568790257\n",
      "loss: 0.0024556047283113003\n",
      "loss: 0.0005688024102710187\n",
      "loss: 0.0009237133199349046\n",
      "loss: 0.00249992567114532\n",
      "loss: 0.000494200037792325\n",
      "loss: 0.0003131596895400435\n",
      "loss: 0.003525110660120845\n",
      "loss: 0.0013623222475871444\n",
      "loss: 0.0006236724439077079\n",
      "loss: 0.0011352509027346969\n",
      "loss: 0.0013839347520843148\n",
      "loss: 0.0032993832137435675\n",
      "loss: 0.003644082695245743\n",
      "loss: 0.0004185653815511614\n",
      "loss: 0.004873400088399649\n",
      "loss: 0.0021238604094833136\n",
      "loss: 0.0025193789042532444\n",
      "loss: 0.0006514968699775636\n",
      "loss: 0.0019248919561505318\n",
      "loss: 0.0024834182113409042\n",
      "loss: 0.0022305590100586414\n",
      "loss: 0.001850769273005426\n",
      "loss: 0.001794129959307611\n",
      "loss: 0.0035503448452800512\n",
      "loss: 0.0012137088924646378\n",
      "loss: 0.0025902842171490192\n",
      "loss: 0.0015569472452625632\n",
      "loss: 0.00280870427377522\n",
      "loss: 0.0010870179394260049\n",
      "loss: 0.002940674312412739\n",
      "loss: 0.0025296458043158054\n",
      "loss: 0.0015067222993820906\n",
      "loss: 0.0011582424631342292\n",
      "loss: 0.002061507198959589\n",
      "loss: 0.00048741858336143196\n",
      "loss: 0.0014179185964167118\n",
      "loss: 0.001443438814021647\n",
      "loss: 0.002683027880266309\n",
      "loss: 0.0010667195310816169\n",
      "loss: 0.001161811756901443\n",
      "loss: 0.0023475836496800184\n",
      "loss: 0.001929095364175737\n",
      "loss: 0.0005973419174551964\n",
      "loss: 0.00203536543995142\n",
      "loss: 0.0007400486501865089\n",
      "loss: 0.005604212637990713\n",
      "loss: 0.0017289454117417336\n",
      "loss: 0.0008688285015523434\n",
      "loss: 0.0011479234090074897\n",
      "loss: 0.0018777698278427124\n",
      "loss: 0.0010253323707729578\n",
      "loss: 0.0018385390285402536\n",
      "loss: 0.00038211068022064865\n",
      "loss: 0.0017052125185728073\n",
      "loss: 0.0017895189812406898\n",
      "loss: 0.0002632412943057716\n",
      "loss: 0.000369847723050043\n",
      "loss: 0.0007794661214575171\n",
      "loss: 0.0015499450964853168\n",
      "loss: 0.0005458096275106072\n",
      "loss: 0.0025233332999050617\n",
      "loss: 0.0005442071706056595\n",
      "loss: 0.00042664873762987554\n",
      "loss: 0.0006003616726957262\n",
      "loss: 0.001114705461077392\n",
      "loss: 0.0011214204132556915\n",
      "loss: 0.0030282612424343824\n",
      "loss: 0.0009001436992548406\n",
      "loss: 0.004322445951402187\n",
      "loss: 0.0025093534495681524\n",
      "loss: 0.0021739236544817686\n",
      "loss: 0.0009148312965407968\n",
      "loss: 0.0008867534925229847\n",
      "loss: 0.002097680466249585\n",
      "loss: 0.0005182313034310937\n",
      "loss: 0.0010402784682810307\n",
      "loss: 0.0011017057113349438\n",
      "loss: 0.0010483070509508252\n",
      "loss: 0.00094391725724563\n",
      "loss: 0.0005005851271562278\n",
      "loss: 0.0010214931098744273\n",
      "loss: 0.0018936041742563248\n",
      "loss: 0.003490092698484659\n",
      "loss: 0.00047983290278352797\n",
      "loss: 0.00027179953758604825\n",
      "loss: 0.002542773261666298\n",
      "loss: 0.0011443729745224118\n",
      "loss: 0.0025627589784562588\n",
      "loss: 0.0009717087377794087\n",
      "loss: 0.0011611434165388346\n",
      "loss: 0.004424680955708027\n",
      "loss: 0.0003186168323736638\n",
      "loss: 0.0003252151363994926\n",
      "loss: 0.0010840205941349268\n",
      "loss: 0.004171488806605339\n",
      "loss: 0.0022895976435393095\n",
      "loss: 0.001320728799328208\n",
      "loss: 0.000630546361207962\n",
      "loss: 0.002180310431867838\n",
      "loss: 0.0012401471612975001\n",
      "loss: 0.0012047835625708103\n",
      "loss: 0.0013055596500635147\n",
      "loss: 0.0016188633162528276\n",
      "loss: 0.001019319868646562\n",
      "loss: 0.0007999191293492913\n",
      "loss: 0.0007174119236879051\n",
      "loss: 0.0006152695859782398\n",
      "loss: 0.00033874224754981697\n",
      "loss: 0.0009663609671406448\n",
      "loss: 0.00112081877887249\n",
      "loss: 0.002742334036156535\n",
      "loss: 0.0007000762852840126\n",
      "loss: 0.0006538351881317794\n",
      "loss: 0.00042753323214128613\n",
      "loss: 0.0006745289429090917\n",
      "loss: 0.007461318280547857\n",
      "loss: 0.0005955476080998778\n",
      "loss: 0.0004309896321501583\n",
      "loss: 0.0005672217230312526\n",
      "loss: 0.0011089238105341792\n",
      "loss: 0.0009657071204856038\n",
      "loss: 0.0003438768908381462\n",
      "loss: 0.0009786043083295226\n",
      "loss: 0.0008952005882747471\n",
      "loss: 0.0012089493684470654\n",
      "loss: 0.0006901302258484066\n",
      "loss: 0.0014840259682387114\n",
      "loss: 0.0008976100943982601\n",
      "loss: 0.0008540768176317215\n",
      "loss: 0.0012863948941230774\n",
      "loss: 0.0005380738875828683\n",
      "loss: 0.002560301683843136\n",
      "loss: 0.002737879054620862\n",
      "loss: 0.0004462004581000656\n",
      "loss: 0.0011822369415313005\n",
      "loss: 0.00158520822878927\n",
      "loss: 0.002049865899607539\n",
      "loss: 0.001335824723355472\n",
      "loss: 0.00045893798233009875\n",
      "loss: 0.0008552237413823605\n",
      "loss: 0.0013595236232504249\n",
      "loss: 0.003259931458160281\n",
      "loss: 0.00068977678893134\n",
      "loss: 0.0006837653345428407\n",
      "loss: 0.0020021817181259394\n",
      "loss: 0.004751902539283037\n",
      "loss: 0.0008490237523801625\n",
      "loss: 0.0012131492840126157\n",
      "loss: 0.0030126755591481924\n",
      "loss: 0.0013595804339274764\n",
      "loss: 0.0006194514571689069\n",
      "loss: 0.0007608549785800278\n",
      "loss: 0.0005495651857927442\n",
      "loss: 0.003086475655436516\n",
      "loss: 0.0005920277326367795\n",
      "loss: 0.002280663698911667\n",
      "loss: 0.0010961497901007533\n",
      "loss: 0.0004963139654137194\n",
      "loss: 0.0005428050644695759\n",
      "loss: 0.0006659780628979206\n",
      "loss: 0.0002120128192473203\n",
      "loss: 0.0006813771324232221\n",
      "loss: 0.0018470203503966331\n",
      "loss: 0.001745710032992065\n",
      "loss: 0.0011816489277407527\n",
      "loss: 0.00080159364733845\n",
      "loss: 0.00580985564738512\n",
      "loss: 0.00048596804845146835\n",
      "loss: 0.001054758089594543\n",
      "loss: 0.001929059042595327\n",
      "loss: 0.0006953920819796622\n",
      "loss: 0.00249922345392406\n",
      "loss: 0.0019053894793614745\n",
      "loss: 0.00061440170975402\n",
      "loss: 0.0021994549315422773\n",
      "loss: 0.0015231295255944133\n",
      "loss: 0.0024304247926920652\n",
      "loss: 0.0007964367978274822\n",
      "loss: 0.001520307152532041\n",
      "loss: 0.000511910009663552\n",
      "loss: 0.000800370064098388\n",
      "loss: 0.0004564425034914166\n",
      "loss: 0.003738773986697197\n",
      "loss: 0.0009064311161637306\n",
      "loss: 0.0011880589881911874\n",
      "loss: 0.0005484079592861235\n",
      "loss: 0.0022398612927645445\n",
      "loss: 0.001901976647786796\n",
      "loss: 0.0002543739101383835\n",
      "loss: 0.0007768689538352191\n",
      "loss: 0.0011641902383416891\n",
      "loss: 0.0026047953870147467\n",
      "loss: 0.0025985606480389833\n",
      "loss: 0.0010840141912922263\n",
      "loss: 0.0012220176868140697\n",
      "loss: 0.0010296553373336792\n",
      "loss: 0.0038166397716850042\n",
      "loss: 0.002929005306214094\n",
      "loss: 0.0027874649967998266\n",
      "loss: 0.0011145739117637277\n",
      "loss: 0.000765464676078409\n",
      "loss: 0.0026155360974371433\n",
      "loss: 0.0033788380678743124\n",
      "loss: 0.0021772021427750587\n",
      "loss: 0.0006000887369737029\n",
      "loss: 0.00251337350346148\n",
      "loss: 0.0008982348372228444\n",
      "loss: 0.00038983309059403837\n",
      "loss: 0.00248540798202157\n",
      "loss: 0.0008762003853917122\n",
      "loss: 0.0006417601252906024\n",
      "loss: 0.00045259660691954195\n",
      "loss: 0.002050998853519559\n",
      "loss: 0.0009824056178331375\n",
      "loss: 0.000321920815622434\n",
      "loss: 0.0018956772983074188\n",
      "loss: 0.001376375206746161\n",
      "loss: 0.0017783083021640778\n",
      "loss: 0.00330480863340199\n",
      "loss: 0.0005317638861015439\n",
      "loss: 0.0012974877608940005\n",
      "loss: 0.0011495028156787157\n",
      "loss: 0.00158814643509686\n",
      "loss: 0.001318640890531242\n",
      "loss: 0.0008265801006928086\n",
      "loss: 0.0009672206942923367\n",
      "loss: 0.001067227916792035\n",
      "loss: 0.001651841215789318\n",
      "loss: 0.0003270305460318923\n",
      "loss: 0.0008600239525549114\n",
      "loss: 0.0006197183392941952\n",
      "loss: 0.0006518772570416331\n",
      "loss: 0.002567236078903079\n",
      "loss: 0.00042778500937856734\n",
      "loss: 0.0007603733101859689\n",
      "loss: 0.0007748312200419605\n",
      "loss: 0.001597382128238678\n",
      "loss: 0.0017912250477820635\n",
      "loss: 0.0011852200841531157\n",
      "loss: 0.0008346981485374272\n",
      "loss: 0.0005012149922549725\n",
      "loss: 0.001467919792048633\n",
      "loss: 0.0011446140706539154\n",
      "loss: 0.0031808752100914717\n",
      "loss: 0.0028073135763406754\n",
      "loss: 0.0011048862943425775\n",
      "loss: 0.001491930685006082\n",
      "loss: 0.00042683040373958647\n",
      "loss: 0.0004862772475462407\n",
      "loss: 0.0026750348042696714\n",
      "loss: 0.0004943652893416584\n",
      "loss: 0.003568116342648864\n",
      "loss: 0.002558085834607482\n",
      "loss: 0.0005447706789709628\n",
      "loss: 0.0006857924163341522\n",
      "loss: 0.0008057369268499315\n",
      "loss: 0.000731568259652704\n",
      "loss: 0.001223630621097982\n",
      "loss: 0.00208710297010839\n",
      "loss: 0.0010835680877789855\n",
      "loss: 0.00244099460542202\n",
      "loss: 0.000608213187661022\n",
      "loss: 0.00206405739299953\n",
      "loss: 0.0022135113831609488\n",
      "loss: 0.0014075648505240679\n",
      "loss: 0.0004010906850453466\n",
      "loss: 0.00045355616020970047\n",
      "loss: 0.0024129555094987154\n",
      "loss: 0.00205317628569901\n",
      "loss: 0.00197026738896966\n",
      "loss: 0.0020980481058359146\n",
      "loss: 0.003114266786724329\n",
      "loss: 0.0035737776197493076\n",
      "loss: 0.0007813657866790891\n",
      "loss: 0.0008860031375661492\n",
      "loss: 0.002679706085473299\n",
      "loss: 0.004261216148734093\n",
      "loss: 0.0011062342673540115\n",
      "loss: 0.00071735680103302\n",
      "loss: 0.001222836785018444\n",
      "loss: 0.0024908906780183315\n",
      "loss: 0.003083995310589671\n",
      "loss: 0.00041172621422447264\n",
      "loss: 0.0007521308143623173\n",
      "loss: 0.003687141230329871\n",
      "loss: 0.0022188255097717047\n",
      "loss: 0.0011824682587757707\n",
      "loss: 0.0016556446207687259\n",
      "loss: 0.0016584250843152404\n",
      "loss: 0.0005118716508150101\n",
      "loss: 0.0015329511370509863\n",
      "loss: 0.0007264328305609524\n",
      "loss: 0.0008392790332436562\n",
      "loss: 0.000736637506633997\n",
      "loss: 0.001274480833671987\n",
      "loss: 0.0007686723838560283\n",
      "loss: 0.0017285265494138002\n",
      "loss: 0.002494555665180087\n",
      "loss: 0.0005431916797533631\n",
      "loss: 0.0007489510462619364\n",
      "loss: 0.0011098686372861266\n",
      "loss: 0.0006048432551324368\n",
      "loss: 0.0021130938548594713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 144\u001b[0m\n\u001b[1;32m    140\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# 训练DQN模型\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(env, model, target_model, optimizer, buffer, gamma, batch_size, num_episodes, target_update_freq)\u001b[0m\n\u001b[1;32m    102\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m target_model(batch_next_state\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m target_values \u001b[38;5;241m=\u001b[39m batch_reward \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m batch_done) \u001b[38;5;241m*\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_q_values\n\u001b[0;32m--> 105\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mq_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    106\u001b[0m target_q_values[\u001b[38;5;28mrange\u001b[39m(batch_size), batch_action] \u001b[38;5;241m=\u001b[39m target_values\n\u001b[1;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(q_values, target_q_values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 定义简单的DQN网络\n",
    "# DQN的目标是学习到一个在给定状态下，选择最优动作的策略，从而使智能体能够在环境中获取最大的累积奖励。\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 定义经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            del self.buffer[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 定义一个简单的环境，将MNIST数据集作为状态空间\n",
    "class MNISTEnvironment:\n",
    "    def __init__(self):\n",
    "        self.dataset = MNIST(root='/data/mwj/data', train=True, download=False,\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]))\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=1, shuffle=True)\n",
    "        self.current_state = None\n",
    "        self.action_space = [i for i in range(10)]\n",
    "\n",
    "    def reset(self):\n",
    "        image, label = next(iter(self.dataloader))\n",
    "        self.current_state = image.view(-1).numpy()\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        try:\n",
    "            next_image, label = next(iter(self.dataloader))\n",
    "            next_state = next_image.view(-1).numpy()\n",
    "        except StopIteration:\n",
    "            done = True\n",
    "            next_state = self.current_state\n",
    "        if action == label:\n",
    "            reward = 1\n",
    "        return next_state, reward, done\n",
    "\n",
    "# 定义DQN的训练过程\n",
    "def train_dqn(env, model, target_model, optimizer, buffer, gamma, batch_size, num_episodes, target_update_freq):\n",
    "    criterion = nn.MSELoss()\n",
    "    total_steps = 0  # 记录总的训练步数\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            epsilon = 0.1\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(env.action_space)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(torch.tensor(state, dtype=torch.float32).to(device))\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            buffer.push((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if len(buffer) > batch_size:\n",
    "                transitions = buffer.sample(batch_size)\n",
    "                batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "\n",
    "                batch_state = torch.tensor(batch_state, dtype=torch.float32).to(device)\n",
    "                batch_action = torch.tensor(batch_action, dtype=torch.long).to(device)\n",
    "                batch_reward = torch.tensor(batch_reward, dtype=torch.float32).to(device)\n",
    "                batch_next_state = torch.tensor(batch_next_state, dtype=torch.float32).to(device)\n",
    "                batch_done = torch.tensor(batch_done, dtype=torch.float32).to(device)\n",
    "\n",
    "                q_values = model(batch_state.to(device))\n",
    "                next_q_values = target_model(batch_next_state.to(device)).max(dim=1)[0]\n",
    "                target_values = batch_reward + (1 - batch_done) * gamma * next_q_values\n",
    "\n",
    "                target_q_values = q_values.clone().detach()\n",
    "                target_q_values[range(batch_size), batch_action] = target_values\n",
    "\n",
    "                loss = criterion(q_values, target_q_values)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 软更新目标网络\n",
    "                if total_steps % target_update_freq == 0:\n",
    "                    soft_update(target_model, model, tau=0.01)  # 设置软更新参数 tau\n",
    "            \n",
    "                # print(f\"loss: {loss.item()}\")\n",
    "            total_steps += 1  # 更新总的训练步数\n",
    "\n",
    "        print(\"Episode {}: Total Reward = {}\".format(episode, total_reward))\n",
    "\n",
    "def soft_update(target_model, model, tau):\n",
    "    for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# 创建环境和模型\n",
    "env = MNISTEnvironment()\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "model = DQN(input_size, output_size).to(device)\n",
    "target_model = DQN(input_size, output_size).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "num_episodes = 1\n",
    "\n",
    "\n",
    "# 训练DQN模型\n",
    "train_dqn(env, model, target_model, optimizer, buffer, gamma, batch_size, num_episodes, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个示例代码中，我们使用DQN来训练一个智能体（agent），通过观察MNIST数据集中的图像来学习识别手写数字。在这个示例中，MNIST数据集被作为状态空间，智能体通过观察图像并采取动作来尝试识别数字。\n",
    "\n",
    "\n",
    "在训练完成后，可以通过评估模型在测试集上的性能来评估其效果。在MNIST手写数字识别问题中，你可以使用测试集的图像和标签来评估模型的准确率（accuracy）。下面是一个简单的示例代码，演示如何评估训练好的模型在测试集上的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn(env, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in env.dataloader:\n",
    "            images = images.view(-1).numpy()\n",
    "            q_values = model(torch.tensor(images, dtype=torch.float32))\n",
    "            action = torch.argmax(q_values).item()\n",
    "            if action == labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# 调用测试函数进行模型评估\n",
    "test_dqn(env, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 假设 y_true 和 y_pred 已定义\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# 为了演示，创建一个示例混淆矩阵\n",
    "cm = np.array([[186, 37, 12], [17, 171, 23], [12, 23, 98]])\n",
    "labels_txt = ['Class 0', 'Class 1', 'Class 2']  # 示例标签\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "tick_marks = np.arange(len(labels_txt))\n",
    "plt.xticks(tick_marks, labels_txt, rotation=45)\n",
    "\n",
    "\n",
    "# 1使用 'Blues' 颜色映射，Y轴从上到下是class0,class1,class2的顺序，不用加[::-1]逆序，但是不能保存为pdf，显示有斜条纹，但是png就可以\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest', aspect='equal')  # 添加 cmap 参数\n",
    "plt.yticks(tick_marks, labels_txt)\n",
    "\n",
    "# 2使用 pcolormesh 替代 imshow，这种会导致Y轴从上到下不是class0,class1,class2的顺序，解决方法：plt.yticks(tick_marks, labels_txt[::-1])增加那个[::-1]\n",
    "# 强制将Y轴文字逆序，但是还有个问题，cm也会变成按照第二维度逆序的，所以也需要将cm也按照列逆序一下\n",
    "# X, Y = np.meshgrid(np.arange(cm.shape[1]), np.arange(cm.shape[0]))\n",
    "# cm = cm[::-1]\n",
    "# plt.pcolormesh(X, Y, cm, cmap='Blues', linewidth=0.5) #edgecolors='k', \n",
    "# plt.yticks(tick_marks, labels_txt[::-1])\n",
    "\n",
    "\n",
    "# 添加颜色条\n",
    "plt.colorbar()\n",
    "\n",
    "# 在单元格中添加数值，并动态设置文本颜色\n",
    "thresh = cm.max() / 2  # 用于判断文本颜色的阈值\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(\n",
    "        j,\n",
    "        i,\n",
    "        f\"{cm[i, j]}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\",  # 动态设置颜色\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.tight_layout()\n",
    "# plt.imshow，保存为png，使用pcolormesh，可以保存为pdf\n",
    "plt.savefig(\"confusionmatrix2_.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
