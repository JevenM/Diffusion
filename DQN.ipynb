{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何使用pytorch框架实现一个mnist手写数字识别的简单的强化学习demo？\n",
    "\n",
    "实现一个简单的基于PyTorch的强化学习（RL）Demo来解决MNIST手写数字识别问题是不太常见的，因为通常RL更适用于连续动作和状态空间的问题，而不是像MNIST这样的离散分类问题。然而，你可以将这个问题转换成一个RL问题，例如，你可以尝试使用深度强化学习算法，如DQN，来训练一个代理（agent）来处理MNIST数据。\n",
    "\n",
    "下面是一个简单的示例代码，演示如何使用PyTorch和DQN来解决MNIST手写数字识别问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.012562339194118977\n",
      "loss: 0.008099504746496677\n",
      "loss: 0.005411564838141203\n",
      "loss: 0.004174706991761923\n",
      "loss: 0.0034522463101893663\n",
      "loss: 0.002754356013610959\n",
      "loss: 0.002635128563269973\n",
      "loss: 0.0029291007667779922\n",
      "loss: 0.005342904012650251\n",
      "loss: 0.005789635237306356\n",
      "loss: 0.004471152555197477\n",
      "loss: 0.006033007055521011\n",
      "loss: 0.004363666754215956\n",
      "loss: 0.004042037297040224\n",
      "loss: 0.006107087712734938\n",
      "loss: 0.005817280150949955\n",
      "loss: 0.004700637888163328\n",
      "loss: 0.0019112963927909732\n",
      "loss: 0.0033415004145354033\n",
      "loss: 0.002527732402086258\n",
      "loss: 0.0015616053715348244\n",
      "loss: 0.004957437515258789\n",
      "loss: 0.0041802204214036465\n",
      "loss: 0.003533989191055298\n",
      "loss: 0.00202165893279016\n",
      "loss: 0.0035462421365082264\n",
      "loss: 0.0010870522819459438\n",
      "loss: 0.001648181350901723\n",
      "loss: 0.002967839129269123\n",
      "loss: 0.0006663869135081768\n",
      "loss: 0.0022631410975009203\n",
      "loss: 0.00031358475098386407\n",
      "loss: 0.0013479801127687097\n",
      "loss: 0.0032708204817026854\n",
      "loss: 0.0029314581770449877\n",
      "loss: 0.0011303186183795333\n",
      "loss: 0.0025356963742524385\n",
      "loss: 0.003118448192253709\n",
      "loss: 0.0017165429890155792\n",
      "loss: 0.00375930592417717\n",
      "loss: 0.002820338122546673\n",
      "loss: 0.0008604589966125786\n",
      "loss: 0.0033885010052472353\n",
      "loss: 0.0014107618480920792\n",
      "loss: 0.0025661156978458166\n",
      "loss: 0.0016799032455310225\n",
      "loss: 0.0014528532046824694\n",
      "loss: 0.0023916473146528006\n",
      "loss: 0.0005401356029324234\n",
      "loss: 0.0011763358488678932\n",
      "loss: 0.0022090994752943516\n",
      "loss: 0.0011416553752496839\n",
      "loss: 0.0008286235970444977\n",
      "loss: 0.0010184819111600518\n",
      "loss: 0.0009480615262873471\n",
      "loss: 0.0034031409304589033\n",
      "loss: 0.0009069369989447296\n",
      "loss: 0.00030490368953906\n",
      "loss: 0.0024134849663823843\n",
      "loss: 0.0018385384464636445\n",
      "loss: 0.0003115593863185495\n",
      "loss: 0.0011043440317735076\n",
      "loss: 0.0015052742091938853\n",
      "loss: 0.0007222624844871461\n",
      "loss: 0.0003296022186987102\n",
      "loss: 0.001648670993745327\n",
      "loss: 0.004711949732154608\n",
      "loss: 0.0015770121244713664\n",
      "loss: 0.0015117990551516414\n",
      "loss: 0.00033856616937555373\n",
      "loss: 0.0016432658303529024\n",
      "loss: 0.0038269024807959795\n",
      "loss: 0.00043430892401374876\n",
      "loss: 0.0037302381824702024\n",
      "loss: 0.003528932575136423\n",
      "loss: 0.0014599153073504567\n",
      "loss: 0.00048162959865294397\n",
      "loss: 0.0005929889157414436\n",
      "loss: 0.0006785590085200965\n",
      "loss: 0.0008468645974062383\n",
      "loss: 0.0029912181198596954\n",
      "loss: 0.00031691850745119154\n",
      "loss: 0.0007024265360087156\n",
      "loss: 0.00047875032760202885\n",
      "loss: 0.0015358240343630314\n",
      "loss: 0.007573753595352173\n",
      "loss: 0.0011377999326214194\n",
      "loss: 0.0062746829353272915\n",
      "loss: 0.003887797240167856\n",
      "loss: 0.0037376489490270615\n",
      "loss: 0.0018494073301553726\n",
      "loss: 0.003238316625356674\n",
      "loss: 0.007461113389581442\n",
      "loss: 0.0032358875032514334\n",
      "loss: 0.0024708067066967487\n",
      "loss: 0.0012647219700738788\n",
      "loss: 0.004252330400049686\n",
      "loss: 0.0006084296619519591\n",
      "loss: 0.0014152040239423513\n",
      "loss: 0.0026206281036138535\n",
      "loss: 0.0004956735647283494\n",
      "loss: 0.0016671185148879886\n",
      "loss: 0.0012141868937760592\n",
      "loss: 0.0004983441322110593\n",
      "loss: 0.0011120367562398314\n",
      "loss: 0.0005162072484381497\n",
      "loss: 0.0019335513934493065\n",
      "loss: 0.0010757171548902988\n",
      "loss: 0.0022000877652317286\n",
      "loss: 0.001042816205881536\n",
      "loss: 0.001163059496320784\n",
      "loss: 0.0009876894764602184\n",
      "loss: 0.0008236758294515312\n",
      "loss: 0.0009309444576501846\n",
      "loss: 0.00043998361798003316\n",
      "loss: 0.0001960261579370126\n",
      "loss: 0.0011186515912413597\n",
      "loss: 0.001291866647079587\n",
      "loss: 0.0006152267451398075\n",
      "loss: 0.0011074269423261285\n",
      "loss: 0.00021048453345429152\n",
      "loss: 0.0003630340506788343\n",
      "loss: 0.0007588276639580727\n",
      "loss: 0.00034804342431016266\n",
      "loss: 0.0004423523787409067\n",
      "loss: 0.0004705965693574399\n",
      "loss: 0.0006355368532240391\n",
      "loss: 0.0007640778785571456\n",
      "loss: 0.0003190301649738103\n",
      "loss: 0.00014439875667449087\n",
      "loss: 0.003461219137534499\n",
      "loss: 0.004326823633164167\n",
      "loss: 0.0004414177674334496\n",
      "loss: 0.00018335827917326242\n",
      "loss: 0.00032542593544349074\n",
      "loss: 0.00015922868624329567\n",
      "loss: 0.0024941812735050917\n",
      "loss: 0.0005553046357817948\n",
      "loss: 0.00048352297744713724\n",
      "loss: 0.0003006925981026143\n",
      "loss: 0.003382966620847583\n",
      "loss: 0.005092869978398085\n",
      "loss: 0.00019527167023625225\n",
      "loss: 0.004478229209780693\n",
      "loss: 0.00045273135765455663\n",
      "loss: 0.00027998595032840967\n",
      "loss: 0.0003114350838586688\n",
      "loss: 0.0013003175845369697\n",
      "loss: 0.0003458700084593147\n",
      "loss: 0.00032091609318740666\n",
      "loss: 0.00042275787563994527\n",
      "loss: 0.00017450680024921894\n",
      "loss: 0.002485282253473997\n",
      "loss: 0.0005813510506413877\n",
      "loss: 0.0006153883296065032\n",
      "loss: 0.0004297216946724802\n",
      "loss: 0.0021061219740659\n",
      "loss: 0.003193038748577237\n",
      "loss: 0.0004977117641828954\n",
      "loss: 0.0005606903578154743\n",
      "loss: 0.0015018987469375134\n",
      "loss: 0.0009417342371307313\n",
      "loss: 0.0020009272266179323\n",
      "loss: 0.0011827613925561309\n",
      "loss: 0.0005494991200976074\n",
      "loss: 0.0004041909705847502\n",
      "loss: 0.002263021655380726\n",
      "loss: 0.00203399034217\n",
      "loss: 0.0005902012344449759\n",
      "loss: 0.005584431812167168\n",
      "loss: 0.0014615602558478713\n",
      "loss: 0.0009174475562758744\n",
      "loss: 0.003165480447933078\n",
      "loss: 0.000921299506444484\n",
      "loss: 0.00046813508379273117\n",
      "loss: 0.0027387612499296665\n",
      "loss: 0.0004032870347145945\n",
      "loss: 0.0016273176297545433\n",
      "loss: 0.0019027431262657046\n",
      "loss: 0.0019988378044217825\n",
      "loss: 0.0003144699730910361\n",
      "loss: 0.0019469511462375522\n",
      "loss: 0.0016946196556091309\n",
      "loss: 0.0010895113227888942\n",
      "loss: 0.0015764603158459067\n",
      "loss: 0.0012453723466023803\n",
      "loss: 0.0014216355048120022\n",
      "loss: 0.0008625093032605946\n",
      "loss: 0.0007342112949118018\n",
      "loss: 0.0012164259096607566\n",
      "loss: 0.0005252563860267401\n",
      "loss: 0.0018415696686133742\n",
      "loss: 0.00046578794717788696\n",
      "loss: 0.0003654354950413108\n",
      "loss: 0.0008912553312256932\n",
      "loss: 0.0011723487405106425\n",
      "loss: 0.0011094581568613648\n",
      "loss: 0.00026708602672442794\n",
      "loss: 0.0009937185095623136\n",
      "loss: 0.0008075462537817657\n",
      "loss: 0.0024328806903213263\n",
      "loss: 0.0006261375383473933\n",
      "loss: 0.0023651360534131527\n",
      "loss: 0.0007745250477455556\n",
      "loss: 0.0005769384442828596\n",
      "loss: 0.00039832937181927264\n",
      "loss: 0.0008495214278809726\n",
      "loss: 0.0023874423932284117\n",
      "loss: 0.000663018727209419\n",
      "loss: 0.001324775512330234\n",
      "loss: 0.0006758141680620611\n",
      "loss: 0.0009496938437223434\n",
      "loss: 0.0031366252806037664\n",
      "loss: 0.0008688644738867879\n",
      "loss: 0.0008659933810122311\n",
      "loss: 0.0015341469552367926\n",
      "loss: 0.000980678480118513\n",
      "loss: 0.0013775049010291696\n",
      "loss: 0.000991986715234816\n",
      "loss: 0.0006646838155575097\n",
      "loss: 0.00020173890516161919\n",
      "loss: 0.001602310105226934\n",
      "loss: 0.00045850203605368733\n",
      "loss: 0.00028137105982750654\n",
      "loss: 0.0005875431816093624\n",
      "loss: 0.0010315763065591455\n",
      "loss: 0.0027261932846158743\n",
      "loss: 0.0002998019626829773\n",
      "loss: 0.0006859550485387444\n",
      "loss: 0.002015970181673765\n",
      "loss: 0.004102669190615416\n",
      "loss: 0.0026141605339944363\n",
      "loss: 0.0026138133835047483\n",
      "loss: 0.0007177951629273593\n",
      "loss: 0.0006706471904180944\n",
      "loss: 0.0023768669925630093\n",
      "loss: 0.0008517528185620904\n",
      "loss: 0.00326210493221879\n",
      "loss: 0.001226540538482368\n",
      "loss: 0.00045201738248579204\n",
      "loss: 0.0005700200563296676\n",
      "loss: 0.003173458157107234\n",
      "loss: 0.0007825010688975453\n",
      "loss: 0.001919377245940268\n",
      "loss: 0.0009916573762893677\n",
      "loss: 0.0031563632655888796\n",
      "loss: 0.00045001861872151494\n",
      "loss: 0.001569657470099628\n",
      "loss: 0.0004130510787945241\n",
      "loss: 0.0026236854027956724\n",
      "loss: 0.00030758464708924294\n",
      "loss: 0.0016028694808483124\n",
      "loss: 0.0005837101489305496\n",
      "loss: 0.0009654233581386507\n",
      "loss: 0.0006630552234128118\n",
      "loss: 0.0004356824792921543\n",
      "loss: 0.0007867802050895989\n",
      "loss: 0.0029604327864944935\n",
      "loss: 0.0007935565081425011\n",
      "loss: 0.0048145391047000885\n",
      "loss: 0.0015835625817999244\n",
      "loss: 0.006146397441625595\n",
      "loss: 0.0010296193649992347\n",
      "loss: 0.0020527211017906666\n",
      "loss: 0.0012198422336950898\n",
      "loss: 0.0007966976845636964\n",
      "loss: 0.002138335956260562\n",
      "loss: 0.00042153598042204976\n",
      "loss: 0.0017919879173859954\n",
      "loss: 0.003120396053418517\n",
      "loss: 0.003132896963506937\n",
      "loss: 0.0012217145413160324\n",
      "loss: 0.0023254358675330877\n",
      "loss: 0.0028489225078374147\n",
      "loss: 0.0003502779291011393\n",
      "loss: 0.0017491815378889441\n",
      "loss: 0.0021444361191242933\n",
      "loss: 0.0006569179240614176\n",
      "loss: 0.0008221642347052693\n",
      "loss: 0.0018941097659990191\n",
      "loss: 0.0008316081366501749\n",
      "loss: 0.0005228963564150035\n",
      "loss: 0.0005456435028463602\n",
      "loss: 0.0023232540115714073\n",
      "loss: 0.0009216161561198533\n",
      "loss: 0.00039480935083702207\n",
      "loss: 0.0011444835690781474\n",
      "loss: 0.0017570689087733626\n",
      "loss: 0.0007998846704140306\n",
      "loss: 0.0016700158594176173\n",
      "loss: 0.0011302169878035784\n",
      "loss: 0.0018226243555545807\n",
      "loss: 0.002133038593456149\n",
      "loss: 0.0015193712897598743\n",
      "loss: 0.0005368593265302479\n",
      "loss: 0.001715100253932178\n",
      "loss: 0.0005943160504102707\n",
      "loss: 0.0016339157009497285\n",
      "loss: 0.0007196599617600441\n",
      "loss: 0.0008474077912978828\n",
      "loss: 0.002828640164807439\n",
      "loss: 0.005367595236748457\n",
      "loss: 0.0007806185749359429\n",
      "loss: 0.0008929599425755441\n",
      "loss: 0.0013208402087911963\n",
      "loss: 0.0011546139139682055\n",
      "loss: 0.0057162633165717125\n",
      "loss: 0.0009080246090888977\n",
      "loss: 0.0008172689122147858\n",
      "loss: 0.0008325762464664876\n",
      "loss: 0.0014267958467826247\n",
      "loss: 0.001708664232864976\n",
      "loss: 0.0012073060497641563\n",
      "loss: 0.0009080478921532631\n",
      "loss: 0.0008280322072096169\n",
      "loss: 0.0011223836336284876\n",
      "loss: 0.0009002626757137477\n",
      "loss: 0.0018717985367402434\n",
      "loss: 0.0009629908017814159\n",
      "loss: 0.0032976919319480658\n",
      "loss: 0.0030571322422474623\n",
      "loss: 0.000888296402990818\n",
      "loss: 0.001110527548007667\n",
      "loss: 0.00045955527457408607\n",
      "loss: 0.0011214505648240447\n",
      "loss: 0.000730892876163125\n",
      "loss: 0.0025218434166163206\n",
      "loss: 0.00030425103614106774\n",
      "loss: 0.0015439342241734266\n",
      "loss: 0.0022421248722821474\n",
      "loss: 0.002591171767562628\n",
      "loss: 0.0011054622009396553\n",
      "loss: 0.002125559141859412\n",
      "loss: 0.00065346818882972\n",
      "loss: 0.0009950341191142797\n",
      "loss: 0.0031539485789835453\n",
      "loss: 0.0004457230679690838\n",
      "loss: 0.0006877534324303269\n",
      "loss: 0.0012099905870854855\n",
      "loss: 0.0020899698138237\n",
      "loss: 0.0025083504151552916\n",
      "loss: 0.0016827507643029094\n",
      "loss: 0.0003400423738639802\n",
      "loss: 0.0014966236194595695\n",
      "loss: 0.0015588366659358144\n",
      "loss: 0.0005051837651990354\n",
      "loss: 0.003542958525940776\n",
      "loss: 0.00030284229433164\n",
      "loss: 0.0008464821730740368\n",
      "loss: 0.003045295365154743\n",
      "loss: 0.0003440387372393161\n",
      "loss: 0.0005737234023399651\n",
      "loss: 0.0007551279268227518\n",
      "loss: 0.0013965100515633821\n",
      "loss: 0.0010326671181246638\n",
      "loss: 0.0015116446884348989\n",
      "loss: 0.0017690298845991492\n",
      "loss: 0.003524204483255744\n",
      "loss: 0.0024764433037489653\n",
      "loss: 0.0005315520684234798\n",
      "loss: 0.0015930816298350692\n",
      "loss: 0.0018504838226363063\n",
      "loss: 0.0020893968176096678\n",
      "loss: 0.000942194543313235\n",
      "loss: 0.0010743197053670883\n",
      "loss: 0.005172437056899071\n",
      "loss: 0.0011947095626965165\n",
      "loss: 0.0008380789658986032\n",
      "loss: 0.0024234310258179903\n",
      "loss: 0.0021688833367079496\n",
      "loss: 0.0023025318514555693\n",
      "loss: 0.001523554907180369\n",
      "loss: 0.0015182208735495806\n",
      "loss: 0.0009509906521998346\n",
      "loss: 0.001250385190360248\n",
      "loss: 0.000980685348622501\n",
      "loss: 0.0016504405066370964\n",
      "loss: 0.0009461860172450542\n",
      "loss: 0.00406259298324585\n",
      "loss: 0.0009351694025099277\n",
      "loss: 0.0013897748431190848\n",
      "loss: 0.0006196931353770196\n",
      "loss: 0.0005079383845441043\n",
      "loss: 0.0003819089033640921\n",
      "loss: 0.0005896735819987953\n",
      "loss: 0.0008909126627258956\n",
      "loss: 0.0007516597397625446\n",
      "loss: 0.0005250764661468565\n",
      "loss: 0.002901286818087101\n",
      "loss: 0.0005564968450926244\n",
      "loss: 0.001396271400153637\n",
      "loss: 0.0011180862784385681\n",
      "loss: 0.0013293892843648791\n",
      "loss: 0.0004576187639031559\n",
      "loss: 0.0012503809994086623\n",
      "loss: 0.0003660671063698828\n",
      "loss: 0.0034669784363359213\n",
      "loss: 0.0015103182522580028\n",
      "loss: 0.001103667775169015\n",
      "loss: 0.0009215534664690495\n",
      "loss: 0.001033884473145008\n",
      "loss: 0.0010799525771290064\n",
      "loss: 0.0011892573675140738\n",
      "loss: 0.0006849858909845352\n",
      "loss: 0.0010078303748741746\n",
      "loss: 0.0010974728502333164\n",
      "loss: 0.0004503785166889429\n",
      "loss: 0.0008523695287294686\n",
      "loss: 0.0006015896797180176\n",
      "loss: 0.003178147366270423\n",
      "loss: 0.0009076492860913277\n",
      "loss: 0.0006984215578995645\n",
      "loss: 0.0012073175748810172\n",
      "loss: 0.001048984588123858\n",
      "loss: 0.0005273191491141915\n",
      "loss: 0.004187862854450941\n",
      "loss: 0.0024259397760033607\n",
      "loss: 0.0006183207733556628\n",
      "loss: 0.00027239538030698895\n",
      "loss: 0.0024149983655661345\n",
      "loss: 0.0005724295042455196\n",
      "loss: 0.004649966489523649\n",
      "loss: 0.0019655160140246153\n",
      "loss: 0.0003072167746722698\n",
      "loss: 0.002246298361569643\n",
      "loss: 0.0007056845352053642\n",
      "loss: 0.0006844747113063931\n",
      "loss: 0.00622622761875391\n",
      "loss: 0.00034205717383883893\n",
      "loss: 0.002961531048640609\n",
      "loss: 0.002482806099578738\n",
      "loss: 0.0003820566926151514\n",
      "loss: 0.0003789593174587935\n",
      "loss: 0.000999856973066926\n",
      "loss: 0.0007958635105751455\n",
      "loss: 0.0021952209062874317\n",
      "loss: 0.000716189038939774\n",
      "loss: 0.0022320423740893602\n",
      "loss: 0.003579448675736785\n",
      "loss: 0.0006676562479697168\n",
      "loss: 0.0014926219591870904\n",
      "loss: 0.0016422637272626162\n",
      "loss: 0.0013596458593383431\n",
      "loss: 0.000582390814088285\n",
      "loss: 0.0005918056122027338\n",
      "loss: 0.0023660436272621155\n",
      "loss: 0.001507140463218093\n",
      "loss: 0.0022372640669345856\n",
      "loss: 0.002010492142289877\n",
      "loss: 0.0004763845936395228\n",
      "loss: 0.0005632531829178333\n",
      "loss: 0.0007453752914443612\n",
      "loss: 0.00027846379089169204\n",
      "loss: 0.0010397230507805943\n",
      "loss: 0.0016949698328971863\n",
      "loss: 0.0012885787291452289\n",
      "loss: 0.0006885679322294891\n",
      "loss: 0.0008110484923236072\n",
      "loss: 0.0018426489550620317\n",
      "loss: 0.0007008115062490106\n",
      "loss: 0.0007198050734587014\n",
      "loss: 0.0007993541657924652\n",
      "loss: 0.00042628170922398567\n",
      "loss: 0.0034150925930589437\n",
      "loss: 0.0006118844612501562\n",
      "loss: 0.0013685791054740548\n",
      "loss: 0.0011069182073697448\n",
      "loss: 0.001652655191719532\n",
      "loss: 0.0024128819350153208\n",
      "loss: 0.00028165275580249727\n",
      "loss: 0.002012164331972599\n",
      "loss: 0.006991947535425425\n",
      "loss: 0.002935988362878561\n",
      "loss: 0.0023108371533453465\n",
      "loss: 0.0011419240618124604\n",
      "loss: 0.0013889274559915066\n",
      "loss: 0.0006334861973300576\n",
      "loss: 0.0008896882063709199\n",
      "loss: 0.0008236334542743862\n",
      "loss: 0.0022863061167299747\n",
      "loss: 0.0002751785214059055\n",
      "loss: 0.0007486493559554219\n",
      "loss: 0.0013988447608426213\n",
      "loss: 0.0016014123102650046\n",
      "loss: 0.0009733724291436374\n",
      "loss: 0.00251327664591372\n",
      "loss: 0.0010308721102774143\n",
      "loss: 0.004977624863386154\n",
      "loss: 0.0005503254942595959\n",
      "loss: 0.0011503066634759307\n",
      "loss: 0.0007479485357180238\n",
      "loss: 0.003791517810896039\n",
      "loss: 0.00024183967616409063\n",
      "loss: 0.0021257493644952774\n",
      "loss: 0.002427157247439027\n",
      "loss: 0.00028448764351196587\n",
      "loss: 0.0008869130979292095\n",
      "loss: 0.0026594705414026976\n",
      "loss: 0.0017507171723991632\n",
      "loss: 0.0005100988200865686\n",
      "loss: 0.00047780759632587433\n",
      "loss: 0.0012828608741983771\n",
      "loss: 0.0019907415844500065\n",
      "loss: 0.0003685019910335541\n",
      "loss: 0.0005886149010621011\n",
      "loss: 0.0006538489833474159\n",
      "loss: 0.002286217175424099\n",
      "loss: 0.0005709572578780353\n",
      "loss: 0.0006777635426260531\n",
      "loss: 0.0006199064082466066\n",
      "loss: 0.0013664817670360208\n",
      "loss: 0.0032673224341124296\n",
      "loss: 0.0012981684412807226\n",
      "loss: 0.001955952262505889\n",
      "loss: 0.003083998803049326\n",
      "loss: 0.005786614492535591\n",
      "loss: 0.0024284450337290764\n",
      "loss: 0.0012986299116164446\n",
      "loss: 0.0012958705192431808\n",
      "loss: 0.002175785368308425\n",
      "loss: 0.002111456822603941\n",
      "loss: 0.0024152633268386126\n",
      "loss: 0.0009519489831291139\n",
      "loss: 0.0022775502875447273\n",
      "loss: 0.002278375206515193\n",
      "loss: 0.0007495162426494062\n",
      "loss: 0.00059352075913921\n",
      "loss: 0.0014520275872200727\n",
      "loss: 0.001057482440955937\n",
      "loss: 0.0006240512593649328\n",
      "loss: 0.001793845440261066\n",
      "loss: 0.002645249245688319\n",
      "loss: 0.0014920418616384268\n",
      "loss: 0.0005891616456210613\n",
      "loss: 0.0021973003167659044\n",
      "loss: 0.004302304703742266\n",
      "loss: 0.0033579454757273197\n",
      "loss: 0.0007377119618467987\n",
      "loss: 0.002043060725554824\n",
      "loss: 0.0016349161742255092\n",
      "loss: 0.0011172350496053696\n",
      "loss: 0.0012247232953086495\n",
      "loss: 0.0010119060752913356\n",
      "loss: 0.0018470622599124908\n",
      "loss: 0.001854797126725316\n",
      "loss: 0.00155580451246351\n",
      "loss: 0.0005831901216879487\n",
      "loss: 0.0008923152345232666\n",
      "loss: 0.0003912185784429312\n",
      "loss: 0.0016991982702165842\n",
      "loss: 0.0013770671794191003\n",
      "loss: 0.001185440574772656\n",
      "loss: 0.0010494323214516044\n",
      "loss: 0.0009311007452197373\n",
      "loss: 0.0007254122756421566\n",
      "loss: 0.0013433044077828526\n",
      "loss: 0.000820406072307378\n",
      "loss: 0.004552842117846012\n",
      "loss: 0.0008089912007562816\n",
      "loss: 0.0005200858577154577\n",
      "loss: 0.001697084284387529\n",
      "loss: 0.000620783946942538\n",
      "loss: 0.0012664834503084421\n",
      "loss: 0.0005287355161271989\n",
      "loss: 0.001637703157030046\n",
      "loss: 0.0006323577254079282\n",
      "loss: 0.0006370978662744164\n",
      "loss: 0.0006278689834289253\n",
      "loss: 0.001620684051886201\n",
      "loss: 0.0010236829984933138\n",
      "loss: 0.001055693137459457\n",
      "loss: 0.0044519174844026566\n",
      "loss: 0.0007576625212095678\n",
      "loss: 0.0006057044374756515\n",
      "loss: 0.0030420266557484865\n",
      "loss: 0.0007141403038986027\n",
      "loss: 0.0011360502103343606\n",
      "loss: 0.0007498206687159836\n",
      "loss: 0.0006685456610284746\n",
      "loss: 0.0005396076594479382\n",
      "loss: 0.0007032303255982697\n",
      "loss: 0.0005133456434123218\n",
      "loss: 0.0004482436052057892\n",
      "loss: 0.001371891819871962\n",
      "loss: 0.0005472408956848085\n",
      "loss: 0.0015128458617255092\n",
      "loss: 0.0005610913503915071\n",
      "loss: 0.0012207812396809459\n",
      "loss: 0.000651263166218996\n",
      "loss: 0.002901561325415969\n",
      "loss: 0.0010921868961304426\n",
      "loss: 0.0046136942692101\n",
      "loss: 0.003980168607085943\n",
      "loss: 0.0006774484063498676\n",
      "loss: 0.0007169762975536287\n",
      "loss: 0.0023188565392047167\n",
      "loss: 0.0024454554077237844\n",
      "loss: 0.0005271774134598672\n",
      "loss: 0.0005154369864612818\n",
      "loss: 0.0018757496727630496\n",
      "loss: 0.0027700024656951427\n",
      "loss: 0.0019649043679237366\n",
      "loss: 0.004062851425260305\n",
      "loss: 0.001207159017212689\n",
      "loss: 0.00430053798481822\n",
      "loss: 0.004087566863745451\n",
      "loss: 0.0007782707689329982\n",
      "loss: 0.001100145629607141\n",
      "loss: 0.0017698154551908374\n",
      "loss: 0.0052443696185946465\n",
      "loss: 0.0018507337663322687\n",
      "loss: 0.00227349903434515\n",
      "loss: 0.001409303629770875\n",
      "loss: 0.0008088952163234353\n",
      "loss: 0.0016947109252214432\n",
      "loss: 0.0005458408850245178\n",
      "loss: 0.002158858347684145\n",
      "loss: 0.002140665426850319\n",
      "loss: 0.0005886202561669052\n",
      "loss: 0.0003168850962538272\n",
      "loss: 0.0007649025646969676\n",
      "loss: 0.0010794573463499546\n",
      "loss: 0.0009054666734300554\n",
      "loss: 0.0015979055315256119\n",
      "loss: 0.0017358288168907166\n",
      "loss: 0.00141378294210881\n",
      "loss: 0.0012641176581382751\n",
      "loss: 0.0005371560109779239\n",
      "loss: 0.0008612648816779256\n",
      "loss: 0.0019956924952566624\n",
      "loss: 0.0007610013126395643\n",
      "loss: 0.0005784534150734544\n",
      "loss: 0.0018047683406621218\n",
      "loss: 0.00119267706759274\n",
      "loss: 0.0010023497743532062\n",
      "loss: 0.00043007131898775697\n",
      "loss: 0.0021046821493655443\n",
      "loss: 0.0007261623977683485\n",
      "loss: 0.0017837536288425326\n",
      "loss: 0.001291693071834743\n",
      "loss: 0.0017644033068791032\n",
      "loss: 0.0017054278869181871\n",
      "loss: 0.0008865557610988617\n",
      "loss: 0.0003623579686973244\n",
      "loss: 0.0008291276171803474\n",
      "loss: 0.0006392464274540544\n",
      "loss: 0.000422188313677907\n",
      "loss: 0.0005950411432422698\n",
      "loss: 0.00018015709065366536\n",
      "loss: 0.00034698177478276193\n",
      "loss: 0.0032468547578901052\n",
      "loss: 0.0004103217215742916\n",
      "loss: 0.0009328253800049424\n",
      "loss: 0.00023992665228433907\n",
      "loss: 0.0011830661678686738\n",
      "loss: 0.0026899557560682297\n",
      "loss: 0.0003435592807363719\n",
      "loss: 0.0013712503714486957\n",
      "loss: 0.0019202996045351028\n",
      "loss: 0.0008213885012082756\n",
      "loss: 0.00031740040867589414\n",
      "loss: 0.0028101950883865356\n",
      "loss: 0.0006697450880892575\n",
      "loss: 0.000329402246279642\n",
      "loss: 0.001527660759165883\n",
      "loss: 0.0036926071625202894\n",
      "loss: 0.0009798334212973714\n",
      "loss: 0.002674382645636797\n",
      "loss: 0.0019897515885531902\n",
      "loss: 0.0008822502568364143\n",
      "loss: 0.000618588353972882\n",
      "loss: 0.000423240679083392\n",
      "loss: 0.00024351946194656193\n",
      "loss: 0.0015537969302386045\n",
      "loss: 0.0010196494404226542\n",
      "loss: 0.0009559140889905393\n",
      "loss: 0.0006783013814128935\n",
      "loss: 0.0028468328528106213\n",
      "loss: 0.0012340311659500003\n",
      "loss: 0.0003662503440864384\n",
      "loss: 0.000511069200001657\n",
      "loss: 0.0003600740747060627\n",
      "loss: 0.0010714437812566757\n",
      "loss: 0.0007653071079403162\n",
      "loss: 0.0018543187761679292\n",
      "loss: 0.0003096537839155644\n",
      "loss: 0.0006281581008806825\n",
      "loss: 0.001212993054650724\n",
      "loss: 0.00029393608565442264\n",
      "loss: 0.0008564821328036487\n",
      "loss: 0.0009728159639053047\n",
      "loss: 0.003359562251716852\n",
      "loss: 0.0009292701142840087\n",
      "loss: 0.0003030736406799406\n",
      "loss: 0.0005613117828033864\n",
      "loss: 0.0005692701088264585\n",
      "loss: 0.0006970805115997791\n",
      "loss: 0.0019486561650410295\n",
      "loss: 0.0028191518504172564\n",
      "loss: 0.0011174491373822093\n",
      "loss: 0.0005888398154638708\n",
      "loss: 0.0004847165255341679\n",
      "loss: 0.0022559158969670534\n",
      "loss: 0.0028813323006033897\n",
      "loss: 0.0009768700692802668\n",
      "loss: 0.0022806019987910986\n",
      "loss: 0.00150375219527632\n",
      "loss: 0.0004645702429115772\n",
      "loss: 0.0016887456877157092\n",
      "loss: 0.0039328704588115215\n",
      "loss: 0.004811003804206848\n",
      "loss: 0.0022029646206647158\n",
      "loss: 0.0004723312449641526\n",
      "loss: 0.002467904007062316\n",
      "loss: 0.0004721127625089139\n",
      "loss: 0.0006243036477826536\n",
      "loss: 0.0011757140746340156\n",
      "loss: 0.0016874263528734446\n",
      "loss: 0.0010188505984842777\n",
      "loss: 0.0008139529381878674\n",
      "loss: 0.001373019185848534\n",
      "loss: 0.0016116105252876878\n",
      "loss: 0.0021306134294718504\n",
      "loss: 0.0005658005247823894\n",
      "loss: 0.0025059690233319998\n",
      "loss: 0.0013043269282206893\n",
      "loss: 0.0028315531089901924\n",
      "loss: 0.0005847883294336498\n",
      "loss: 0.0022903564386069775\n",
      "loss: 0.0005087208119221032\n",
      "loss: 0.0004603535926435143\n",
      "loss: 0.0008967792964540422\n",
      "loss: 0.001319354516454041\n",
      "loss: 0.0018368728924542665\n",
      "loss: 0.00133901194203645\n",
      "loss: 0.0016543027013540268\n",
      "loss: 0.001055621774867177\n",
      "loss: 0.0039451164193451405\n",
      "loss: 0.002902660286054015\n",
      "loss: 0.0005901465192437172\n",
      "loss: 0.0047716619446873665\n",
      "loss: 0.0011146290926262736\n",
      "loss: 0.0008501458214595914\n",
      "loss: 0.0008396667544730008\n",
      "loss: 0.0009193735313601792\n",
      "loss: 0.0005400952650234103\n",
      "loss: 0.0008792333537712693\n",
      "loss: 0.0002626564528327435\n",
      "loss: 0.0020224701147526503\n",
      "loss: 0.00030259130289778113\n",
      "loss: 0.0013566429261118174\n",
      "loss: 0.0024717594496905804\n",
      "loss: 0.0016617143992334604\n",
      "loss: 0.002490898361429572\n",
      "loss: 0.0012444236781448126\n",
      "loss: 0.002171205123886466\n",
      "loss: 0.0010579084046185017\n",
      "loss: 0.0005346891121007502\n",
      "loss: 0.0005923002609051764\n",
      "loss: 0.0004997739451937377\n",
      "loss: 0.0011352923465892673\n",
      "loss: 0.0012903859606012702\n",
      "loss: 0.0009580582263879478\n",
      "loss: 0.00136366521473974\n",
      "loss: 0.0007985709235072136\n",
      "loss: 0.0010948452400043607\n",
      "loss: 0.00225517968647182\n",
      "loss: 0.0007436504238285124\n",
      "loss: 0.004336139652878046\n",
      "loss: 0.0008889225427992642\n",
      "loss: 0.0022014304995536804\n",
      "loss: 0.0012334393104538321\n",
      "loss: 0.0019540125504136086\n",
      "loss: 0.0024423834402114153\n",
      "loss: 0.0008352729491889477\n",
      "loss: 0.0014051705366000533\n",
      "loss: 0.0030780930537730455\n",
      "loss: 0.003920292016118765\n",
      "loss: 0.0004699132405221462\n",
      "loss: 0.0008897241204977036\n",
      "loss: 0.0005115823005326092\n",
      "loss: 0.0020798449404537678\n",
      "loss: 0.0005556353717111051\n",
      "loss: 0.00047599346726201475\n",
      "loss: 0.001527252490632236\n",
      "loss: 0.0017156839603558183\n",
      "loss: 0.000907176174223423\n",
      "loss: 0.0004231988568790257\n",
      "loss: 0.0024556047283113003\n",
      "loss: 0.0005688024102710187\n",
      "loss: 0.0009237133199349046\n",
      "loss: 0.00249992567114532\n",
      "loss: 0.000494200037792325\n",
      "loss: 0.0003131596895400435\n",
      "loss: 0.003525110660120845\n",
      "loss: 0.0013623222475871444\n",
      "loss: 0.0006236724439077079\n",
      "loss: 0.0011352509027346969\n",
      "loss: 0.0013839347520843148\n",
      "loss: 0.0032993832137435675\n",
      "loss: 0.003644082695245743\n",
      "loss: 0.0004185653815511614\n",
      "loss: 0.004873400088399649\n",
      "loss: 0.0021238604094833136\n",
      "loss: 0.0025193789042532444\n",
      "loss: 0.0006514968699775636\n",
      "loss: 0.0019248919561505318\n",
      "loss: 0.0024834182113409042\n",
      "loss: 0.0022305590100586414\n",
      "loss: 0.001850769273005426\n",
      "loss: 0.001794129959307611\n",
      "loss: 0.0035503448452800512\n",
      "loss: 0.0012137088924646378\n",
      "loss: 0.0025902842171490192\n",
      "loss: 0.0015569472452625632\n",
      "loss: 0.00280870427377522\n",
      "loss: 0.0010870179394260049\n",
      "loss: 0.002940674312412739\n",
      "loss: 0.0025296458043158054\n",
      "loss: 0.0015067222993820906\n",
      "loss: 0.0011582424631342292\n",
      "loss: 0.002061507198959589\n",
      "loss: 0.00048741858336143196\n",
      "loss: 0.0014179185964167118\n",
      "loss: 0.001443438814021647\n",
      "loss: 0.002683027880266309\n",
      "loss: 0.0010667195310816169\n",
      "loss: 0.001161811756901443\n",
      "loss: 0.0023475836496800184\n",
      "loss: 0.001929095364175737\n",
      "loss: 0.0005973419174551964\n",
      "loss: 0.00203536543995142\n",
      "loss: 0.0007400486501865089\n",
      "loss: 0.005604212637990713\n",
      "loss: 0.0017289454117417336\n",
      "loss: 0.0008688285015523434\n",
      "loss: 0.0011479234090074897\n",
      "loss: 0.0018777698278427124\n",
      "loss: 0.0010253323707729578\n",
      "loss: 0.0018385390285402536\n",
      "loss: 0.00038211068022064865\n",
      "loss: 0.0017052125185728073\n",
      "loss: 0.0017895189812406898\n",
      "loss: 0.0002632412943057716\n",
      "loss: 0.000369847723050043\n",
      "loss: 0.0007794661214575171\n",
      "loss: 0.0015499450964853168\n",
      "loss: 0.0005458096275106072\n",
      "loss: 0.0025233332999050617\n",
      "loss: 0.0005442071706056595\n",
      "loss: 0.00042664873762987554\n",
      "loss: 0.0006003616726957262\n",
      "loss: 0.001114705461077392\n",
      "loss: 0.0011214204132556915\n",
      "loss: 0.0030282612424343824\n",
      "loss: 0.0009001436992548406\n",
      "loss: 0.004322445951402187\n",
      "loss: 0.0025093534495681524\n",
      "loss: 0.0021739236544817686\n",
      "loss: 0.0009148312965407968\n",
      "loss: 0.0008867534925229847\n",
      "loss: 0.002097680466249585\n",
      "loss: 0.0005182313034310937\n",
      "loss: 0.0010402784682810307\n",
      "loss: 0.0011017057113349438\n",
      "loss: 0.0010483070509508252\n",
      "loss: 0.00094391725724563\n",
      "loss: 0.0005005851271562278\n",
      "loss: 0.0010214931098744273\n",
      "loss: 0.0018936041742563248\n",
      "loss: 0.003490092698484659\n",
      "loss: 0.00047983290278352797\n",
      "loss: 0.00027179953758604825\n",
      "loss: 0.002542773261666298\n",
      "loss: 0.0011443729745224118\n",
      "loss: 0.0025627589784562588\n",
      "loss: 0.0009717087377794087\n",
      "loss: 0.0011611434165388346\n",
      "loss: 0.004424680955708027\n",
      "loss: 0.0003186168323736638\n",
      "loss: 0.0003252151363994926\n",
      "loss: 0.0010840205941349268\n",
      "loss: 0.004171488806605339\n",
      "loss: 0.0022895976435393095\n",
      "loss: 0.001320728799328208\n",
      "loss: 0.000630546361207962\n",
      "loss: 0.002180310431867838\n",
      "loss: 0.0012401471612975001\n",
      "loss: 0.0012047835625708103\n",
      "loss: 0.0013055596500635147\n",
      "loss: 0.0016188633162528276\n",
      "loss: 0.001019319868646562\n",
      "loss: 0.0007999191293492913\n",
      "loss: 0.0007174119236879051\n",
      "loss: 0.0006152695859782398\n",
      "loss: 0.00033874224754981697\n",
      "loss: 0.0009663609671406448\n",
      "loss: 0.00112081877887249\n",
      "loss: 0.002742334036156535\n",
      "loss: 0.0007000762852840126\n",
      "loss: 0.0006538351881317794\n",
      "loss: 0.00042753323214128613\n",
      "loss: 0.0006745289429090917\n",
      "loss: 0.007461318280547857\n",
      "loss: 0.0005955476080998778\n",
      "loss: 0.0004309896321501583\n",
      "loss: 0.0005672217230312526\n",
      "loss: 0.0011089238105341792\n",
      "loss: 0.0009657071204856038\n",
      "loss: 0.0003438768908381462\n",
      "loss: 0.0009786043083295226\n",
      "loss: 0.0008952005882747471\n",
      "loss: 0.0012089493684470654\n",
      "loss: 0.0006901302258484066\n",
      "loss: 0.0014840259682387114\n",
      "loss: 0.0008976100943982601\n",
      "loss: 0.0008540768176317215\n",
      "loss: 0.0012863948941230774\n",
      "loss: 0.0005380738875828683\n",
      "loss: 0.002560301683843136\n",
      "loss: 0.002737879054620862\n",
      "loss: 0.0004462004581000656\n",
      "loss: 0.0011822369415313005\n",
      "loss: 0.00158520822878927\n",
      "loss: 0.002049865899607539\n",
      "loss: 0.001335824723355472\n",
      "loss: 0.00045893798233009875\n",
      "loss: 0.0008552237413823605\n",
      "loss: 0.0013595236232504249\n",
      "loss: 0.003259931458160281\n",
      "loss: 0.00068977678893134\n",
      "loss: 0.0006837653345428407\n",
      "loss: 0.0020021817181259394\n",
      "loss: 0.004751902539283037\n",
      "loss: 0.0008490237523801625\n",
      "loss: 0.0012131492840126157\n",
      "loss: 0.0030126755591481924\n",
      "loss: 0.0013595804339274764\n",
      "loss: 0.0006194514571689069\n",
      "loss: 0.0007608549785800278\n",
      "loss: 0.0005495651857927442\n",
      "loss: 0.003086475655436516\n",
      "loss: 0.0005920277326367795\n",
      "loss: 0.002280663698911667\n",
      "loss: 0.0010961497901007533\n",
      "loss: 0.0004963139654137194\n",
      "loss: 0.0005428050644695759\n",
      "loss: 0.0006659780628979206\n",
      "loss: 0.0002120128192473203\n",
      "loss: 0.0006813771324232221\n",
      "loss: 0.0018470203503966331\n",
      "loss: 0.001745710032992065\n",
      "loss: 0.0011816489277407527\n",
      "loss: 0.00080159364733845\n",
      "loss: 0.00580985564738512\n",
      "loss: 0.00048596804845146835\n",
      "loss: 0.001054758089594543\n",
      "loss: 0.001929059042595327\n",
      "loss: 0.0006953920819796622\n",
      "loss: 0.00249922345392406\n",
      "loss: 0.0019053894793614745\n",
      "loss: 0.00061440170975402\n",
      "loss: 0.0021994549315422773\n",
      "loss: 0.0015231295255944133\n",
      "loss: 0.0024304247926920652\n",
      "loss: 0.0007964367978274822\n",
      "loss: 0.001520307152532041\n",
      "loss: 0.000511910009663552\n",
      "loss: 0.000800370064098388\n",
      "loss: 0.0004564425034914166\n",
      "loss: 0.003738773986697197\n",
      "loss: 0.0009064311161637306\n",
      "loss: 0.0011880589881911874\n",
      "loss: 0.0005484079592861235\n",
      "loss: 0.0022398612927645445\n",
      "loss: 0.001901976647786796\n",
      "loss: 0.0002543739101383835\n",
      "loss: 0.0007768689538352191\n",
      "loss: 0.0011641902383416891\n",
      "loss: 0.0026047953870147467\n",
      "loss: 0.0025985606480389833\n",
      "loss: 0.0010840141912922263\n",
      "loss: 0.0012220176868140697\n",
      "loss: 0.0010296553373336792\n",
      "loss: 0.0038166397716850042\n",
      "loss: 0.002929005306214094\n",
      "loss: 0.0027874649967998266\n",
      "loss: 0.0011145739117637277\n",
      "loss: 0.000765464676078409\n",
      "loss: 0.0026155360974371433\n",
      "loss: 0.0033788380678743124\n",
      "loss: 0.0021772021427750587\n",
      "loss: 0.0006000887369737029\n",
      "loss: 0.00251337350346148\n",
      "loss: 0.0008982348372228444\n",
      "loss: 0.00038983309059403837\n",
      "loss: 0.00248540798202157\n",
      "loss: 0.0008762003853917122\n",
      "loss: 0.0006417601252906024\n",
      "loss: 0.00045259660691954195\n",
      "loss: 0.002050998853519559\n",
      "loss: 0.0009824056178331375\n",
      "loss: 0.000321920815622434\n",
      "loss: 0.0018956772983074188\n",
      "loss: 0.001376375206746161\n",
      "loss: 0.0017783083021640778\n",
      "loss: 0.00330480863340199\n",
      "loss: 0.0005317638861015439\n",
      "loss: 0.0012974877608940005\n",
      "loss: 0.0011495028156787157\n",
      "loss: 0.00158814643509686\n",
      "loss: 0.001318640890531242\n",
      "loss: 0.0008265801006928086\n",
      "loss: 0.0009672206942923367\n",
      "loss: 0.001067227916792035\n",
      "loss: 0.001651841215789318\n",
      "loss: 0.0003270305460318923\n",
      "loss: 0.0008600239525549114\n",
      "loss: 0.0006197183392941952\n",
      "loss: 0.0006518772570416331\n",
      "loss: 0.002567236078903079\n",
      "loss: 0.00042778500937856734\n",
      "loss: 0.0007603733101859689\n",
      "loss: 0.0007748312200419605\n",
      "loss: 0.001597382128238678\n",
      "loss: 0.0017912250477820635\n",
      "loss: 0.0011852200841531157\n",
      "loss: 0.0008346981485374272\n",
      "loss: 0.0005012149922549725\n",
      "loss: 0.001467919792048633\n",
      "loss: 0.0011446140706539154\n",
      "loss: 0.0031808752100914717\n",
      "loss: 0.0028073135763406754\n",
      "loss: 0.0011048862943425775\n",
      "loss: 0.001491930685006082\n",
      "loss: 0.00042683040373958647\n",
      "loss: 0.0004862772475462407\n",
      "loss: 0.0026750348042696714\n",
      "loss: 0.0004943652893416584\n",
      "loss: 0.003568116342648864\n",
      "loss: 0.002558085834607482\n",
      "loss: 0.0005447706789709628\n",
      "loss: 0.0006857924163341522\n",
      "loss: 0.0008057369268499315\n",
      "loss: 0.000731568259652704\n",
      "loss: 0.001223630621097982\n",
      "loss: 0.00208710297010839\n",
      "loss: 0.0010835680877789855\n",
      "loss: 0.00244099460542202\n",
      "loss: 0.000608213187661022\n",
      "loss: 0.00206405739299953\n",
      "loss: 0.0022135113831609488\n",
      "loss: 0.0014075648505240679\n",
      "loss: 0.0004010906850453466\n",
      "loss: 0.00045355616020970047\n",
      "loss: 0.0024129555094987154\n",
      "loss: 0.00205317628569901\n",
      "loss: 0.00197026738896966\n",
      "loss: 0.0020980481058359146\n",
      "loss: 0.003114266786724329\n",
      "loss: 0.0035737776197493076\n",
      "loss: 0.0007813657866790891\n",
      "loss: 0.0008860031375661492\n",
      "loss: 0.002679706085473299\n",
      "loss: 0.004261216148734093\n",
      "loss: 0.0011062342673540115\n",
      "loss: 0.00071735680103302\n",
      "loss: 0.001222836785018444\n",
      "loss: 0.0024908906780183315\n",
      "loss: 0.003083995310589671\n",
      "loss: 0.00041172621422447264\n",
      "loss: 0.0007521308143623173\n",
      "loss: 0.003687141230329871\n",
      "loss: 0.0022188255097717047\n",
      "loss: 0.0011824682587757707\n",
      "loss: 0.0016556446207687259\n",
      "loss: 0.0016584250843152404\n",
      "loss: 0.0005118716508150101\n",
      "loss: 0.0015329511370509863\n",
      "loss: 0.0007264328305609524\n",
      "loss: 0.0008392790332436562\n",
      "loss: 0.000736637506633997\n",
      "loss: 0.001274480833671987\n",
      "loss: 0.0007686723838560283\n",
      "loss: 0.0017285265494138002\n",
      "loss: 0.002494555665180087\n",
      "loss: 0.0005431916797533631\n",
      "loss: 0.0007489510462619364\n",
      "loss: 0.0011098686372861266\n",
      "loss: 0.0006048432551324368\n",
      "loss: 0.0021130938548594713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 144\u001b[0m\n\u001b[1;32m    140\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# 训练DQN模型\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(env, model, target_model, optimizer, buffer, gamma, batch_size, num_episodes, target_update_freq)\u001b[0m\n\u001b[1;32m    102\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m target_model(batch_next_state\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m target_values \u001b[38;5;241m=\u001b[39m batch_reward \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m batch_done) \u001b[38;5;241m*\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_q_values\n\u001b[0;32m--> 105\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mq_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    106\u001b[0m target_q_values[\u001b[38;5;28mrange\u001b[39m(batch_size), batch_action] \u001b[38;5;241m=\u001b[39m target_values\n\u001b[1;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(q_values, target_q_values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 定义简单的DQN网络\n",
    "# DQN的目标是学习到一个在给定状态下，选择最优动作的策略，从而使智能体能够在环境中获取最大的累积奖励。\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 定义经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            del self.buffer[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 定义一个简单的环境，将MNIST数据集作为状态空间\n",
    "class MNISTEnvironment:\n",
    "    def __init__(self):\n",
    "        self.dataset = MNIST(root='/data/mwj/data', train=True, download=False,\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]))\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=1, shuffle=True)\n",
    "        self.current_state = None\n",
    "        self.action_space = [i for i in range(10)]\n",
    "\n",
    "    def reset(self):\n",
    "        image, label = next(iter(self.dataloader))\n",
    "        self.current_state = image.view(-1).numpy()\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        try:\n",
    "            next_image, label = next(iter(self.dataloader))\n",
    "            next_state = next_image.view(-1).numpy()\n",
    "        except StopIteration:\n",
    "            done = True\n",
    "            next_state = self.current_state\n",
    "        if action == label:\n",
    "            reward = 1\n",
    "        return next_state, reward, done\n",
    "\n",
    "# 定义DQN的训练过程\n",
    "def train_dqn(env, model, target_model, optimizer, buffer, gamma, batch_size, num_episodes, target_update_freq):\n",
    "    criterion = nn.MSELoss()\n",
    "    total_steps = 0  # 记录总的训练步数\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            epsilon = 0.1\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(env.action_space)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(torch.tensor(state, dtype=torch.float32).to(device))\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            buffer.push((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if len(buffer) > batch_size:\n",
    "                transitions = buffer.sample(batch_size)\n",
    "                batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "\n",
    "                batch_state = torch.tensor(batch_state, dtype=torch.float32).to(device)\n",
    "                batch_action = torch.tensor(batch_action, dtype=torch.long).to(device)\n",
    "                batch_reward = torch.tensor(batch_reward, dtype=torch.float32).to(device)\n",
    "                batch_next_state = torch.tensor(batch_next_state, dtype=torch.float32).to(device)\n",
    "                batch_done = torch.tensor(batch_done, dtype=torch.float32).to(device)\n",
    "\n",
    "                q_values = model(batch_state.to(device))\n",
    "                next_q_values = target_model(batch_next_state.to(device)).max(dim=1)[0]\n",
    "                target_values = batch_reward + (1 - batch_done) * gamma * next_q_values\n",
    "\n",
    "                target_q_values = q_values.clone().detach()\n",
    "                target_q_values[range(batch_size), batch_action] = target_values\n",
    "\n",
    "                loss = criterion(q_values, target_q_values)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 软更新目标网络\n",
    "                if total_steps % target_update_freq == 0:\n",
    "                    soft_update(target_model, model, tau=0.01)  # 设置软更新参数 tau\n",
    "            \n",
    "                # print(f\"loss: {loss.item()}\")\n",
    "            total_steps += 1  # 更新总的训练步数\n",
    "\n",
    "        print(\"Episode {}: Total Reward = {}\".format(episode, total_reward))\n",
    "\n",
    "def soft_update(target_model, model, tau):\n",
    "    for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# 创建环境和模型\n",
    "env = MNISTEnvironment()\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "model = DQN(input_size, output_size).to(device)\n",
    "target_model = DQN(input_size, output_size).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "num_episodes = 1\n",
    "\n",
    "\n",
    "# 训练DQN模型\n",
    "train_dqn(env, model, target_model, optimizer, buffer, gamma, batch_size, num_episodes, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个示例代码中，我们使用DQN来训练一个智能体（agent），通过观察MNIST数据集中的图像来学习识别手写数字。在这个示例中，MNIST数据集被作为状态空间，智能体通过观察图像并采取动作来尝试识别数字。\n",
    "\n",
    "\n",
    "在训练完成后，可以通过评估模型在测试集上的性能来评估其效果。在MNIST手写数字识别问题中，你可以使用测试集的图像和标签来评估模型的准确率（accuracy）。下面是一个简单的示例代码，演示如何评估训练好的模型在测试集上的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn(env, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in env.dataloader:\n",
    "            images = images.view(-1).numpy()\n",
    "            q_values = model(torch.tensor(images, dtype=torch.float32))\n",
    "            action = torch.argmax(q_values).item()\n",
    "            if action == labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# 调用测试函数进行模型评估\n",
    "test_dqn(env, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
